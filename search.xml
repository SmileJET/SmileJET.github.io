<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>【2020 MICCAI】A Macro-Micro Weakly-supervised Framework for AS-OCT Tissue Segmentation</title>
    <url>/20200729l1/</url>
    <content><![CDATA[<h1 id="motivation">Motivation</h1>
<ul>
<li>提出一种新颖的弱监督方法，用于图像中角膜和虹膜的分割</li>
<li>从弱标签样本中提取信息外，添加微观模型用于指导宏观模型</li>
<li>除了从完全注释图像中学习逐像素注释信息外，宏观微观流还旨在为训练模型提供更多高级语义信息</li>
<li>引入不确定性策略，以获得更准确和更稳定的指导</li>
</ul>
<h1 id="methods">Methods</h1>
<p>该框架由宏观模型和微观模型两部分组成。完全标注数据为每一个像素点的人工标签，弱标签数据为通过圆圈、点、涂鸦等标注出感兴趣的区域。</p>
<p>分为两个阶段进行优化：</p>
<ul>
<li>使用完全标注的数据和弱标签数据分别训练微观模型和宏观模型</li>
<li>将两个模型联合训练，这个阶段只使用弱标签数据，两个模型相互提供指导，以实现更好的分割效果</li>
</ul>
<p><img src="/20200729l1/image-20200729162055083.png"></p>
<h2 id="loss-functions-for-the-macro-and-micro-model">Loss Functions for the Macro and Micro Model</h2>
<p>第一阶段，使用弱标签数据训练宏观模型，使用完全标注数据训练微观模型。</p>
<p>训练损失分别为： <span class="math display">\[
\mathcal{L}_{\text {micro}}\left(x_{i}\right)=-\frac{1}{K \times C} \sum_{k=1}^{K} \sum_{c=1}^{C} y_{i}^{s}(k, c) \log m_{i}^{s}(k, c)
\]</span></p>
<p><span class="math display">\[
\mathcal{L}_{\text {macro}}\left(x_{j}\right)=-\frac{1}{\sum_{k=1}^{K} s_{j}(k) \times C} \sum_{k=1}^{K} \sum_{c=1}^{C} s_{j}(k) \cdot y_{j}^{w}(k, c) \log m_{j}^{w}(k, c)
\]</span></p>
<h2 id="uncertainty-aware-kl-loss-for-the-macroscopic-flow">Uncertainty-aware KL Loss for the Macroscopic Flow</h2>
<p>两个模型的输出之间采用KL散度损失来微调宏观模型，并引入不确定性图来选取可靠的像素点用于指导。因为可以将蒙特卡洛样本的方差视为不确定性的近似值，所以可以将微观模型的不确定性图<span class="math inline">\(U\)</span>表示为： <span class="math display">\[
\mu_c = \frac{1}{T}\sum_{t=1}^{T}{p_t^c} \quad and \quad U=\frac{1}{T \times C}\sum_{t=1}^{T}\sum_{c=1}^{C}(p_t^c-\mu_c)^2
\]</span> 因此可以得到不确定性图指导的KL损失： <span class="math display">\[
\begin{aligned}
\mathcal{L}_{U K L} &amp;=\frac{\mathbb{I}(U&lt;\tau) \cdot \mathcal{L}_{K L}\left(m_{j}^{w} \| m_{i}^{s}\right)}{\sum \mathbb{I}(U&lt;\tau)} \\
&amp;=\frac{1}{\sum_{k=1}^{K} \mathbb{I}(U(k)&lt;\tau) \times C} \sum_{k=1}^{K} \sum_{c=1}^{C} \mathbb{I}(U(k)&lt;\tau) \cdot m_{j}^{w}(k, c) \log \left(\frac{m_{j}^{w}(k, c)}{m_{i}^{s}(k, c)}\right)
\end{aligned}
\]</span></p>
<h2 id="uncertainty-aware-ema-as-the-microscopic-flow">Uncertainty-aware EMA as the Microscopic Flow</h2>
<p>本文使用宏观模型学习得到的分割结果为微观模型提供线索，提出一个不确定性指数移动平均机制（uncertainty-aware exponential moving average, UEMS）用于联合训练 <span class="math display">\[
\theta^{s}=\alpha \theta^{s}+(1-\alpha) \theta^{w} \quad \text { and } \quad \alpha=\frac{\sum_{k=1}^{K} \mathbb{I}(U(k)&lt;\tau)}{\sum_{k=1}^{K} \mathbb{1}}
\]</span></p>
<h1 id="experiments">Experiments</h1>
<p><img src="/20200729l1/image-20200729164304826.png"></p>
<p><img src="/20200729l1/image-20200729164324167.png"></p>
<p><img src="/20200729l1/image-20200729164339936.png"></p>
]]></content>
      <categories>
        <category>阅读笔记</category>
      </categories>
      <tags>
        <tag>2020</tag>
        <tag>MICCAI</tag>
        <tag>腾讯</tag>
        <tag>弱监督学习</tag>
        <tag>图像分割</tag>
      </tags>
  </entry>
  <entry>
    <title>【2020】A novel Region of Interest Extraction Layer for Instance Segmentation</title>
    <url>/20200801l4/</url>
    <content><![CDATA[<p>Code: <a href="https://github.com/IMPLabUniPr/mmdetection-groie" target="_blank" rel="external nofollow noopener noreferrer">https://github.com/IMPLabUniPr/mmdetection-groie</a></p>
<p>帮助融合FPN的结果，分为四个阶段：</p>
<ul>
<li>RoI Align将特征图对齐</li>
<li>RoI的结果可能会有些问题，使用一个5X5的卷积作为预处理</li>
<li>通过加和操作融合不同层的结果</li>
<li>起到一个Attention的作用，融合全局结果，消除无用信息</li>
</ul>
<p><img src="/20200801l4/image-20200508205331216.png"></p>
<p><img src="/20200801l4/image-20200508205358554.png"></p>
]]></content>
      <categories>
        <category>阅读笔记</category>
      </categories>
      <tags>
        <tag>2020</tag>
        <tag>图像分割</tag>
      </tags>
  </entry>
  <entry>
    <title>【2020 CVPR】C2FNAS: Coarse-to-Fine Neural Architecture Search for 3D Medical Image Segmentation</title>
    <url>/20200712l1/</url>
    <content><![CDATA[<h1 id="motivation">Motivation</h1>
<p>3D卷积神经网络在医疗影像中很成功，但是选择或设计一个合理的网络结构是非常耗时的。</p>
<p>由于内存限制和较大的搜索空间，NAS算法经常存在搜索阶段和部署阶段之间不一致，3D医学图像中更加消耗内存和时间，这种情况更加严重。</p>
<p>Neural Architecture Search（NAS）提出以来，通常是在较浅的网络中进行搜索，同时在更深层次进行部署。</p>
<p><img src="/20200712l1/image-20200712090439271.png"></p>
<h2 id="本文贡献">本文贡献</h2>
<ul>
<li>本文从头开始搜索3D分割网络，不牺牲网络大小或输入大小</li>
<li>基于医学图像分割先验，为每个阶段设计了特定的搜索空间和搜索方法</li>
<li>本文模型在MSD挑战的10个数据集上达到了state-of-the-art性能，以及强大的鲁棒性和迁移能力</li>
</ul>
<h1 id="methods">Methods</h1>
<h2 id="coarse-to-fine-neural-architecture-search">Coarse-to-fine Neural Architecture Search</h2>
<p>为了避免搜索阶段和部署阶段之间网络大小和输入大小的不一致，本文提出了用于3D医学图像分割的从粗到精的神经体系结构搜索方案。</p>
<p>为了减少搜索空间，采用了两种先验：</p>
<ul>
<li>U形的Encoder-Decoder结构</li>
<li>降采样和升采样之间使用skip-connections</li>
</ul>
<p><img src="/20200712l1/image-20200712091058072.png"></p>
<p>搜索空间<span class="math inline">\(A\)</span>分为两部分：较小的拓扑结构搜索空间<span class="math inline">\(S\)</span>和巨大的操作搜索空间<span class="math inline">\(C\)</span>： <span class="math display">\[
A=S \times C
\]</span></p>
<p>本文方法看作构建一个彩色的有向无环图(DAG)的过程，整个过程分为两个阶段：</p>
<ul>
<li>Coarse stage: 搜索宏观的网络结构</li>
<li>Fine stage: 为每个节点搜索最好的操作</li>
</ul>
<h2 id="coarse-stage-macro-level-search">Coarse Stage: Macro-level Search</h2>
<p>该阶段主要集中于搜索网络的拓扑结构。</p>
<p><img src="/20200712l1/image-20200712093605764.png"></p>
<p><img src="/20200712l1/image-20200712093654402.png"></p>
<h2 id="fine-stage-micro-level-search">Fine Stage: Micro-level Search</h2>
<p>卷积操作的搜索空间主要有三种：</p>
<ul>
<li><span class="math inline">\(3 \times 3 \times 3\)</span> 3D 卷积操作</li>
<li><span class="math inline">\(3 \times 3 \times 1\)</span>后紧接<span class="math inline">\(1 \times 1 \times 3\)</span> P3D卷积</li>
<li><span class="math inline">\(3 \times 3 \times 1\)</span> 2D 卷积操作</li>
</ul>
<p>为了在提高搜索效率的同时解决内存限制的问题，我们采用具有统一采样[10]的单路径单发NAS作为我们的搜索方法。</p>
<h1 id="experiments">Experiments</h1>
<p>最终的网络结构如图所示，每一个卷积操作前加一个<span class="math inline">\(1 \times 1 \times 1\)</span>卷积操作进行图像预处理。</p>
<p><img src="/20200712l1/image-20200712093925650.png"></p>
<p><img src="/20200712l1/image-20200712094018389.png"></p>
<p><img src="/20200712l1/image-20200712094030293.png"></p>
<p><img src="/20200712l1/image-20200712094042405.png"></p>
<p><img src="/20200712l1/image-20200712094058260.png"></p>
]]></content>
      <categories>
        <category>阅读笔记</category>
      </categories>
      <tags>
        <tag>2020</tag>
        <tag>图像分割</tag>
        <tag>CVPR</tag>
        <tag>NAS</tag>
        <tag>约翰霍普金斯大学</tag>
        <tag>NVIDIA</tag>
      </tags>
  </entry>
  <entry>
    <title>【2020】An automatic COVID-19 CT segmentation based on U-Net with attention mechanism</title>
    <url>/20200801l1/</url>
    <content><![CDATA[<h1 id="motivation">Motivation</h1>
<p>新型冠状病毒分割，在unet基础上做修改：</p>
<ul>
<li><p>加入注意力机制（通道+空间）</p></li>
<li><p>focal tversky loss（检测小区域的病灶位置）</p></li>
</ul>
<h1 id="methods">Methods</h1>
<p><img src="/20200801l1/image-20200801091847025.png"></p>
<p>Res_dil block（提高感受野）：</p>
<p><img src="/20200801l1/image-20200801091916347.png"></p>
<p>注意力机制：</p>
<p><img src="/20200801l1/image-20200801091943733.png"></p>
<h1 id="experiments">Experiments</h1>
<p><img src="/20200801l1/image-20200801091958908.png"></p>
<p><img src="/20200801l1/image-20200801092019925.png"></p>
]]></content>
      <categories>
        <category>阅读笔记</category>
      </categories>
      <tags>
        <tag>2020</tag>
        <tag>图像分割</tag>
        <tag>COVID-19</tag>
      </tags>
  </entry>
  <entry>
    <title>【2017】Attention is All You Need</title>
    <url>/20200819l1/</url>
    <content><![CDATA[<p><img src="/20200819l1/4155986-c83f5d273ab26cf4.png"></p>
<ul>
<li><a href="https://mp.weixin.qq.com/s/RLxWevVWHXgX-UcoxDS70w" target="_blank" rel="external nofollow noopener noreferrer">细讲 | Attention Is All You Need</a></li>
<li><a href="https://www.jianshu.com/p/b1030350aadb" target="_blank" rel="external nofollow noopener noreferrer">一步步解析Attention is All You Need！</a></li>
</ul>
]]></content>
      <categories>
        <category>阅读笔记</category>
      </categories>
      <tags>
        <tag>2017</tag>
        <tag>注意力机制</tag>
      </tags>
  </entry>
  <entry>
    <title>【2020 CVPR】CPR-GCN:Conditional Partial-Residual Graph Convolutional Network in Automated Anatomical Labeling of Coronary Arteries</title>
    <url>/20200630l1/</url>
    <content><![CDATA[<p><code>图卷积之前了解不多，这篇文章没怎么读懂，还是做一下记录。</code></p>
<h1 id="motivation">Motivation</h1>
<p>现有方法通常依赖于位置信息和对冠状动脉树的拓扑结构的先验知识，当主分支混乱时，很容易产生不好的结果。</p>
<p>目前图卷积在结构数据中应用广泛，这篇文章基于GCN提出了conditional partial-residual graph convolutional network （CPR-GCN）。</p>
<p>如下图，受试者之间的冠状动脉相差很大，这是标记系统的主要挑战。</p>
<p><img src="/20200630l1/image-20200630140915596.png"></p>
<p>本文贡献：</p>
<ul>
<li>提出了CPR-GCN，这是一种条件的残差图卷积网络，可以端到端标记冠状动脉树。</li>
<li>本文首次提出在冠状动脉标注领域中使用3D图像特征。</li>
<li>CPR-GCN与混合模型（3D CNN结合BiLSTMs）联合训练，有很好的效果。</li>
</ul>
<p>相关工作中提出的问题：</p>
<ul>
<li>TreeLab-Net主要是基于两个分支的网络，而在冠状动脉中经常有多个分支，这个超出了TreeLab-Net的能力。</li>
<li>堆叠多个GCN层容易出现过渡平滑。</li>
</ul>
<h1 id="methods">Methods</h1>
<p>CPR-GCN主要包含partial-residual GCN和conditions extractor两部分。</p>
<p><img src="/20200630l1/image-20200630142318333.png"></p>
<p><img src="/20200630l1/image-20200630142345121.png"></p>
<h2 id="position-domain-features">Position Domain Features</h2>
<p>参考文献【22】中使用球状坐标转换将3D坐标<span class="math inline">\(P_k=[(x_i, y_i, z_i)]_{i=1}^{Length_k}\)</span>转换成了2D坐标<span class="math inline">\([(\varphi_i, \theta_i)]_{i=1}^{Length_k}\)</span>。</p>
<p>角度范围在<span class="math inline">\([0,2\pi)\)</span>，容易在<span class="math inline">\(2\pi\)</span>与<span class="math inline">\(0\)</span>之间出现问题。</p>
<p>本文使用<span class="math inline">\(S^2\)</span>形式表示<span class="math inline">\(\varphi , \theta\)</span>克服此问题。</p>
<p>这里使用<span class="math inline">\(2 \times 2\)</span>的矩阵<span class="math inline">\(M=\left[\begin{array}{ll}\sin \theta &amp; \sin \varphi \\ \cos \theta &amp; \cos \varphi\end{array}\right]\)</span>，<span class="math inline">\(\sin(\cdot), \cos(\cdot)\)</span>的周期使得<span class="math inline">\(M\)</span>在整个<span class="math inline">\(S^2\)</span>上是稳定的，这种坐标变换被称为<span class="math inline">\(SCT S^2\)</span>。</p>
<p><span class="math display">\[\begin{array}{ll}
x=r \sin \theta \cos \varphi &amp; r=\sqrt{x^{2}+y^{2}+z^{2}} \\
y=r \sin \theta \sin \varphi &amp; \cos \theta=z / r, \theta \in[0, \pi] \\
z=r \cos \theta &amp; \sin \varphi=x /(\operatorname{rin} \theta), \cos \varphi=y /(\operatorname{rin} \theta)
\end{array}\]</span></p>
<h2 id="image-domain-conditions">Image Domain Conditions</h2>
<p>大多数医学图像（MRI、CCTA）属于3D信息，具有顺序的依赖性，这里使用3D CNN提取空间特征，然后使用BiLSTM学习顺序特征。</p>
<p>将最后一个状态作为条件信息，因为图像的重要性不如位置域特征。</p>
<p><img src="/20200630l1/image-20200630145438310.png"></p>
<h2 id="partial-residual-block-of-gcn">Partial-Residual Block of GCN</h2>
<p>图卷积的计算公式如下所示：</p>
<p><span class="math display">\[\begin{array}{l}
H^{(l+1)}=\sigma\left(\tilde{D}^{-\frac{1}{2}} \tilde{A} \tilde{D}^{-\frac{1}{2}} H^{l} W^{l}\right) \\
\tilde{A}=A+I_{N}, \tilde{D}_{i i}=\sum_{j} \tilde{A}_{i j}
\end{array}\]</span></p>
<p>将来自位置特征<span class="math inline">\(x\)</span>与CCTA图像域条件<span class="math inline">\(y\)</span>的组合作为图模型中的节点，边缘定义为parent-children关系。</p>
<p>每当分支结构存在下级分支时，再加入更多节点。</p>
<p><img src="/20200630l1/image-20200630145655538.png"></p>
<h1 id="experiments">Experiments</h1>
<p><img src="/20200630l1/image-20200630150056209.png"></p>
<p><img src="/20200630l1/image-20200630150110724.png"></p>
<p><img src="/20200630l1/image-20200630150125011.png"></p>
<p><img src="/20200630l1/image-20200630150150696.png"></p>
<p><img src="/20200630l1/image-20200630150208027.png"></p>
]]></content>
      <categories>
        <category>阅读笔记</category>
      </categories>
      <tags>
        <tag>2020</tag>
        <tag>图像分割</tag>
        <tag>CVPR</tag>
        <tag>图卷积</tag>
        <tag>阿里巴巴</tag>
      </tags>
  </entry>
  <entry>
    <title>【2018】Bag of Tricks for Image Classification with Convolutional Neural Networks</title>
    <url>/20200721l1/</url>
    <content><![CDATA[<h1 id="motivation">Motivation</h1>
<p>我们都知道trick在CNNs中的重要性，但是很少有文章详细讲解他们使用的trick，更少有文章对比各个trick对最后效果影响，这篇文章把CNNs里几种重要的trick做了详细对比，可以认为是一篇在CNNs中使用trick的cookbook。</p>
<p>这篇文章的trick有五个方面：model architecture, data augmentation, loss function, learning rate schedule，optimization。总结一句话就是，网络input stem和downsample模块、mixup、label smoothing、cosine learning rate decay、lr warmup、zero γ对网络影响都不小。</p>
<p><img src="/20200721l1/image-20200721105523992.png"></p>
<h1 id="methods">Methods</h1>
<p>训练网络的Backbone:</p>
<p><img src="/20200721l1/image-20200721105548168.png"></p>
<h2 id="训练速度">训练速度</h2>
<p>在训练模型时，共识是batchsize尽可能大一些，有以下优缺点：</p>
<ul>
<li>优点：模型收敛后的精度相对更高</li>
<li>缺点：（1）模型收敛速度慢（2）占用更多机器显存</li>
</ul>
<h3 id="large-batch-training">Large-batch training</h3>
<h4 id="linear-scaling-learning-rate">Linear scaling learning rate</h4>
<p>增大batch size，单个batch数据中噪声的影响会更小，可以使用大的学习率步长。</p>
<h4 id="learning-rate-warmup">Learning rate warmup</h4>
<p>训练开始阶段使用较小学习率，训练稳定时换回初始学习率</p>
<h4 id="zero-gamma">Zero <span class="math inline">\(\gamma\)</span></h4>
<p>Resnet网络结构中采用多个残差块结构，其结果为<span class="math inline">\(x+block(x)\)</span>，最后一层通常为batch Norm层，输出为<span class="math inline">\(\gamma\hat{x}+\beta\)</span>，这里将<span class="math inline">\(\gamma\)</span>和<span class="math inline">\(\beta\)</span>均置为0，可以使模型在初始阶段更容易训练。</p>
<h4 id="no-bias-decay">No bias decay</h4>
<p>只对卷积层和全连接层中的weight参数做正则化，不对bias参数做正则化，防止过拟合。</p>
<h3 id="low-precision-training">Low-precision training</h3>
<p>采用FP16进行训练，加快训练速度。</p>
<h1 id="experiments">Experiments</h1>
<p>此外还可以尝试修改ResNet的结构、学习率衰减策略、标签平滑、知识蒸馏、Mixup</p>
<p><img src="/20200721l1/image-20200721194527204.png"></p>
<p><img src="/20200721l1/image-20200721194541505.png"></p>
<p><img src="/20200721l1/image-20200721194805812.png"></p>
<p><img src="/20200721l1/image-20200721194817285.png"></p>
<p><img src="/20200721l1/image-20200721194828049.png"></p>
<p><img src="/20200721l1/image-20200721194848341.png"></p>
<p><img src="/20200721l1/image-20200721194907989.png"></p>
]]></content>
      <categories>
        <category>阅读笔记</category>
      </categories>
      <tags>
        <tag>2018</tag>
        <tag>trick</tag>
        <tag>亚马逊</tag>
      </tags>
  </entry>
  <entry>
    <title>【2020 EUR J NUCL MED MOL I】CTumorGAN: a unified framework for automatic computed tomography tumor segmentation</title>
    <url>/20200807l1/</url>
    <content><![CDATA[<p>Accepted by <code>European Journal of Nuclear Medicine and Molecular Imaging</code></p>
<h1 id="motivation">Motivation</h1>
<p>提出的深度模型通常可以很好应用于目标数据集和任务，但用于其他数据集或问题的可能性非常有限。</p>
<p><img src="/20200807l1/image-20200807171938865.png"></p>
<h1 id="methods">Methods</h1>
<p>使用生成器+PatchGAN的方式作为整体框架，能够应用于多种病灶分割任务。</p>
<p><img src="/20200807l1/image-20200807172020185.png"></p>
<p><img src="/20200807l1/image-20200807172752184.png"></p>
<p><img src="/20200807l1/image-20200807172237853.png"></p>
<h1 id="experiments">Experiments</h1>
<p><img src="/20200807l1/image-20200807172257162.png"></p>
<p><img src="/20200807l1/image-20200807172354945.png"></p>
<p><img src="/20200807l1/image-20200807172309075.png"></p>
]]></content>
      <categories>
        <category>阅读笔记</category>
      </categories>
      <tags>
        <tag>2020</tag>
        <tag>图像分割</tag>
        <tag>EUR J NUCL MED MOL I</tag>
      </tags>
  </entry>
  <entry>
    <title>【2020 CVPR】CenterMask: single shot instance segmentation with point representation</title>
    <url>/20200731l4/</url>
    <content><![CDATA[<h1 id="motivation">Motivation</h1>
<p><img src="/20200731l4/image-20200731203827066.png"></p>
<p><img src="/20200731l4/image-20200731203906900.png"></p>
<h1 id="methods">Methods</h1>
<p>引入局部形状预测和全局显著性预测，分别预测每个物体的形状和全局下都有哪些物体，一个负责局部一个负责全局</p>
<p><img src="/20200731l4/image-20200731203940571.png"></p>
<p><img src="/20200731l4/image-20200731204006555.png"></p>
<h1 id="experiments">Experiments</h1>
<p><img src="/20200731l4/image-20200731204033519.png"></p>
<p><img src="/20200731l4/image-20200731204052202.png"></p>
<p><img src="/20200731l4/image-20200731204108886.png"></p>
<p><img src="/20200731l4/image-20200731204133640.png"></p>
<p><img src="/20200731l4/image-20200731204152791.png"></p>
]]></content>
      <categories>
        <category>阅读笔记</category>
      </categories>
      <tags>
        <tag>2020</tag>
        <tag>图像分割</tag>
        <tag>CVPR</tag>
        <tag>美团</tag>
      </tags>
  </entry>
  <entry>
    <title>【2020】ClassMix: Segmentation-Based Data Augmentation for Semi-Supervised Learning</title>
    <url>/20200720l2/</url>
    <content><![CDATA[<p>Code: <a href="https://github.com/WilhelmT/ClassMix" target="_blank" rel="external nofollow noopener noreferrer">https://github.com/WilhelmT/ClassMix</a></p>
<h1 id="motivation">Motivation</h1>
<h2 id="本文贡献">本文贡献</h2>
<ul>
<li>引入了一种新的语义分割增强策略，ClassMix</li>
<li>利用一致性正则化和伪标签进行语义分割</li>
<li>在Cityscapes数据集和Pascal VOC数据集的半监督学习中都具有一定的有效性。</li>
</ul>
<h1 id="methods">Methods</h1>
<p>在无标注数据中采样两张图像A和B，使用分割模型分别进行预测，在A的预测结果中通过argmax生成预测结果，随机选取一半预测标签作为mask，将A图中mask区域合并到B图中。</p>
<p>Loss部分，有标注部分使用真实标签，无标注部分使用生成的伪标签。</p>
<p><img src="/20200720l2/image-20200720170305343.png"></p>
<p><img src="/20200720l2/image-20200720170207551.png"></p>
<h1 id="experiments">Experiments</h1>
<p><img src="/20200720l2/image-20200720170636800.png"></p>
<p><img src="/20200720l2/image-20200720170650354.png"></p>
<p><img src="/20200720l2/image-20200720170711910.png"></p>
]]></content>
      <categories>
        <category>阅读笔记</category>
      </categories>
      <tags>
        <tag>2020</tag>
        <tag>图像分割</tag>
        <tag>半监督学习</tag>
        <tag>数据增强</tag>
        <tag>code</tag>
      </tags>
  </entry>
  <entry>
    <title>【2020 TIP】Coarse-to-Fine Semantic Segmentation From Image-Level Labels</title>
    <url>/20200519l1/</url>
    <content><![CDATA[<p>IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 29, 2020</p>
<h1 id="motivation">Motivation</h1>
<p>语义分割分为三类：</p>
<ul>
<li>基于像素级标注的全监督训练</li>
<li>基于目标级标注的弱监督训练（Bbox、spots、scribbles）</li>
<li>基于图像级标注的弱监督训练</li>
</ul>
<p>基于像素级的标注需要耗费太多精力，本文考虑基于图像级标注来实现语义分割，可以大大减少像素级标注的工作量。</p>
<h1 id="method">Method</h1>
<p><img src="/20200519l1/image-20200520091312082.png"></p>
<p>首先通过初始网络生成初始分割结果，再利用图模型优化，指导语义分割模型学习，反复迭代优化。</p>
<p>框架分为三部分： coarse mask generation, coarse mask enhancement, recursive mask refinement</p>
<p><img src="/20200519l1/image-20200520091439726.png"></p>
<h2 id="coarse-mask-generation">Coarse Mask Generation</h2>
<p>采用一个8层的CNN结构，能够无监督的方式生成初始分割结果，这一步骤不考虑类别信息。</p>
<h2 id="coarse-mask-enhancement">Coarse Mask Enhancement</h2>
<p>通过GrabCut算法优化分割结果。</p>
<h2 id="recursive-mask-refinement">Recursive Mask Refinement</h2>
<p>将类别信息赋值给分割结果，作为语义分割模型的标签指导训练，生成的结果再返回到Coarse Mask Enhancement。</p>
<p><img src="/20200519l1/image-20200520092215557.png"></p>
<h2 id="model-parameterization">Model Parameterization</h2>
<p><img src="/20200519l1/image-20200520092259624.png"></p>
<h2 id="extend-the-proposed-framework-to-foreground-segmentation">Extend the Proposed Framework to Foreground Segmentation</h2>
<p>框架可以延伸到前景分割中，提出dilated feature pyramid network ，采用空洞卷积扩大感受野。</p>
<p>Inspired by dilated convolution and multi-scale feature learning, we propose the Dilated Feature Pyramid Network (DFPN) for foreground segmentation task as shown in Fig. 4. The proposed DFPN has the same architecture as FPN [43] except adding the dilated convolution layers for three branches to enlarge the receptive field of the network.</p>
<p><img src="/20200519l1/image-20200520092358141.png"></p>
<h1 id="experiments">Experiments</h1>
<p>训练阶段只采用了有单个类别的图像，测试发现可以识别图像中多个类别信息。</p>
<p><img src="/20200519l1/image-20200519164102851.png"></p>
]]></content>
      <categories>
        <category>阅读笔记</category>
      </categories>
      <tags>
        <tag>2020</tag>
        <tag>图像分割</tag>
        <tag>半监督学习</tag>
        <tag>TIP</tag>
        <tag>Image level label</tag>
      </tags>
  </entry>
  <entry>
    <title>【2020 MICCAI】Cross-Domain Medical Image Translation by Shared Latent Gaussian Mixture Model</title>
    <url>/20200804l1/</url>
    <content><![CDATA[<h1 id="motivation">Motivation</h1>
<ul>
<li>现有图像跨域翻译模型都有不错的性能来辅助图像分割任务，然而这些模型在翻译过程中不能很好保留图像的细节。</li>
<li>CT与MRI之间的巨大差异，往往会导致性能不佳，在临床上重要性较低，而且成对的CT和MRI数据通常很难收集到。本文考虑CT图像的非增强图像与增强图像的转换问题。</li>
<li>为了在医学图像转换过程中保留精细的结构，本文提出了一个基于patch的模型，该模型使用了来自高斯混合模型的共享潜在变量。</li>
</ul>
<p><img src="/20200804l1/image-20200804143803150.png"></p>
<h1 id="methods">Methods</h1>
<p><img src="/20200804l1/image-20200804144948326.png"></p>
<h2 id="unsupervised-image-to-image-translation-networks-unit">Unsupervised Image-to-Image Translation Networks (UNIT)</h2>
<ul>
<li>整体网络结构由两组变分自编码器组成分别用于两个不同的数据域，并通过共享的潜在空间<span class="math inline">\(Z\)</span>实现图像翻译。</li>
<li>共享的潜在空间独立于源域和目标域，并被强制遵循具有unit variance的高斯分布。</li>
</ul>
<h2 id="patch-based-mixtures-gaussian-image-to-image-translation">Patch-Based Mixtures Gaussian Image-to-Image Translation</h2>
<p>使用整张图像会导致细节丢失，本文提出基于Patch的方法，从源域和目标域随机选取相同位置的图像块进行实验。</p>
<p>两个编码器和两个生成器组成了两个VAE模型<span class="math inline">\(E_1(x_1, \theta_1), E_2(x_2, \theta_2)\)</span>，训练损失中加入了KL散度： <span class="math display">\[
\begin{array}{l}
\mathcal{L}_{V A E_{1}}\left(E_{1}, G_{1}, \boldsymbol{\Theta}_{1}, \mathbf{\Sigma}_{1}, \boldsymbol{\Sigma}_{z}, \mu_{z}\right)=\lambda_{1} \operatorname{KL}\left(q_{1}\left(z \mid x_{1}\right)|| p(z)\right) \\
-\lambda_{2} \mathbb{E}_{z \sim q_{1}\left(z \mid x_{1}\right)}\left[\log p G_{1}\left(x_{1} \mid z\right)\right] \\
\mathcal{L}_{V A E_{2}}\left(E_{2}, G_{2}, \boldsymbol{\Theta}_{2}, \boldsymbol{\Sigma}_{2}, \boldsymbol{\Sigma}_{z}, \mu_{z}\right)=\lambda_{1} \operatorname{KL}\left(q_{2}\left(z \mid x_{2}\right) \| p(z)\right) \\
-\lambda_{2} \mathbb{E}_{z \sim q_{2}\left(z \mid x_{2}\right)}\left[\log p G_{1}\left(x_{2} \mid z\right)\right]
\end{array}
\]</span> 除此之外还有两个对抗生成网络<span class="math inline">\(GAN_1={G_1, D_1}, GAN_2={G_2, D_2}\)</span>。如果分别从第一或第二域采样图像，则D1，D2被约束为输出true，如果分别从G1，G2生成图像，则D1，D2被约束为false。 <span class="math display">\[
\begin{array}{l}
\mathcal{L}_{G A N_{1}}\left(E_{2}, G_{1}, D_{1}, \boldsymbol{\Theta}_{1}, \mathbf{\Sigma}_{1}, \mathbf{\Sigma}_{z}, \mu_{z}\right)=\lambda_{0} \mathbb{E}_{x_{1} \sim P_{\mathcal{X}_{1}}}\left[\log D_{1}\left(x_{1}\right)\right] \\
+\lambda_{0} \mathbb{E}_{z \sim q_{2}\left(z \mid x_{2}\right)}\left[\log D_{1}\left(G_{1}(z)\right)\right] \\
\mathcal{L}_{G A N_{2}}\left(E_{1}, G_{2}, D_{2}, \Theta_{2}, \boldsymbol{\Sigma}_{2}, \boldsymbol{\Sigma}_{z}, \mu_{z}\right)=\lambda_{0} \mathbb{E}_{x_{2} \sim P_{\mathcal{X}_{2}}}\left[\log D_{2}\left(x_{2}\right)\right] \\
+\lambda_{0} \mathbb{E}_{z \sim q_{1}\left(z \mid x_{1}\right)}\left[\log D_{2}\left(G_{2}(z)\right)\right]
\end{array}
\]</span> 还加入了循环一致性约束： <span class="math display">\[
\begin{array}{l}
\mathcal{L}_{C C_{2}}\left(E_{1}, G_{1}, E_{2}, G_{2}, \boldsymbol{\Theta}_{1}, \boldsymbol{\Theta}_{2}, \boldsymbol{\Sigma}_{1}, \boldsymbol{\Sigma}_{2}, \boldsymbol{\Sigma}_{z}, \mu_{z}\right) \\
=\lambda_{3} \operatorname{KL}\left(q_{1}\left(z \mid x_{1}\right) \| p(z)\right) \\
+\lambda_{4} \operatorname{KL}\left(q_{2}\left(z \mid x_{1}^{1-&gt;2}\right) \| p(z)\right)-\lambda_{4} \mathbb{E}_{z \sim q_{2}\left(z \mid x_{1}^{1}&gt;2\right)}\left[\log p G_{1}\left(x_{1} \mid z\right)\right] \\
\mathcal{L}_{C C_{2}}\left(E_{2}, G_{2}, E_{1}, G_{1}, \boldsymbol{\Theta}_{1}, \boldsymbol{\Theta}_{2}, \boldsymbol{\Sigma}_{1}, \boldsymbol{\Sigma}_{2}, \boldsymbol{\Sigma}_{z}, \mu_{z}\right) \\
=\lambda_{3} \operatorname{KL}\left(q_{2}\left(z \mid x_{2}\right) \| p(z)\right) \\
+\lambda_{4} \operatorname{KL}\left(q_{1}\left(z \mid x_{2}^{2-&gt;1}\right) \| p(z)\right)-\lambda_{4} \mathbb{E}_{z \sim q_{1}\left(z \mid x_{2}^{2}&gt;1\right)}\left[\log p G_{2}\left(x_{2} \mid z\right)\right]
\end{array}
\]</span> 最终损失为： <span class="math display">\[
\begin{array}{l}
\arg \min \left(E_{1}, E_{2}, G_{1}, G_{2}, \Theta_{1}, \Theta_{2}, \boldsymbol{\Sigma}_{1}, \boldsymbol{\Sigma}_{2}, \boldsymbol{\Sigma}_{z}, \mu_{z}\right) \max \left(D_{1}, D_{2}\right) \\
\mathcal{L}_{V A E_{1}}\left(E_{1}, G_{1}, \boldsymbol{\Theta}_{1}, \boldsymbol{\Sigma}_{1}, \boldsymbol{\Sigma}_{z}, \mu_{z}\right)+\mathcal{L}_{V A E_{2}}\left(E_{2}, G_{2}, \boldsymbol{\Theta}_{1}, \boldsymbol{\Sigma}_{1}, \boldsymbol{\Sigma}_{z}, \mu_{z}\right) \\
+\mathcal{L}_{C C 1}\left(E_{1}, G_{1}, E_{2}, G_{2}, \boldsymbol{\Theta}_{1}, \boldsymbol{\Sigma}_{1}, \boldsymbol{\Theta}_{2}, \boldsymbol{\Sigma}_{2}, \boldsymbol{\Sigma}_{z}, \mu_{z}\right) \\
+\mathcal{L}_{C C 2}\left(E_{1}, G_{1}, E_{2}, G_{2}, \boldsymbol{\Theta}_{1}, \boldsymbol{\Sigma}_{1}, \boldsymbol{\Theta}_{2}, \boldsymbol{\Sigma}_{2}, \boldsymbol{\Sigma}_{z}, \mu_{z}\right) \\
+\mathcal{L}_{G A N_{1}}\left(E_{1}, G_{1}, D_{1}, \boldsymbol{\Theta}_{1}, \boldsymbol{\Sigma}_{1}\right)+\mathcal{L}_{G A N_{2}}\left(E_{2}, G_{2}, D_{2}, \Theta_{2}, \boldsymbol{\Sigma}_{2}\right)
\end{array}
\]</span></p>
<h1 id="experiments">Experiments</h1>
<p><img src="/20200804l1/image-20200804151516158.png"></p>
<p><img src="/20200804l1/image-20200804151527153.png"></p>
<p><img src="/20200804l1/image-20200804151540082.png"></p>
]]></content>
      <categories>
        <category>阅读笔记</category>
      </categories>
      <tags>
        <tag>2020</tag>
        <tag>MICCAI</tag>
        <tag>图像翻译</tag>
      </tags>
  </entry>
  <entry>
    <title>【2020 ECCV】Conditional Convolutions for Instance Segmentation</title>
    <url>/20200810l1/</url>
    <content><![CDATA[<p>Code: <a href="https://github.com/aim-uofa/AdelaiDet/" target="_blank" rel="external nofollow noopener noreferrer">https://github.com/aim-uofa/AdelaiDet/</a></p>
<h1 id="motivation">Motivation</h1>
<p>实例分割需要两种类型的信息：1）外观信息以对对象进行分类； 2）位置信息以区分属于同一类别的多个对象。</p>
<p>本文提出CondInst，优点有两个：</p>
<ul>
<li>实例分割通过全卷积网络解决，不需要ROI裁剪和特征对齐</li>
<li>使用条件卷积动态生成mask的channel数量，mask head可以非常紧凑，加速推理速度</li>
</ul>
<p><img src="/20200810l1/image-20200810164249797.png"></p>
<p><img src="/20200810l1/image-20200810164354483.png"></p>
<h1 id="methods">Methods</h1>
<p>基于FCOS 进行改进，主要亮点是动态卷积</p>
<p>不需要预测bbox，但可以通过bbox NMS进行加速</p>
<p><img src="/20200810l1/image-20200810164721230.png"></p>
<h1 id="experiments">Experiments</h1>
<p><img src="/20200810l1/image-20200810164917962.png"></p>
]]></content>
      <categories>
        <category>阅读笔记</category>
      </categories>
      <tags>
        <tag>2020</tag>
        <tag>图像分割</tag>
        <tag>code</tag>
        <tag>ECCV</tag>
      </tags>
  </entry>
  <entry>
    <title>【2019 ICCV】CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features</title>
    <url>/20200720l1/</url>
    <content><![CDATA[<p>Code: <a href="https://github.com/clovaai/CutMix-PyTorch" target="_blank" rel="external nofollow noopener noreferrer">https://github.com/clovaai/CutMix-PyTorch</a></p>
<h1 id="motivation">Motivation</h1>
<p>Regional Dropout可以提高分类任务准确性，引导模型关注较弱判别性的区域，使得模型具有更好的泛化能力。</p>
<p>这些方法通过覆盖黑色像素或者加入随机噪声补丁来取出训练数据上的信息，这样导致部分信息丢失，而且训练效率较低。</p>
<p><img src="/20200720l1/image-20200720154731427.png"></p>
<h1 id="methods">Methods</h1>
<p>本文将Mixup与Cutout相结合，在其中一张图像中裁剪一块覆盖在另一张图像相同位置上，对应的label按照patch大小进行等比例混合。 <span class="math display">\[
\tilde{x}=\mathbf{M} \odot x_{A}+(\mathbf{1}-\mathbf{M}) \odot x_{B}
\]</span></p>
<p><span class="math display">\[
\tilde{y} =\lambda y_A + (1-\lambda)y_B
\]</span></p>
<p>边界框的坐标采样自均匀分布 <span class="math display">\[
\begin{array}{l}
r_{x} \sim \text { Unif }(0, W), \quad r_{w}=W \sqrt{1-\lambda} \\
r_{y} \sim \text { Unif }(0, H), \quad r_{h}=H \sqrt{1-\lambda}
\end{array}
\]</span></p>
<p><span class="math display">\[
\frac{r_wr_h}{WH}=1-\lambda
\]</span></p>
<p>Motivation shared by Cutout,确保两个武器能够从一张图像中识别出两个目标的部分视角，从而提高训练效率。Cutout能够使模型关注目标的less discriminative部分，部分像素未使用，导致训练效率较低。Mixup引入了伪影使得图像不自然。CutMix能够精确定位两个目标，有效改进了Cutout。</p>
<p><img src="/20200720l1/image-20200720160323387.png"></p>
<p><img src="/20200720l1/image-20200720160133187.png"></p>
<p><img src="/20200720l1/image-20200720160151306.png"></p>
<h1 id="experiments">Experiments</h1>
<p><img src="/20200720l1/image-20200720160233357.png"></p>
<p><img src="/20200720l1/image-20200720160249837.png"></p>
]]></content>
      <categories>
        <category>阅读笔记</category>
      </categories>
      <tags>
        <tag>code</tag>
        <tag>ICCV</tag>
        <tag>2019</tag>
      </tags>
  </entry>
  <entry>
    <title>【2020 ISBI】DRU-net: An Efficient Deep Convolutional Neural Network for Medical Image Segmentation</title>
    <url>/20200801l5/</url>
    <content><![CDATA[<p>Code: <a href="https://github.com/MinaJf/DRU-net" target="_blank" rel="external nofollow noopener noreferrer">https://github.com/MinaJf/DRU-net</a></p>
<p>融合了ResNet和DenseNet的思想，一个效率高，一个准确性高。</p>
<p><img src="/20200801l5/image-20200510105217058.png"></p>
<p><img src="/20200801l5/image-20200510105245070.png"></p>
<p><img src="/20200801l5/image-20200510105324524.png"></p>
]]></content>
      <categories>
        <category>阅读笔记</category>
      </categories>
      <tags>
        <tag>2020</tag>
        <tag>图像分割</tag>
        <tag>code</tag>
        <tag>ISBI</tag>
      </tags>
  </entry>
  <entry>
    <title>【2017 TPAMI】DeepLabv2: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs</title>
    <url>/20200709l1/</url>
    <content><![CDATA[<p>Code: <a href="https://bitbucket.org/aquariusjay/deeplab-public-ver2/src/master/" target="_blank" rel="external nofollow noopener noreferrer">https://bitbucket.org/aquariusjay/deeplab-public-ver2/src/master/</a></p>
<h1 id="motivation">Motivation</h1>
<h2 id="dcnn在图像语义分割中的挑战">DCNN在图像语义分割中的挑战</h2>
<ul>
<li>特征图的分辨率减小</li>
<li>目标的多尺度问题</li>
<li>由于DCNN的不变性产生的定位准确率减小问题</li>
</ul>
<h2 id="本文优势">本文优势</h2>
<ul>
<li>速度：通过空洞卷积，本文的DCNN能在Nvidia Titan X GPU上以8FPS速度运行，全连接的CRF在CPU上的推断时间为0.5秒</li>
<li>准确率：在多个数据集上获得state-of-art结果</li>
<li>简便性：本文系统由两个非常完善的模块组成，DCNN和CRF</li>
</ul>
<h1 id="methods">Methods</h1>
<p><img src="/20200709l1/image-20200709091315408.png"></p>
<h2 id="atrous-convolution-for-dense-feature-extraction-and-field-of-view-enlargement">Atrous Convolution for Dense Feature Extraction and Field-of-View Enlargement</h2>
<p><img src="/20200709l1/image-20200709091423758.png"></p>
<p><img src="/20200709l1/image-20200709091445146.png"></p>
<p>卷积公式：</p>
<p><span class="math display">\[
y[i]=\sum_{k=1}^{K}{x[i+r\cdot k]w[k]}
\]</span></p>
<h2 id="multiscale-image-representations-using-atrous-spatial-pyramid-pooling">Multiscale Image Representations using Atrous Spatial Pyramid Pooling</h2>
<p>尝试从两种方法来处理语义分割中的尺度可变性：</p>
<ul>
<li>使用共享参数方式并行DCNN分支从原始图像的多个输出中提取DCNN的Score Map</li>
<li>受R-CNN的空间金字塔启发，提出ASPP结构</li>
</ul>
<p><img src="/20200709l1/image-20200709091744292.png"></p>
<h2 id="structured-prediction-with-fully-connected-conditional-random-fields-for-accurate-boundary-recovery">Structured Prediction with Fully-Connected Conditional Random Fields for Accurate Boundary Recovery</h2>
<p>继续DeepLab v1中的条件随机场对网络输出进行后处理。</p>
<h1 id="experiments">Experiments</h1>
<p><img src="/20200709l1/image-20200709092341631.png"></p>
<p><img src="/20200709l1/image-20200709092359416.png"></p>
<p><img src="/20200709l1/image-20200709092429362.png"></p>
]]></content>
      <categories>
        <category>阅读笔记</category>
      </categories>
      <tags>
        <tag>2017</tag>
        <tag>code</tag>
        <tag>DeepLab</tag>
        <tag>TPAMI</tag>
        <tag>谷歌</tag>
      </tags>
  </entry>
  <entry>
    <title>【2015 ICLR】DeepLabv1: Semantic image segmentation with deep convolutional nets and fully connected CRFs</title>
    <url>/20200708l1/</url>
    <content><![CDATA[<p>Code: <a href="https://github.com/TheLegendAli/DeepLab-Context" target="_blank" rel="external nofollow noopener noreferrer">https://github.com/TheLegendAli/DeepLab-Context</a></p>
<h1 id="motivation">Motivation</h1>
<h2 id="存在的问题">存在的问题</h2>
<ul>
<li>重复池化和下采样导致分辨率大幅下降，位置信息丢失难以恢复</li>
<li>空间不变性导致细节信息丢失</li>
</ul>
<h2 id="主要的优势">主要的优势</h2>
<ul>
<li>速度很快，DCNN 8fps，CRF需要0.5秒</li>
<li>准确率高，当时在PASCAL的语义分割集上效果最好</li>
<li>结构简单，DCNN和CRF的组合</li>
</ul>
<h1 id="methods">Methods</h1>
<h2 id="空洞卷积">空洞卷积</h2>
<p>不更改卷积核大小的情况下，通过增加Input stride增加卷积核的感受野。</p>
<p><img src="/20200708l1/image-20200709083417500.png"></p>
<h2 id="crf">CRF</h2>
<p>卷积网络在分类精度和定位精度之间存在一个自然的取舍：具有多个最大池化层的更深模型在分类任务中很成功，但其不变性的增加和更大的感受野使得从高层次输出中预测位置变得更具挑战性。</p>
<p><img src="/20200708l1/image-20200709084653268.png"></p>
<p>本文采用条件随机场（CRFs）处理分割中不平滑问题，使用全连接的CRF模型修复预测结果中的一些小的结构。能量函数为：</p>
<p><span class="math display">\[
E(x)=\sum_{i}{\theta_i(x_i)}+\sum_{ij}{\theta_{ij}(x_i, x_j)}
\]</span></p>
<p><img src="/20200708l1/image-20200709084713666.png"></p>
<h1 id="experiments">Experiments</h1>
<p><img src="/20200708l1/image-20200709084729187.png"></p>
<p><img src="/20200708l1/image-20200709084745073.png"></p>
<p><img src="/20200708l1/image-20200709084759185.png"></p>
<p><img src="/20200708l1/image-20200709084831731.png"></p>
]]></content>
      <categories>
        <category>阅读笔记</category>
      </categories>
      <tags>
        <tag>code</tag>
        <tag>DeepLab</tag>
        <tag>2015</tag>
        <tag>ICLR</tag>
        <tag>加利福尼亚大学</tag>
      </tags>
  </entry>
  <entry>
    <title>【2018 ECCV】DeepLabv3+:Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation</title>
    <url>/20200707l1/</url>
    <content><![CDATA[<p>Code: <a href="https://github.com/tensorflow/models/tree/master/research/deeplab" target="_blank" rel="external nofollow noopener noreferrer">https://github.com/tensorflow/models/tree/master/research/deeplab</a></p>
<h1 id="motivation">Motivation</h1>
<p>在语义分割任务中，spatial pyramid pooling module（SPP）可以捕获更多尺度信息，encoder-decoder结构可以更好恢复物体的边缘信息。</p>
<p><img src="/20200707l1/image-20200709104913290.png"></p>
<p>将DeepLab v3作为Encoder，添加新的Decoder，得到DeepLab v3+</p>
<h1 id="methods">Methods</h1>
<p><img src="/20200707l1/image-20200709105003493.png"></p>
<h2 id="encoder-decoder-with-atrous-convolution">Encoder-Decoder with Atrous Convolution</h2>
<h3 id="encoder-decoder-with-atrous-convolution-1">Encoder-Decoder with Atrous Convolution</h3>
<p><span class="math display">\[
y[i]=\sum_{k=1}^{K}{x[i+r\cdot k]w[k]}
\]</span></p>
<h3 id="depthwise-separable-convolution">Depthwise separable convolution</h3>
<p>将卷积替换为空洞可分离卷积，在保持性能的同时，降低模型的计算复杂度。</p>
<p>Depthwise separable convolution = depthwise convolution + pointwise convolution</p>
<p><img src="/20200707l1/image-20200709105532171.png"></p>
<h3 id="modified-aligned-xception">Modified Aligned Xception</h3>
<p><img src="/20200707l1/image-20200709105846543.png"></p>
<h1 id="experiments">Experiments</h1>
<p><img src="/20200707l1/image-20200709105914625.png"></p>
<p><img src="/20200707l1/image-20200709105928960.png"></p>
<p><img src="/20200707l1/image-20200709105953953.png"></p>
]]></content>
      <categories>
        <category>阅读笔记</category>
      </categories>
      <tags>
        <tag>2018</tag>
        <tag>code</tag>
        <tag>ECCV</tag>
        <tag>DeepLab</tag>
        <tag>谷歌</tag>
      </tags>
  </entry>
  <entry>
    <title>【2017】DeepLabv3:Rethinking Atrous Convolution for Semantic Image Segmentation</title>
    <url>/20200709l2/</url>
    <content><![CDATA[<p>Code: <a href="https://github.com/eveningdong/DeepLabV3-Tensorflow" target="_blank" rel="external nofollow noopener noreferrer">https://github.com/eveningdong/DeepLabV3-Tensorflow</a></p>
<h1 id="motivation">Motivation</h1>
<p>为了处理在多个尺度上分割对象的问题，本文设计了atrous卷积模块，该模块采用级联或并行的atrous卷积，通过采用多个atrous速率来捕获多尺度上下文。</p>
<ul>
<li>使用空洞卷积，防止分辨率过低情况</li>
<li>串联不同膨胀率的空洞卷积或者并行不同膨胀率的空洞卷积（v2的ASPP），来获取更多上下文信息</li>
</ul>
<p><img src="/20200709l2/image-20200709102519424.png"></p>
<p><img src="/20200709l2/image-20200709102546058.png"></p>
<h1 id="methods">Methods</h1>
<h2 id="atrous-convolution-for-dense-feature-extraction">Atrous Convolution for Dense Feature Extraction</h2>
<p>同v2中的空洞卷积，卷积计算公式为：</p>
<p><span class="math display">\[
y[i]=\sum_{k=1}^{K}{x[i+r\cdot k]w[k]}
\]</span></p>
<h2 id="going-deeper-with-atrous-convolution">Going Deeper with Atrous Convolution</h2>
<p>采用空洞卷积，将多个ResNet Block级联排列。</p>
<p><img src="/20200709l2/image-20200709103023997.png"></p>
<h3 id="multi-grid-method">Multi-grid Method</h3>
<p>每个Block中的三个卷积使用各自的Unit Rate。</p>
<h2 id="atrous-spatial-pyramid-pooling">Atrous Spatial Pyramid Pooling</h2>
<p>不同atrous rates的ASPP可以有效捕获到多尺度信息。但随着采样率变大，有效过滤器权重的数量会越来越少。</p>
<p><img src="/20200709l2/image-20200709103805802.png"></p>
<p>为了克服上述问题，并将全局上下文信息纳入模型，采用图像级特征。在模型最后一个特征图上应用全局平均池化，将得到的图像级特征放入<span class="math inline">\(1 \times 1\)</span>卷积中，然后双线性插值上采样至所需空间尺寸。</p>
<p><img src="/20200709l2/image-20200709104325142.png"></p>
<h1 id="experiments">Experiments</h1>
<p><img src="/20200709l2/image-20200709104413826.png"></p>
<p><img src="/20200709l2/image-20200709104439976.png"></p>
]]></content>
      <categories>
        <category>阅读笔记</category>
      </categories>
      <tags>
        <tag>2017</tag>
        <tag>code</tag>
        <tag>DeepLab</tag>
        <tag>谷歌</tag>
      </tags>
  </entry>
  <entry>
    <title>【2019 CVPR】Dual Attention Network for Scene Segmentation</title>
    <url>/20200801l9/</url>
    <content><![CDATA[<p>Code: <a href="https://github.com/junfu1115/DANet/" target="_blank" rel="external nofollow noopener noreferrer">https://github.com/junfu1115/DANet/</a></p>
<p>加入注意力机制，同时对空间和通道进行。</p>
<p><img src="/20200801l9/image-20200420120019703.png"></p>
<p><img src="/20200801l9/image-20200420120052772.png"></p>
<p><img src="/20200801l9/image-20200420120101963.png"></p>
]]></content>
      <categories>
        <category>阅读笔记</category>
      </categories>
      <tags>
        <tag>图像分割</tag>
        <tag>CVPR</tag>
        <tag>code</tag>
        <tag>2019</tag>
      </tags>
  </entry>
  <entry>
    <title>【2019 ICML】EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks</title>
    <url>/20200706l1/</url>
    <content><![CDATA[<p>Code: <a href="https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet" target="_blank" rel="external nofollow noopener noreferrer">https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet</a></p>
<h1 id="motivation">Motivation</h1>
<ul>
<li>系统研究了缩放模型，发现平衡好网络的深度、宽度和输入数据的分辨率可以带来更好的性能。</li>
<li>使用网络结构搜索设计了新的baseline网络，并扩展获得EfficientNet，比之前的卷积网络有更好的准确性和效率。</li>
</ul>
<p><img src="/20200706l1/image-20200706155430777.png"></p>
<p><img src="/20200706l1/image-20200706155556921.png"></p>
<h1 id="methods">Methods</h1>
<h2 id="problem-formulation">Problem Formulation</h2>
<p>定义普通卷积网络为：</p>
<p><span class="math display">\[
\mathcal{N}=\bigodot_{i=1 \ldots s} \mathcal{F}_{i}^{L_{i}}\left(X_{\left\langle H_{i}, W_{i}, C_{i}\right\rangle}\right)
\]</span></p>
<p>为了进一步减小设计空间，限制所有层必须以恒定的比例均匀缩放。本文的目标是在任何给定的资源约束条件下最大化模型准确性，这可以表述为优化问题:</p>
<p><span class="math display">\[
\begin{array}{ll}
\max _{d, w, r} &amp; \text {Accuracy}(\mathcal{N}(d, w, r)) \\
\text {s.t.} &amp; \mathcal{N}(d, w, r)=\bigodot_{i=1 \ldots s} \hat{\mathcal{F}}_{i}^{d \cdot \hat{L}_{i}}\left(X_{\left\langle r \cdot \hat{H}_{i}, r \cdot \hat{W}_{i}, w \cdot \hat{C}_{i}\right\rangle}\right) \\
&amp; \operatorname{Memory}(\mathcal{N}) \leq \text { target_memory } \\
&amp; \operatorname{FLOPS}(\mathcal{N}) \leq \text { target_flops }
\end{array}
\]</span></p>
<h2 id="scaling-dimensions">Scaling Dimensions</h2>
<p>经分析，增大网络的宽度、深度和分辨率都可以给网络带来提升，但随着增大其增益逐渐减小。（如：ResNet-1000的准确度和ResNet-101的准确率相近）</p>
<p><img src="/20200706l1/image-20200706160309728.png"></p>
<h2 id="观察">观察</h2>
<ul>
<li>扩大网络宽度，深度或分辨率的任何维度都可以提高准确性，但是对于较大的模型，准确性的增益会降低。</li>
<li>为了追求更好的准确性和效率，在ConvNet扩展过程中平衡网络宽度，深度和分辨率的所有维度至关重要。</li>
</ul>
<h2 id="compound-scaling">Compound Scaling</h2>
<p>对于更高分辨率的图像，应该增加网络深度，以使较大的接收场可以帮助捕获相似的特征，这些特征在较大的图像中包括更多的像素。</p>
<p>需要协调和平衡不同的缩放比例，而不是常规的一维缩放比例。</p>
<p><img src="/20200706l1/image-20200706160711710.png"></p>
<p>在本文中，提出了一种新的复合缩放方法，该方法使用复合系数<span class="math inline">\(\phi\)</span>原则上均匀地缩放网络的宽度，深度和分辨率。</p>
<p><span class="math display">\[
depth: d = \alpha^\phi  \\
width: w = \beta^\phi  \\
resolution: r = \gamma^\phi  \\
s.t. \alpha \cdot \beta^2 \cdot \gamma^2 \approx 2,
\alpha \geq 1, \beta \geq 1, \gamma \geq 1
\]</span></p>
<h2 id="efficientnet-architecture">EfficientNet Architecture</h2>
<p><img src="/20200706l1/image-20200706161926434.png"></p>
<p>从基线EfficientNet-B0开始，我们应用复合缩放方法通过两个步骤对其进行扩展：</p>
<ul>
<li>将<span class="math inline">\(\phi = 1\)</span>，如果资源的两倍空间可用，则通过小网格搜索<span class="math inline">\(\alpha,\beta,\gamma\)</span>。发现最好的结果为EfficientNet-B0，<span class="math inline">\(\alpha=1.2,\beta=1.1,\gamma=1.15\)</span>，其中<span class="math inline">\(\alpha \cdot \beta^2 \cdot \gamma^2 \approx 2\)</span>。</li>
<li>固定<span class="math inline">\(\alpha,\beta,\gamma\)</span>，缩放不同的<span class="math inline">\(\phi\)</span>，得到EfficientNetB1-B7。</li>
</ul>
<h1 id="experiments">Experiments</h1>
<p><img src="/20200706l1/image-20200706162417088.png"></p>
<p><img src="/20200706l1/image-20200706162434145.png"></p>
<p><img src="/20200706l1/image-20200706162449048.png"></p>
<p><img src="/20200706l1/image-20200706162503418.png"></p>
<p><img src="/20200706l1/image-20200706162520225.png"></p>
<p><img src="/20200706l1/image-20200706162539148.png"></p>
<p><img src="/20200706l1/image-20200706162553755.png"></p>
<p><img src="/20200706l1/image-20200706162605961.png"></p>
<p><img src="/20200706l1/image-20200706162651082.png"></p>
<p><img src="/20200706l1/image-20200706162634345.png"></p>
]]></content>
      <categories>
        <category>阅读笔记</category>
      </categories>
      <tags>
        <tag>code</tag>
        <tag>2019</tag>
        <tag>谷歌</tag>
        <tag>ICML</tag>
        <tag>网络结构搜索</tag>
      </tags>
  </entry>
  <entry>
    <title>【2020 ECCV】Feature Pyramid Transformer</title>
    <url>/20200805l2/</url>
    <content><![CDATA[<p>Code: <a href="https://github.com/ZHANGDONG-NJUST" target="_blank" rel="external nofollow noopener noreferrer">https://github.com/ZHANGDONG-NJUST</a></p>
<h1 id="motivation">Motivation</h1>
<p>受特征金字塔的发展变化提出本文方法，可以应用于多种任务。</p>
<p><img src="/20200805l2/image-20200805210036448.png"></p>
<h1 id="methods">Methods</h1>
<p><img src="/20200805l2/image-20200805210246785.png"></p>
<p><img src="/20200805l2/image-20200805210345717.png"></p>
<h1 id="experiments">Experiments</h1>
<p><img src="/20200805l2/image-20200805210430045.png"></p>
<p><img src="/20200805l2/image-20200805210444035.png"></p>
<p><img src="/20200805l2/image-20200805210503363.png"></p>
<p><img src="/20200805l2/image-20200805210520938.png"></p>
<p><img src="/20200805l2/image-20200805210535400.png"></p>
]]></content>
      <categories>
        <category>阅读笔记</category>
      </categories>
      <tags>
        <tag>2020</tag>
        <tag>code</tag>
        <tag>ECCV</tag>
        <tag>backbone</tag>
        <tag>南京理工大学</tag>
        <tag>阿里巴巴达摩院</tag>
      </tags>
  </entry>
  <entry>
    <title>【2020 CMPB】Hybrid Attention for Automatic Segmentation of Whole Fetal Head in Prenatal Ultrasound Volumes</title>
    <url>/20200801l6/</url>
    <content><![CDATA[<p>Accepted by Computer Methods and Programs in Biomedicine</p>
<p>将整个数据以0.4进行降采样，再作为输入。</p>
<p>加入深度监督和注意力模块。</p>
<p><img src="/20200801l6/image-20200510124533489.png"></p>
<p><img src="/20200801l6/image-20200510124602869.png"></p>
<p><img src="/20200801l6/image-20200510124632480.png"></p>
]]></content>
      <categories>
        <category>阅读笔记</category>
      </categories>
      <tags>
        <tag>2020</tag>
        <tag>图像分割</tag>
        <tag>CMPB</tag>
      </tags>
  </entry>
  <entry>
    <title>【2020 CVPR】FOAL: Fast Online Adaptive Learning for cardiac motion estimation</title>
    <url>/20200702l1/</url>
    <content><![CDATA[<h1 id="motivation">Motivation</h1>
<ul>
<li>临床应用中，训练数据与测试数据不匹配，性能急剧下降</li>
<li>在部署应用前不可能收集到所有代表性的数据集用于训练模型</li>
</ul>
<p>提出的框架包含一个在线自适应阶段和一个离线元学习阶段。</p>
<p>本文贡献：</p>
<ul>
<li>提出了一种新颖的在线模型自适应方法，将一个训练好的baseline模型适应于新视频。</li>
<li>提出一种元学习方法提升在线优化器。</li>
<li>提出用于在密集运动估计任务中训练元学习的实用解决方案。</li>
<li>本文方法不限于运动估计任务。</li>
</ul>
<p>本文采用了与参考文献22类似的self-supervision思想。</p>
<p>通常很难获得真正的心脏运动，因此使用分割蒙版对上述工作进行了定量评估。 在这项工作中，我们也使用这种类型的评估。</p>
<p><img src="/20200702l1/image-20200703111837160.png"></p>
<h1 id="methods">Methods</h1>
<p><img src="/20200702l1/image-20200703112008806.png"></p>
<h2 id="dense-motion-tracking">Dense Motion Tracking</h2>
<p>整体思想是一种从参考文献22中得到启发的端到端无监督学习方法。</p>
<p>为了进行无监督学习，空间变换器网络[13]用于将源图像变形/扭曲为参考图像，并使用图像重建损失<span class="math inline">\(L_{mse}\)</span>来最小化扭曲的源图像和参考图像之间的差异。</p>
<p><span class="math inline">\(L_{mse}\)</span>是均方误差（MSE）；<span class="math inline">\(L_{smooth}\)</span>是参考文献26中提出的运动域平滑损失，用于避免突然的运动变化；<span class="math inline">\(L_{con}\)</span>是文献22中提出的双向（前后）流一致性损失。</p>
<p><span class="math display">\[L_{total} = L_{mse} + \alpha_sL_{smooth} + \beta_cL_{con}\]</span></p>
<h2 id="online-optimizer">Online Optimizer</h2>
<p>在提出dense tracking的背景下，扩展了tracker以解决数据集分布不匹配的问题。</p>
<p>背后的思想是使给定的tracker能够自动适应新的视频<span class="math inline">\(x\)</span>。</p>
<p><img src="/20200702l1/image-20200703135434138.png"></p>
<p>通过随机梯度下降更新参数，梯度下降步骤是在在线阶段对网络的所有参数执行的。因此，太多的步骤在所有图像对（源和参考）上对它们进行优化在计算上是相当昂贵的。本文目标是仅使用少量的在线样本就可以在少数几个步骤中适应离线模型。通过元学习来优化此过程。（该idea来自MAML【参考文献7】）</p>
<h2 id="meta-learning">Meta-learning</h2>
<p>元学习更新策略：</p>
<p><span class="math display">\[\theta \leftarrow \theta-\beta \nabla_{\theta} \frac{1}{N} \sum_{i}^{N} L_{i}\left(f_{\theta_{i}^{\prime}}\right)\]</span></p>
<p><img src="/20200702l1/image-20200703143808146.png"></p>
<h2 id="practical-version-of-the-meta-learning">Practical Version of the Meta-Learning</h2>
<h3 id="memory-limitation-and-solution">Memory limitation and solution</h3>
<p>Dense motion tracker需要较大尺寸的图像，元学习的优化器需要计算与特定视频相关联的每个独立模型的导数。未解决这个问题，通过利用等式中的梯度算子和平均算子是可交换的性质，交换两个运算符，如下所示：</p>
<p><span class="math display">\[\nabla_{\theta} \frac{1}{N} \sum_{i}^{N} L_{i}\left(f_{\theta_{i}^{\prime}}\right) \Leftrightarrow \frac{1}{N} \sum_{i}^{N} \nabla_{\theta} L_{i}\left(f_{\theta_{i}^{\prime}}\right)\]</span></p>
<h3 id="first-order-derivative-approximation">First order derivative approximation</h3>
<p>上式中需要二阶导数，涉及到计算二阶Hessian矩阵，这在计算上是昂贵的。作为解决方法，本文使用一阶近似，其有效性在MAML中得到了证明[7]。</p>
<h1 id="experiments">Experiments</h1>
<p><img src="/20200702l1/image-20200703144556934.png"></p>
<p><img src="/20200702l1/image-20200703144608830.png"></p>
<p><img src="/20200702l1/image-20200703144540889.png"></p>
<p><img src="/20200702l1/image-20200703144627503.png"></p>
<p><img src="/20200702l1/image-20200703144646182.png"></p>
<p><img src="/20200702l1/image-20200703144702220.png"></p>
]]></content>
      <categories>
        <category>阅读笔记</category>
      </categories>
      <tags>
        <tag>2020</tag>
        <tag>CVPR</tag>
        <tag>元学习</tag>
        <tag>无监督学习</tag>
        <tag>剑桥大学</tag>
      </tags>
  </entry>
  <entry>
    <title>【2020 CVPR】FocalMix: Semi-Supervised Learning for 3D Medical Image Detection</title>
    <url>/20200701l1/</url>
    <content><![CDATA[<h1 id="motivation">Motivation</h1>
<p>作者认为医学影像的成功，不仅归因于深度学习的最新进展，而且归因于大量经过仔细注释的数据。</p>
<ul>
<li>标注医学影像数据非常昂贵且十分耗时</li>
<li>医院信息系统中存储大量未处理过的医学影像</li>
</ul>
<p>如何能够在不加标注的情况下利用这些原始医学影像来提升深度学习模型。</p>
<p>本文贡献：</p>
<ul>
<li>提出了一个新颖的3D医疗影像半监督学习检测框架FocalMix</li>
<li>本文工作最先将半监督学习应用于医学图像检测领域</li>
<li>通过广泛实验，证明了本文方法的有效性</li>
</ul>
<p><img src="/20200701l1/image-20200701220851655.png"></p>
<h1 id="methods">Methods</h1>
<p>MixMatch框架主要包括两个模块：</p>
<ul>
<li>target prediction</li>
<li>MixUp augmentation</li>
</ul>
<p><img src="/20200701l1/image-20200701221231901.png"></p>
<h2 id="soft-target-focal-loss">Soft-target Focal Loss</h2>
<p>半监督学习通常涉及soft training targets问题。</p>
<p>提出的soft-target focal loss为</p>
<p><span class="math display">\[SFL(p)=[\alpha_0+y(\alpha_1-\alpha_0)]\cdot|y-p|^\gamma \cdot CE(y,p)\]</span></p>
<h2 id="anchor-level-target-prediction">Anchor-level Target Prediction</h2>
<h2 id="mixup-augmentation-for-detection">MixUp Augmentation for Detection</h2>
<p>MixUp增强是MixMatch框架中的重要组件，它鼓励模型在训练示例之间线性表现，以实现更好的泛化性能。</p>
<p>本文介绍了两种适用于医学图像中病灶检测的MixUp方法：图像级MixUp和对象级MixUp。</p>
<h1 id="experiments">Experiments</h1>
<p><img src="/20200701l1/image-20200701230247513.png"></p>
<p><img src="/20200701l1/image-20200701230259874.png"></p>
<p><img src="/20200701l1/image-20200701230309856.png"></p>
<p><img src="/20200701l1/image-20200701230320644.png"></p>
<p><img src="/20200701l1/image-20200701230339667.png"></p>
]]></content>
      <categories>
        <category>阅读笔记</category>
      </categories>
      <tags>
        <tag>2020</tag>
        <tag>CVPR</tag>
        <tag>半监督学习</tag>
        <tag>目标检测</tag>
        <tag>北京大学</tag>
        <tag>医准智能</tag>
      </tags>
  </entry>
  <entry>
    <title>【2019 MICCAI】Improving RetinaNet for CT Lesion Detection with Dense Masks from Weak RECIST Labels</title>
    <url>/20200619l1/</url>
    <content><![CDATA[<h1 id="motivation">Motivation</h1>
<ul>
<li>目前的CT病变检测都是采用的两阶段的方法，使用centroid或bbox标注</li>
<li>现有方法对小目标检测能力较弱</li>
</ul>
<h2 id="faster-r-cnn">Faster R-CNN</h2>
<ul>
<li>两阶段检测</li>
<li>检测速度慢</li>
</ul>
<p><img src="/20200619l1/image-20200619111304812.png"></p>
<h2 id="dce">3DCE</h2>
<ul>
<li>小病灶的检测效果不好</li>
</ul>
<p><img src="/20200619l1/image-20200619111352772.png"></p>
<h2 id="retinanet">RetinaNet</h2>
<p>ResNet+FPN作为backbone，再利用单级的目标识别法+Focal Loss。</p>
<p>本文在RetinaNet的基础上进行改进。</p>
<p><img src="/20200619l1/image-20200619111444428.png"></p>
<h1 id="method">Method</h1>
<p>改进之处：</p>
<ul>
<li><p>Optimized anchor configuration；摒弃默认的anchor配置，采用differential evolution search algorithm来找合适的anchor配置。</p></li>
<li><p>Dense mark supervision</p></li>
<li><p>Attention gate</p></li>
</ul>
<h2 id="model-design">Model Design</h2>
<ul>
<li><p>主干网络：RetinaNet</p></li>
<li><p>网络结构使用VGG19，尝试使用Resnet50，效果变差</p></li>
<li><p>采用了特征金字塔，利于检测小目标</p></li>
<li><p>使用Focal Loss，解决样本不平衡问题</p></li>
<li><p>采用差分进化算法选取最优锚点</p></li>
</ul>
<p><img src="/20200619l1/image-20200619111759526.png"></p>
<h2 id="weak-recist-labels">Weak RECIST Labels</h2>
<ul>
<li>大多数病变存在凸轮廓线</li>
<li>Deep Lesion提供病变区域的直径</li>
<li>基于GrubCut方法生成Mask标签</li>
</ul>
<h1 id="experiments">Experiments</h1>
<p>lesion detection sensitivities at different false positives (FP) per image</p>
<p><img src="/20200619l1/image-20200619112424396.png"></p>
<p><img src="/20200619l1/image-20200619112339682.png"></p>
<p><img src="/20200619l1/image-20200619112444858.png"></p>
<h1 id="几个小问题">几个小问题</h1>
<ul>
<li>作者在文章中提到：网络主干结构采用了VGG-19，也尝试了ResNet-50，但是效果却变差了。这是为什么？</li>
<li>Attention 现在非常流行，但在分割任务中还没有看到<strong>solid的paper</strong>能证明加入attention module能<strong>显著提升性能</strong>，反例倒是有（e.g., NVIDIA 在BraTS 2018中的Slide）。</li>
<li>有没有可能直接进行分割来代替目标检测？idea不是空穴来风，参见<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1904.13300" rel="external nofollow noopener noreferrer" target="_blank">Segmentation is All You Need</a>。</li>
</ul>
<h1 id="参考文献">参考文献</h1>
<ol type="1">
<li><a href="https://zhuanlan.zhihu.com/p/72278969" target="_blank" rel="external nofollow noopener noreferrer">MICCAI 2019-CT病灶检测:与AI一教高下</a></li>
</ol>
]]></content>
      <categories>
        <category>阅读笔记</category>
      </categories>
      <tags>
        <tag>MICCAI</tag>
        <tag>2019</tag>
        <tag>目标检测</tag>
      </tags>
  </entry>
  <entry>
    <title>【2020 MICCAI】Instance-aware Self-supervised Learning for Nuclei Segmentation</title>
    <url>/20200728l1/</url>
    <content><![CDATA[<h1 id="motivation">Motivation</h1>
<ul>
<li>病理图像中细胞核分布广泛，形态上存在巨大差异</li>
<li>标注工作量大</li>
<li>自监督学习可以缓解数据量不足的问题，分为两个阶段
<ul>
<li>在大型无标注数据集上预训练网络模型</li>
<li>在特定小型有标注数据集上微调预训练模型</li>
</ul></li>
</ul>
<h1 id="methods">Methods</h1>
<h2 id="image-manipulation">Image Manipulation</h2>
<p>先通过锚点生成一个patch，再随机裁剪一个同样大小的patch，且核数量相近的作为正样本，再从正样本中裁剪出一小块作为负样本。</p>
<p>锚点，正样本和负样本构成标准的三元组数据，该数据用于自我监督学习中的代理任务。</p>
<p><img src="/20200728l1/image-20200728091137247.png"></p>
<h2 id="self-supervised-approach-with-triplet-learning-and-ranking">Self-supervised Approach with Triplet Learning and Ranking</h2>
<p>分为两部分：Triplet Learning和Ranking</p>
<p>下图中三个Encoder共享权重，得到的三个特征向量均为128-d</p>
<p><img src="/20200728l1/image-20200728091215708.png"></p>
<h3 id="scale-wise-triplet-learning">Scale-wise Triplet Learning</h3>
<p>Anchor、正样本、负样本三者之间存在的scale和核数量不同，通过triplet learning学习三者之间的关系。 <span class="math display">\[
\mathcal{L}_{S T}\left(z_{a}, z_{p}, z_{n}\right)=\sum \max \left(0, d\left(z_{a}, z_{p}\right)-d\left(z_{a}, z_{n}\right)+m_{1}\right)
\]</span></p>
<h3 id="count-ranking">Count Ranking</h3>
<p>正样本比负样本存在更多的核数量，通过pair-wise count ranking loss强制网络识别包含更大核数量的样例。 <span class="math display">\[
\mathcal{L}_{CR} = \sum{max(0, f(z_n)-f(z_p)+m_2)}
\]</span></p>
<h3 id="目标函数">目标函数</h3>
<p><span class="math display">\[
\mathcal{L} = \mathcal{L}_{ST} + \mathcal{L}_{CR}
\]</span></p>
<h2 id="fine-tuning-on-target-task">Fine-tuning on Target Task</h2>
<p>采用现有的单阶段核实例分割框架用于实例分割，主干网络采用ResNet-101，最终框架可以称为ResUNet-101。</p>
<p>将模型的Encoder通过上文提及的自监督方法进行预训练，然后加入随机初始化的Decoder，在目标任务上进行微调预训练模型。</p>
<h1 id="experiments">Experiments</h1>
<p><img src="/20200728l1/image-20200728092745547.png"></p>
<p><img src="/20200728l1/image-20200728092759626.png"></p>
<p><img src="/20200728l1/image-20200728092809137.png"></p>
<p><img src="/20200728l1/image-20200728092827794.png"></p>
]]></content>
      <categories>
        <category>阅读笔记</category>
      </categories>
      <tags>
        <tag>2020</tag>
        <tag>MICCAI</tag>
        <tag>腾讯</tag>
        <tag>图像分割</tag>
        <tag>自监督学习</tag>
        <tag>深圳大学</tag>
      </tags>
  </entry>
  <entry>
    <title>【2020 MICCAI】Learning Directional Feature Maps for Cardiac MRI Segmentation</title>
    <url>/20200727l1/</url>
    <content><![CDATA[<p>Code: <a href="https://github.com/c-feng/DirectionalFeature" target="_blank" rel="external nofollow noopener noreferrer">https://github.com/c-feng/DirectionalFeature</a></p>
<h1 id="motivation">Motivation</h1>
<p>心脏MRI边界不清晰，强度分布不均匀，类间模糊，类内不一致。</p>
<p>nnUNet虽然有State-of-the-art性能，但需要消耗大量内存和计算量。</p>
<p>本文提出在方向信息指导下对原始分割特征进行改进，以更好实现对心脏MRI的分割。</p>
<p><img src="/20200727l1/image-20200727150257605.png"></p>
<h1 id="methods">Methods</h1>
<p>提出了一种简单而有效的方法来利用像素之间的方向关系，它可以同时增强类之间的差异和类内的相似性。</p>
<p><img src="/20200727l1/image-20200727150542488.png"></p>
<h2 id="df-module-to-learn-a-direction-field">DF Module to Learn a Direction Field</h2>
<p>前景元素计算像素点到边界点的单位向量，背景设定为0。 <span class="math display">\[
D F(p)=\left\{\begin{array}{ll}
\frac{\overrightarrow{b p}}{|\overrightarrow{b p}|} &amp; p \in \text { foreground } \\
(0,0) &amp; \text { otherwise. }
\end{array}\right.
\]</span></p>
<h2 id="frf-module-for-feature-rectifification-and-fusion">FRF Module for Feature Rectifification and Fusion</h2>
<p>在方向向量的指导下，提出了一个特征校正与融合（FRF）模块，利用中心区域的特性，逐步校正初始分割特征图中的错误。 <span class="math display">\[
\forall p \in \Omega, F^{k}(p)=F^{(k-1)}\left(p_{x}+D F(p)_{x}, p_{y}+D F(p)_{y}\right)
\]</span> <img src="/20200727l1/image-20200727150935272.png"></p>
<h2 id="training-objective">Training Objective</h2>
<p><span class="math display">\[
L_{D F}=\sum_{p \in \Omega} w(p)\left(\|D F(p)-\hat{D} F(p)\|_{2}+\alpha \times\left\|\cos ^{-1}\langle D F(p), \hat{D} F(p)\rangle\right\|^{2}\right)
\]</span></p>
<p><span class="math display">\[
L=L_{CE}^i+L_{CE}^{f}+\lambda L_{DF}
\]</span></p>
<h1 id="experiments">Experiments</h1>
<p><img src="/20200727l1/image-20200727151413179.png"></p>
<p><img src="/20200727l1/image-20200727151449233.png"></p>
<p><img src="/20200727l1/image-20200727151505605.png"></p>
]]></content>
      <categories>
        <category>阅读笔记</category>
      </categories>
      <tags>
        <tag>2020</tag>
        <tag>MICCAI</tag>
        <tag>图像分割</tag>
        <tag>code</tag>
      </tags>
  </entry>
  <entry>
    <title>【2020 MICCAI】Learning Semantics-enriched Representation via Self-discovery, Self-classification, and Self-restoration</title>
    <url>/20200805l1/</url>
    <content><![CDATA[<p>Code: <a href="https://github.com/JLiangLab/SemanticGenesis" target="_blank" rel="external nofollow noopener noreferrer">https://github.com/JLiangLab/SemanticGenesis</a></p>
<h1 id="motivation">Motivation</h1>
<p>自我监督学习中的一个关键问题是如何直接从未标记的数据中“提取”适当的监督信号。</p>
<h1 id="methods">Methods</h1>
<p>整体框架分为三部分：</p>
<ul>
<li>self-discovery of anatomical patterns from similar patients</li>
<li>self-classifification of the patterns</li>
<li>self-restoration of the transformed patterns.</li>
</ul>
<p><img src="/20200805l1/image-20200805103106974.png"></p>
<h2 id="self-discovery-of-anatomical-patterns">Self-discovery of anatomical patterns</h2>
<p>在2D/3D图像中裁剪出C个随机但固定坐标的patches/cubes，用于学习它们之间的相似性。</p>
<p>由于坐标系是在参考病人中随机选择的，一些解剖模式对放射科医生来说可能不是很有意义，但这些模式仍然与人体丰富的局部语义联系在一起。</p>
<h2 id="self-classifification-of-anatomical-patterns">Self-classifification of anatomical patterns</h2>
<p>在降采样最低端接入一系列全连接层实现图像分类任务。</p>
<p>其目标是鼓励模型从患者图像中反复出现的解剖模式中学习，从而学习一种深层语义丰富的表达方式。</p>
<h2 id="self-restoration-of-anatomical-patterns">Self-restoration of anatomical patterns</h2>
<p>自我恢复的目的是使模型通过从转换后的模式中恢复原始的解剖模式来学习不同的视觉表示集。</p>
<p>对数据进行一系列增强操作，如non-linear, local-shuffling, out-painting, in-painting，使得图像恢复出原图。</p>
<h1 id="experiments">Experiments</h1>
<p><img src="/20200805l1/image-20200805103932323.png"></p>
<p><img src="/20200805l1/image-20200805103948003.png"></p>
]]></content>
      <categories>
        <category>阅读笔记</category>
      </categories>
      <tags>
        <tag>2020</tag>
        <tag>MICCAI</tag>
        <tag>code</tag>
      </tags>
  </entry>
  <entry>
    <title>【2020】Medical Image Segmentation Using a U-Net type of Architecture</title>
    <url>/20200801l7/</url>
    <content><![CDATA[<p>在Encoder与Decoder之间加入监督信息，应用于CT和MRI。</p>
<p>降低输入图像尺寸，增大channel，想要获得更多信息。</p>
<p>​ <img src="/20200801l7/image-20200516170336554.png" alt="image-20200516170336554"></p>
]]></content>
      <categories>
        <category>阅读笔记</category>
      </categories>
      <tags>
        <tag>2020</tag>
        <tag>图像分割</tag>
      </tags>
  </entry>
  <entry>
    <title>【2020 ECCV】Mining Cross-Image Semantics for Weakly Supervised Semantic Segmentation</title>
    <url>/20200711l1/</url>
    <content><![CDATA[<p>Code: <a href="https://github.com/GuoleiSun/MCIS_wsss" target="_blank" rel="external nofollow noopener noreferrer">https://github.com/GuoleiSun/MCIS_wsss</a></p>
<h1 id="motivation">Motivation</h1>
<p>本文研究仅从图像级标注的监督信息学习语义分割的问题。</p>
<p>弱监督语义分割，即只给定图像级分类标注的语义分割。当前流行的方法是利用分类器的目标定位图（object localization maps）作为监督信号。</p>
<p>这类方法很难使得定位图（ localization maps）捕获到更加完全的目标内容。</p>
<h2 id="本文贡献">本文贡献</h2>
<ul>
<li>讨论了跨图像语义相关性对于完整对象模式学习以及对象位置推断的价值，通过在配对训练样本上共同注意力分类器的工作实现的。</li>
<li>本文的共同注意力分类器以更全面的方式挖掘语义线索。还通过共同注意力和对比注意力机制从跨图像间挖掘相似性和差异性进行互补监督。</li>
<li>本文方法在弱监督语义分割上泛化性很好，可以应用于Web上爬虫得到的图像数据。</li>
</ul>
<h1 id="methods">Methods</h1>
<p>不同于之前的方法，只关注单张图像的信息，本文提出的方法挖掘不同图像之间的信息。</p>
<p>将注意力机制分为两种，co-attention和contrastive co-attention，前者从共同对象区域中学习相同的语义信息，后者注意在其他对象上，实现非共享的语义分类。</p>
<p><img src="/20200711l1/image-20200711192732553.png"></p>
<h1 id="experiments">Experiments</h1>
<p>本文方法在CVPR2020 LID workshop获得最佳论文，并在LID语义分割挑战赛道第一名。</p>
<p><img src="/20200711l1/image-20200711194040487.png"></p>
<p><img src="/20200711l1/image-20200711194104353.png"></p>
<p><img src="/20200711l1/image-20200711194117483.png"></p>
]]></content>
      <categories>
        <category>阅读笔记</category>
      </categories>
      <tags>
        <tag>2020</tag>
        <tag>弱监督学习</tag>
        <tag>code</tag>
        <tag>ECCV</tag>
        <tag>语义分割</tag>
        <tag>苏黎世联邦理工学院</tag>
      </tags>
  </entry>
  <entry>
    <title>【2019 ICCV】Learning a Mixture of Granularity-Specific Experts for Fine-Grained Categorization</title>
    <url>/20200723l1/</url>
    <content><![CDATA[<h1 id="motivation">Motivation</h1>
<p>细粒度任务中的对象通常共享较小的类间方差和较大的类内方差，以及多个对象范围和复杂的背景，从而导致更复杂的问题空间。</p>
<p>如果将此类数据进一步划分为子集进行训练，则由于可获得的数据量较少，每个生成的专家模型都更容易过度拟合。</p>
<p><img src="/20200723l1/image-20200727151845765.png"></p>
<h1 id="methods">Methods</h1>
<p>方法由多名专家和一个门控网络组成。</p>
<p><img src="/20200723l1/image-20200727151930340.png"></p>
<h2 id="experts-for-fine-grained-recognition">Experts for Fine-Grained Recognition</h2>
<p>每一个专家生成基于CAM的Attention Map，找出更细致的关注点，裁剪图像作为输入送给下一个专家模型。</p>
<p><img src="/20200723l1/image-20200727152036884.png"></p>
<h2 id="kl-divergence-based-penalizing-term">KL-Divergence based Penalizing Term</h2>
<p>基于KL散度的惩罚项，鼓励不同专家关注不同的点，产生不同的结果。 <span class="math display">\[
\begin{array}{c}
D_{K L}\left(P^{t} \| P^{t+1}\right)=\sum_{x \in X^{t}} P^{t}(x) \log \left(\frac{P^{t}(x)}{P^{t+1}(x)}\right) \\
=\sum_{x \in X^{t}}\left(P^{t}(x) \log \left(P^{t}(x)\right)-P^{t}(x) \log \left(P^{t+1}(x)\right)\right)
\end{array}
\]</span></p>
<h2 id="mixture-of-experts">Mixture of Experts</h2>
<p><span class="math display">\[
L=\sum_{t=1}^{T} L_{c l s}^{t}+\sum_{t=2}^{T} L_{K L}^{t}+L_{g a t e}
\]</span></p>
<p>将不同专家结果通过Gate网络的输出权重融合，生成最终结果。 <span class="math display">\[
\hat{y}_{i}=\sum_{t=1}^{T} g_{t} * \hat{y}_{i}^{t}
\]</span></p>
<h1 id="experiments">Experiments</h1>
<p><img src="/20200723l1/image-20200727152922353.png"></p>
<p><img src="/20200723l1/image-20200727152951699.png"></p>
<p><img src="/20200723l1/image-20200727153008944.png"></p>
<p><img src="/20200723l1/image-20200727153021753.png"></p>
]]></content>
      <categories>
        <category>阅读笔记</category>
      </categories>
      <tags>
        <tag>ICCV</tag>
        <tag>2019</tag>
        <tag>细粒度分类</tag>
      </tags>
  </entry>
  <entry>
    <title>【2020 MICCAI】Multi-organ Segmentation via Co-training Weight-averaged Models from Few-organ Datasets</title>
    <url>/20200824l1/</url>
    <content><![CDATA[<h1 id="motivation">Motivation</h1>
<p>在同一幅图像上收集所有器官的完整注释通常是相当困难的，因为一些医学中心由于其自身的临床实践可能只注释了部分器官。</p>
<p>在大多数情况下，可以从一个训练集中获取单个或几个器官的注释，并从另一组训练图像中获取其他器官的注释。</p>
<p><img src="/20200824l1/image-20200824143746048.png"></p>
<p>一个直观的解决方案是使用训练数据集通过单独的模型分割每个器官。</p>
<p>这种方法不仅计算量高而且不能很好考虑不同器官之间的空间关系。</p>
<p>其他研究将不同数据集训练模型在其他数据集上生成伪标签，这种方法由于域不同容易产生大量噪声影响最终效果。</p>
<h1 id="methods">Methods</h1>
<p><img src="/20200824l1/image-20200824143837472.png"></p>
<h2 id="pre-training-single-organ-and-multi-organ-models">Pre-training single-organ and multi-organ models</h2>
<p>单独对每个数据集训练一个模型，然后在其他器官上预测结果作为伪标签。</p>
<h2 id="co-training-weight-averaged-models-for-pseudo-label-regularization">Co-training weight-averaged models for pseudo label regularization</h2>
<p>两个相同结构的网络分别使用伪标签数据集进行训练。</p>
<p>另外使用上一轮训练的两个模型生成软伪标签加到原始数据集上训练另外两组模型。</p>
<h1 id="experiments">Experiments</h1>
<p><img src="/20200824l1/image-20200824144425855.png"></p>
<p><img src="/20200824l1/image-20200824144446723.png"></p>
]]></content>
      <categories>
        <category>阅读笔记</category>
      </categories>
      <tags>
        <tag>2020</tag>
        <tag>MICCAI</tag>
        <tag>图像分割</tag>
      </tags>
  </entry>
  <entry>
    <title>【2019 NeurIPS】MixMatch: A Holistic Approach to Semi-Supervised Learning</title>
    <url>/20200715l1/</url>
    <content><![CDATA[<p>Code: <a href="https://github.com/google-research/mixmatch" target="_blank" rel="external nofollow noopener noreferrer">https://github.com/google-research/mixmatch</a></p>
<h1 id="motivation">Motivation</h1>
<p>深度学习的成功多半取决于大量的数据。</p>
<p>例如医疗数据中，很多数据是用非常昂贵的设备采集的，需要通过多名专家共同标注，非常耗时，而且存在隐私问题。</p>
<p>半监督学习（SSL）试图利用无标注的数据来减轻对有标数据的需求。很多SSL方法针对无标注数据增加损失项来使得模型很好的泛化到未见数据上。损失项可分为三类：熵最小化，一致性正则化和一般正则化。</p>
<h2 id="本文贡献">本文贡献</h2>
<ul>
<li>实验证明MixMatch取得了state-of-the-art成果</li>
<li>消融实验证明MixMatch将每个模块加和效果最好</li>
<li>MixMatch对于隐私学习很有效</li>
</ul>
<h1 id="methods">Methods</h1>
<p><img src="/20200715l1/image-20200715210313015.png"></p>
<p><img src="/20200715l1/image-20200715210333773.png"></p>
<p>MixMatch的损失定义如下： <span class="math display">\[
X^{&#39;},U^{&#39;}=MixMatch(X,U,T,K,\alpha)
\]</span></p>
<p><span class="math display">\[
L_X=\frac{1}{X^{&#39;}}\sum_{x,p\in X^{&#39;}}{H(p,p_{model}(y|x;\theta))}
\]</span></p>
<p><span class="math display">\[
L_{U}=\frac{1}{L|U&#39;|}\sum_{u,q\in U&#39;}{||q-p_{model}(y|u;\theta)||_2^2}
\]</span></p>
<p><span class="math display">\[
L=L_{X}+\lambda_U L_U
\]</span></p>
<p>其中，<span class="math inline">\(H(p, q)\)</span>表示分布p和q之间的交叉熵，<span class="math inline">\(T,K,\alpha,\lambda_U\)</span>为超参数。</p>
<h2 id="数据增强">数据增强</h2>
<p>数据增强是减轻缺少有标数据影响的一种方法。类似于大部分半监督学习方法，我们同时对有标和无标数据进行数据增强。对有标数据进行一次数据增强，无标数据进行K次数据增强。这些无标数据增强后得到的结果进行“laebl guessing”获得<span class="math inline">\(q_b\)</span>。</p>
<h2 id="label-guessing">Label Guessing</h2>
<p>对于单个无标样例，计算K次增强后类别预测分布的均值，这个得到的标签带入后续的无监督损失项中。 <span class="math display">\[
\bar{q}_{b}=\frac{1}{K} \sum_{k=1}^{K} \operatorname{pmodel}\left(y \mid \hat{u}_{b, k} ; \theta\right)
\]</span></p>
<h2 id="sharpening">Sharpening</h2>
<p>通过sharpening方法进行熵最小化处理 <span class="math display">\[
Sharpen(p, T):=\frac{P_i^{\frac{1}{T}}}{\sum_{j=1}^{L}{p_j^{\frac{1}{T}}}}
\]</span> 后续需要使用sharpen的输出作为模型预测的目标值，所以选择较低的T保证了模型可以产生低熵的预测。</p>
<h2 id="mixup">MixUp</h2>
<p>同时对有标数据和label guessing结果的无标数据进行MixUp。 <span class="math display">\[
\lambda \sim Beta(\alpha, \alpha)
\]</span></p>
<p><span class="math display">\[
\lambda&#39;=max(\lambda, 1-\lambda)
\]</span></p>
<p><span class="math display">\[
x&#39;=\lambda&#39;x_1+(1-\lambda&#39;)x_2
\]</span></p>
<p><span class="math display">\[
p&#39;=\lambda&#39; p_1 + (1-\lambda&#39;)p_2
\]</span></p>
<h1 id="experiments">Experiments</h1>
<p><img src="/20200715l1/image-20200716082254512.png"></p>
<p><img src="/20200715l1/image-20200716082312332.png"></p>
<p><img src="/20200715l1/image-20200716082341001.png"></p>
<p>消融实验，感觉主要起作用的还是MixUp。</p>
<p><img src="/20200715l1/image-20200716082359993.png"></p>
]]></content>
      <categories>
        <category>阅读笔记</category>
      </categories>
      <tags>
        <tag>半监督学习</tag>
        <tag>code</tag>
        <tag>2019</tag>
        <tag>谷歌</tag>
        <tag>NeurIPS</tag>
      </tags>
  </entry>
  <entry>
    <title>【2020 CVPR】PolarMask: Single Shot Instance Segmentation with Polar Representation</title>
    <url>/20200731l3/</url>
    <content><![CDATA[<p>Code: <a href="https://github.com/xieenze/PolarMask" target="_blank" rel="external nofollow noopener noreferrer">https://github.com/xieenze/PolarMask</a></p>
<h1 id="motivation">Motivation</h1>
<p>基于FCOS实现实例分割，使用极坐标表达实例的轮廓，进一步形成分割结果。</p>
<p>本文方法采用极坐标表示物体，有以下几点好处：</p>
<ul>
<li>极坐标的原点可以看作是物体的中心</li>
<li>从原点开始，轮廓上的点由距离和角度来确定</li>
<li>很方便将所有点连接成一个完整的轮廓</li>
</ul>
<p><img src="/20200731l3/image-20200731173013335.png"></p>
<h1 id="methods">Methods</h1>
<p>PolarMask由一个特征金字塔和两三个特定的heads组成，分别预测Mask、bbox、bbox的类别。</p>
<p><img src="/20200731l3/image-20200731173037090.png"></p>
<h2 id="polar-mask-segmentation">Polar Mask Segmentation</h2>
<p>物体由极坐标表示，物体中心作为原点，从原点向四周发出射线，每条射线夹角为<span class="math inline">\(\theta\)</span>度，所以物体轮廓上的点只需记录到原点的距离即可。</p>
<p><img src="/20200731l3/image-20200731201325262.png"></p>
<p>质心和bbox中心哪一个更好，作者也不确定，但实验表明，质心更好一些。解释为质心有更高概率落到物体上。</p>
<p><img src="/20200731l3/image-20200731201514497.png"></p>
<p><img src="/20200731l3/image-20200731203036123.png"></p>
<h2 id="polar-centerness">Polar Centerness</h2>
<p><span class="math display">\[
\text { Polar Centerness }=\sqrt{\frac{\min \left(\left\{d_{1}, d_{2}, \ldots, d_{n}\right\}\right)}{\max \left(\left\{d_{1}, d_{2}, \ldots, d_{n}\right\}\right)}}
\]</span></p>
<p><span class="math display">\[
Polar IoU Loss = log{\frac{\sum_{i=1}^{n}{d_{max}}}{\sum_{i=1}^{n}{d_{min}}}}
\]</span></p>
<p><img src="/20200731l3/image-20200731202551136.png"></p>
<h1 id="experiments">Experiments</h1>
<p><img src="/20200731l3/image-20200731202913800.png"></p>
<p><img src="/20200731l3/image-20200731202935763.png"></p>
<p><img src="/20200731l3/image-20200731202951481.png"></p>
<p><img src="/20200731l3/image-20200731203010184.png"></p>
]]></content>
      <categories>
        <category>阅读笔记</category>
      </categories>
      <tags>
        <tag>2020</tag>
        <tag>图像分割</tag>
        <tag>CVPR</tag>
        <tag>code</tag>
        <tag>香港大学</tag>
      </tags>
  </entry>
  <entry>
    <title>【2020 MICCAI】PraNet: Parallel Reverse Attention Network for Polyp Segmentation</title>
    <url>/20200628l1/</url>
    <content><![CDATA[<p>Code: <a href="https://github.com/DengPingFan/PraNet" target="_blank" rel="external nofollow noopener noreferrer">https://github.com/DengPingFan/PraNet</a></p>
<h1 id="motivation">Motivation</h1>
<p>息肉分割存在两个主要问题：</p>
<ul>
<li>相同类型的息肉具有不同的大小、颜色和质地</li>
<li>息肉与其周围粘膜之间的边界不清晰</li>
</ul>
<p>本文提出一种新颖的深度神经网络，称为并行反向注意力网络（PraNet）。模仿临床医生，首先粗略定位息肉，然后根据局部特征准确提取其Mask。此方法具有三个优点：</p>
<ul>
<li>更好的学习能力</li>
<li>增强的泛化能力</li>
<li>更高的训练效率</li>
</ul>
<p>本文贡献：</p>
<ul>
<li>提出了实时息肉分割框架PraNet</li>
<li>介绍了针对息肉分割的评价指标，提供SOTA模型</li>
<li>在五个数据集上都优于现有模型，并且能够实时推断、训练时间减少</li>
</ul>
<h1 id="methods">Methods</h1>
<p><img src="/20200628l1/image-20200628204135114.png"></p>
<h2 id="feature-aggregating-via-parallel-partial-decoder">Feature Aggregating via Parallel Partial Decoder</h2>
<p>比较流行的医学影像分割方法主要依靠U-Net或U-Net的变体，这些方法聚合多级特征，但与高级特征相比，低级特征有较大的空间分辨率而需要更多计算资源，对性能的贡献较小。</p>
<p>针对此问题，提出Parallel Partial Decoder结构。</p>
<ul>
<li>通过Res2Net-based backbone提取5级特征<span class="math inline">\(\{f_i, i=1,...,5\}\)</span></li>
<li>将特征分为低级特征<span class="math inline">\(\{f_i, i=1,2\}\)</span>和高级特征<span class="math inline">\(\{f_i, i=3,4,5\}\)</span></li>
<li>引入partial decoder <span class="math inline">\(p_d(\cdot)\)</span>，用于并行聚合高级特征。通过<span class="math inline">\(PD=p_d(f_3, f_4, f_5)\)</span>计算可以得到全局图<span class="math inline">\(S_g\)</span></li>
</ul>
<h2 id="reverse-attention-module">Reverse Attention Module</h2>
<p>临床阶段，医生首先粗略定位息肉区域，然后仔细找出息肉的精确位置。前文提到的全局图<span class="math inline">\(S_g\)</span>产生自最深层网络，只能获得息肉相对粗糙的位置，而没有结构信息。</p>
<p>提出一种基于擦除方式的策略逐步探索可判别的息肉区域，自适应的并行学习高级特征中的Reverse Attention。</p>
<p>通过将高级特征<span class="math inline">\(\{f_i, i=3,4,5\}\)</span>逐像素与Reverse Attention权重<span class="math inline">\(A_i\)</span>相乘得到Reverse Attention特征。</p>
<p><span class="math display">\[R_i=f_i \odot A_i\]</span></p>
<p><span class="math inline">\(A_i\)</span>主要用于salient object detection，可以通过下面公式计算得到</p>
<p><span class="math display">\[A_i=\ominus(\sigma(P(S_{i+1})))\]</span></p>
<p>其中，<span class="math inline">\(P(\cdot)\)</span>为上采样操作，<span class="math inline">\(\sigma(\cdot)\)</span>为Sigmoid函数，<span class="math inline">\(\ominus(\cdot)\)</span>为从矩阵E中减去输入的逆运算。</p>
<h2 id="learning-process-and-implementation-details">Learning Process and Implementation Details</h2>
<p><span class="math display">\[L=L_{IoU}^{w}+L_{BCE}^{w}\]</span></p>
<p>对IoU损失和BCE损失分别加权，有助于检测目标。</p>
<p>另外对三个side-outputs<span class="math inline">\((S_3, S_4,S_5)\)</span>进行Deep Supervision。</p>
<p><span class="math display">\[\mathcal{L}_{\text {total}}=\mathcal{L}\left(G, S_{g}^{u p}\right)+\sum_{i=3}^{i=5} \mathcal{L}\left(G, S_{i}^{u p}\right)\]</span></p>
<h1 id="experiments">Experiments</h1>
<h2 id="datasets">Datasets</h2>
<ul>
<li>ETIS</li>
<li>CVC-ClinicDB/CVC-612</li>
<li>CVC-ColonDB</li>
<li>EndoScene</li>
<li>Kvasir</li>
</ul>
<p>对比模型：</p>
<ul>
<li>U-Net</li>
<li>U-Net++</li>
<li>ResUNet-mod</li>
<li>ResUNet++</li>
<li>SFA</li>
</ul>
<h2 id="results">Results</h2>
<p><img src="/20200628l1/image-20200628211818324.png"></p>
<p><img src="/20200628l1/image-20200628211859801.png"></p>
<p><img src="/20200628l1/image-20200628211912419.png"></p>
<p><img src="/20200628l1/image-20200628211927688.png"></p>
<p><img src="/20200628l1/image-20200628212011714.png"></p>
]]></content>
      <categories>
        <category>阅读笔记</category>
      </categories>
      <tags>
        <tag>2020</tag>
        <tag>MICCAI</tag>
        <tag>图像分割</tag>
        <tag>code</tag>
        <tag>息肉</tag>
      </tags>
  </entry>
  <entry>
    <title>【2020】Progressive Adversarial Semantic Segmentation</title>
    <url>/20200801l8/</url>
    <content><![CDATA[<p>针对域内和跨域的医学图像分割。</p>
<p>G加工输入数据；D为判别器，针对无监督数据；E生成latent representation</p>
<p><img src="/20200801l8/image-20200517102005015.png"></p>
<p><img src="/20200801l8/image-20200517102030801.png"></p>
]]></content>
      <categories>
        <category>阅读笔记</category>
      </categories>
      <tags>
        <tag>2020</tag>
        <tag>图像分割</tag>
      </tags>
  </entry>
  <entry>
    <title>【2020 IJCAI】Rectifying Pseudo Label Learning via Uncertainty Estimation for Domain Adaptive Semantic Segmentation</title>
    <url>/20200801l2/</url>
    <content><![CDATA[<p>【详细讲解参考】<a href="https://zhuanlan.zhihu.com/p/130220572" target="_blank" rel="external nofollow noopener noreferrer">利用Uncertainty修正Domain Adaptation中的伪标签</a></p>
<p>在分割模型中加入了辅助分类器（本来只是用来防止梯度消失的），结构如下图：有一个主分类器接在res5c后面，一个辅助分类器接在res4b22</p>
<p><img src="/20200801l2/image-20200427180645555.png"></p>
<p>观察到，伪标签错误的地方，往往是两个分类器预测不同结果的地方（Prediction Variance）</p>
<p><img src="/20200801l2/image-20200427180807836.png"></p>
<p>所以很自然的我们对于cross-entropy loss做了一个修正：其中<span class="math inline">\(D*{kl}\)</span> 就是主分类器和辅助分类器预测结果的KL距离(也可以叫作Prediction Variance)，如果差异大，则这个距离也就大，那么Lrect 对于这种不确定的样本，就不惩罚（因为pseudo label很可能是错的）。如果没有后面+<span class="math inline">\(D_{kl}\)</span> 这一项，模型很懒，会趋向把所有pseudo label都说成是不确定的，那么Lrect就等于0了。为了避免这种情况，所以我们加了一个+<span class="math inline">\(D_{kl}\)</span> 。</p>
<p><img src="/20200801l2/image-20200427180917233.png"></p>
<p><img src="/20200801l2/image-20200427180952248.png"></p>
<p>对于一些segmentation里面占得面积小的类别有奇效。因为传统方法按照confidence一刀切。往往数量小的类别（自行车Bike和骑行人Rider）这种类别吃亏，因为confidence score往往只有0.7，0.8，会被0.9的硬阈值卡掉。</p>
<p><img src="/20200801l2/image-20200427181130077.png"></p>
<p><img src="/20200801l2/image-20200427181145729.png"></p>
<p><img src="/20200801l2/image-20200427181214625.png"></p>
]]></content>
      <categories>
        <category>阅读笔记</category>
      </categories>
      <tags>
        <tag>2020</tag>
        <tag>图像分割</tag>
        <tag>IJCAI</tag>
        <tag>uncertainty</tag>
      </tags>
  </entry>
  <entry>
    <title>【2019】See Better Before Looking Closer: Weakly Supervised Data Augmentation Network for Fine-Grained Visual Classification</title>
    <url>/20200624l2/</url>
    <content><![CDATA[<p>Code: <a href="https://github.com/GuYuc/WS-DAN.PyTorch" target="_blank" rel="external nofollow noopener noreferrer">https://github.com/GuYuc/WS-DAN.PyTorch</a></p>
<h1 id="motivation">Motivation</h1>
<h2 id="弱监督学习">弱监督学习</h2>
<p>什么是弱监督学习</p>
<p>弱监督是相对监督而言，所谓的监督简单的说就是label，所以监督的强弱就是从label来划分的，弱监督就是data的label并不是很完善的情况，其种类如下：</p>
<ul>
<li>不完整监督： 部分样本label缺失。 即只有训练数据集的一个（通常很小的）子集有标签，其它数据则没有标签。在很多任务中都存在这种情况。例如，在图像分类中，真值标签是人工标注的；从互联网上获得大量的图片很容易，然而由于人工标注的费用，只能标注其中一个小子集的图像。</li>
<li>粗粒度监督： 只有粗粒度的标签。 又以图像分类任务为例。我们希望图片中的每个物体都被标注；然而我们只有图片级的标签而没有物体级的标签。比如说你有一张水果的图片，但是你不知道图片中的水果具体是苹果还是梨。</li>
<li>不准确监督：给的label包含噪声，甚至是错误的label，比如把“行人”标注为“汽车”。 即给定的标签并不总是真值。出现这种情况的原因有，标注者粗心或疲倦，或者一些图像本身就难以分类。</li>
</ul>
<h2 id="数据增强">数据增强</h2>
<p>数据增强是常用的增加数据训练数据量的方法，被用来预防过拟合和提高深度学习模型的表现。在计算机视觉领域实践应用中常用的数据增强方法主要有：剪裁、翻转、旋转、比例缩放、位移、高斯噪声以及更高级的增强技术条件型生成对抗网络（Conditional GAN）。</p>
<ul>
<li>剪裁(Crop)：从原始图像中随机抽样一部分，然后将此部分调整为原图像大小。这种方法通常也被称为随机剪裁。</li>
<li>翻转(Flip)：可以对图片进行水平和垂直翻转。</li>
<li>旋转(Rotation)：对图像按照图像中心进行旋转一定角度，并将大小作为原图的大小。</li>
<li>比例缩放(Scale)：图像可以向内或者向外缩放。向内缩放后通常图像会小于原图，通常会对超出边界做处内容假设；向外缩放后通常会大于原图，通常会新图中剪裁出一部分。（它和随机剪裁得到对图像具有一定区别，有兴趣可以自己拿一张图片试一下看一下效果）</li>
<li>位移(Translation)：对同图像中对目标按照x或y方向平移，因为多数情况，我们的目标对象可能出现在图像的任何位置。</li>
<li>高斯噪声(Gaussian Noise)：当神经网络试图学习高频特征（即非常频繁出现的无意义模式），而这些高频特征对模型提升没有什么帮助的时候发生过拟合(Overfitting)。因此，对这些数据人为加入噪声，使其特征失真，减弱其对模型的影响，高斯噪声就是这种方法之一，</li>
<li>条件型生成对抗网络(Conditional GANs)：是一种强大的神经网络，能将一张图片从一个领域转换到另一个领域中去，比如改变风景图片的季节、转换图片风格等。</li>
</ul>
<h1 id="methods">Methods</h1>
<h2 id="创新点">创新点</h2>
<ul>
<li>双线性注意力池化机制(Bilinear Attention Pooling, 下文简称BAP)</li>
<li>类center loss的注意力监督机制</li>
<li>基于注意力的数据增强策略</li>
</ul>
<p><img src="/20200624l2/image-20200624213638315.png"></p>
<h2 id="weakly-supervised-attention-learning">Weakly Supervised Attention Learning</h2>
<p>训练过程：</p>
<p><img src="/20200624l2/image-20200624214610530.png"></p>
<p>测试过程：</p>
<p><img src="/20200624l2/image-20200624214631313.png"></p>
<h3 id="spatial-representation">Spatial Representation</h3>
<p>采用Attention map来实现注意力机制</p>
<p><span class="math display">\[A=f(F)=\bigcup_{k=1}^{M} A_{k}\]</span></p>
<h3 id="bilinear-attention-pooling">Bilinear Attention Pooling</h3>
<p>首先通过网络的主干（如resnet18）部分分别得到特征图（a）和注意力图（b）。每一个注意力图都代表了目标的特定部分。通过对注意力图和特征图的元素点乘得到各个分部特征图，让后利用卷积或者池化处理分部特征图，最后将各个分部特征图结合得到特征矩阵。</p>
<p><img src="/20200624l2/image-20200624214702745.png"></p>
<h3 id="attention-regularization">Attention Regularization</h3>
<p>使用attention center loss， 进行弱监督attention 的学习。</p>
<h2 id="attention-guided-data-augmentation">Attention-guided Data Augmentation</h2>
<h3 id="augmentation-map">Augmentation Map</h3>
<p>由Attention Map 生成Augmentation Map，指导后续数据增强。</p>
<p><span class="math display">\[A_{k}^{*}=\frac{A_{k}-\min \left(A_{k}\right)}{\max \left(A_{k}\right)-\min \left(A_{k}\right)}\]</span></p>
<h3 id="attention-cropping">Attention Cropping</h3>
<p>使用<span class="math inline">\(C_{k}\)</span>对图像进行裁剪。</p>
<p><span class="math display">\[C_{k}(i, j)=\left\{\begin{array}{ll}
1, &amp; \text { if } A_{k}^{*}(i, j)&gt;\theta_{c} \\
0, &amp; \text { otherwise }
\end{array}\right.\]</span></p>
<h3 id="attention-dropping">Attention Dropping</h3>
<p>为了鼓励attention map表达不同区域，提出了Attention Dropping，擦除部分图像。</p>
<p><span class="math display">\[D_{k}(i, j)=\left\{\begin{array}{ll}
0, &amp; \text { if } A_{k}^{*}(i, j)&gt;\theta_{d} \\
1, &amp; \text { otherwise }
\end{array}\right.\]</span></p>
<h2 id="object-localization-and-refinement">Object Localization and Refinement</h2>
<p><img src="/20200624l2/image-20200624215158271.png"></p>
<h1 id="experiments">Experiments</h1>
<p>注意力剪裁和随机剪裁的比较： 随机剪裁容易剪裁到图像的背景，而注意力剪裁知道取那些部分会see better。</p>
<p><img src="/20200624l2/f4.png"></p>
<p>注意力丢弃和随机丢弃的比较： 随机丢弃可能会将整个目标丢弃或者只丢弃背景部分，而注意力丢弃对剔除目标显著部分和的注意力求具有更高的效率。</p>
<p><img src="/20200624l2/f4-b.png"></p>
]]></content>
      <categories>
        <category>阅读笔记</category>
      </categories>
      <tags>
        <tag>弱监督学习</tag>
        <tag>code</tag>
        <tag>2019</tag>
        <tag>细粒度分类</tag>
      </tags>
  </entry>
  <entry>
    <title>【2018】Self-Attention Generative Adversarial Networks</title>
    <url>/20200710l1/</url>
    <content><![CDATA[<p>Code: <a href="https://github.com/brain-research/self-attention-gan" target="_blank" rel="external nofollow noopener noreferrer">https://github.com/brain-research/self-attention-gan</a></p>
<h1 id="motivation">Motivation</h1>
<ul>
<li>使用小的卷积核很难发现图像中的依赖关系</li>
<li>使用大的卷积核就丧失了卷积网络参数与计算的效率</li>
</ul>
<p><img src="/20200710l1/image-20200710170445992.png"></p>
<h1 id="methods">Methods</h1>
<p>模型结构参照non-local model[1]</p>
<p><img src="/20200710l1/image-20200710170828360.png"></p>
<h1 id="experiments">Experiments</h1>
<h2 id="稳定训练过程的技巧">稳定训练过程的技巧</h2>
<p>1、在生成器和判别器中均应用了Spectral normalization的方法：这个方法可以限制参数的分布(Lipschitz 条件)，从而减少梯度爆炸等影响，稳定网络的训练。最初该方法只在判别器中使用，SAGAN中在生成器和判别器中均加入了Spectral normalization。 2、训练中使用Two Timescale Update Rule (TTUR)：通常在GAN中G和D采用交替训练的方法，G训练一次后需要训练多次D（常见的是一次G，五次D），SAGAN使用了TTUR的方法，具体来说就是G和D使用不同的学习率（G为0.0001，D为0.0004），使得每一次生成器的训练之后判别器需要更少的训练次数。</p>
<p><img src="/20200710l1/image-20200710171335719.png"></p>
<p><img src="/20200710l1/image-20200710171357415.png"></p>
<p><img src="/20200710l1/image-20200710171412547.png"></p>
<p><img src="/20200710l1/image-20200710171429625.png"></p>
<p><img src="/20200710l1/image-20200710171448198.png"></p>
<h1 id="参考文献">参考文献</h1>
<p>[1] Wang, X., Girshick, R., Gupta, A., and He, K. Non-local neural networks. In CVPR, 2018.</p>
]]></content>
      <categories>
        <category>阅读笔记</category>
      </categories>
      <tags>
        <tag>注意力机制</tag>
        <tag>2018</tag>
        <tag>code</tag>
        <tag>谷歌</tag>
        <tag>GAN</tag>
      </tags>
  </entry>
  <entry>
    <title>【2020】Self-Supervised Nuclei Segmentation in Histopathological Images Using Attention</title>
    <url>/20200724l1/</url>
    <content><![CDATA[<p>Code: <a href="https://github.com/msahasrabudhe/miccai2020_self_sup_nuclei_seg" target="_blank" rel="external nofollow noopener noreferrer">https://github.com/msahasrabudhe/miccai2020_self_sup_nuclei_seg</a></p>
<h1 id="motivation">Motivation</h1>
<p>针对医疗图像标注困难，提出了一种基于自监督的组织病理学图像细胞核分割方法。</p>
<h2 id="本文贡献">本文贡献</h2>
<ul>
<li>基于核是该任务的判别特征的前提下，使用尺度分类作为自监督信号。</li>
<li>采用基于Dilated的全卷积网络生成分割图作为注意力Map</li>
<li>引入正则化约束以生成语义上有意义的分割结果</li>
</ul>
<h1 id="methods">Methods</h1>
<p>主要思想是，给定一张WSI贴片，以一定放大倍数观察，可以通过观察贴片中核的大小和纹理确定放大水平。基于此，假设原子核足以确定放大倍数，并且其他图像不是必需的。</p>
<p><img src="/20200724l1/image-20200724111338865.png"></p>
<p>假设学习了一个对scale敏感的网络，专门用于区分特征的正确比例。给定一组WSIs，以固定的放大倍数<span class="math inline">\(C\)</span>提取所有组织patches。首先，通过Attention network学习一个二分类的分割图<span class="math inline">\(A \in [0, 1]^{1\times H \times W}\)</span>。输入图像与注意力图相乘可以确定放大比例<span class="math inline">\(A \odot I\)</span>。</p>
<p>confidence map表示为<span class="math inline">\(A=\sigma(a)\)</span>，<span class="math inline">\(a=F(I) \in \mathbb{R}^{1\times H \times W}\)</span>，<span class="math inline">\(\sigma(\cdot)\)</span>为sigmoid函数。通过应用<code>稀疏性正则化</code>，迫使注意力图只关注input patch的局部。 <span class="math display">\[
\tau=\frac{1}{B} \sum_{b=1}^{B} \mathbf{a}_{b}^{(\eta)}
\]</span> <span class="math inline">\(J=A \odot I\)</span>表示缩放比例，通过负对数似然法来训练分类网络G，然后训练注意力网络<span class="math inline">\(F\)</span> <span class="math display">\[
\mathcal{L}_{\text {scale }}(\hat{\mathbf{p}}, l)=-\log \hat{p}_{l} ; \hat{p}_{i}=[\operatorname{softmax}(\hat{\mathbf{s}})]_{i} ; \hat{\mathbf{s}}=\mathcal{G}(J) ; 1 \leq i \leq N_{\mathcal{C}}
\]</span></p>
<h2 id="smoothness-regularization">Smoothness Regularization</h2>
<p>希望<span class="math inline">\(A\)</span>在语义上有意义且平滑，要关注核而不是高频区域，在注意力图上加入了平滑度正则化。 <span class="math display">\[
\mathcal{L}_{\mathrm{smooth}}=\frac{1}{(H-1)(W-1)} \sum_{i, j}\left\|A_{i+1, j}-A_{i, j}\right\|_{1}+\left\|A_{i, j+1}-A_{i, j}\right\|_{1}
\]</span></p>
<h2 id="transformation-equivariance">Transformation Equivariance</h2>
<p>加入语义一致性的常用约束 <span class="math display">\[
\mathcal{L}_{\mathrm{equiv}}=\frac{1}{H W}\|\sigma(t(\mathcal{F}(I)))-\sigma(\mathcal{F}(t(I)))\|_{2}^{2}
\]</span> 总的损失为三者相加，其中<span class="math inline">\(\mathcal{L}_{scale}\)</span>为自监督loss <span class="math display">\[
\mathcal{L}_{total} = \mathcal{L}_{scale}+\mathcal{L}_{smooth}+\mathcal{L}_{equiv}
\]</span></p>
<h2 id="post-processing-validation-and-model-selection">Post Processing, Validation, and Model Selection</h2>
<p>通过三步后处理得到最终结果：</p>
<ol type="1">
<li>使用开和闭的形态学操作</li>
<li>应用高斯模糊</li>
<li>使用分水岭算法得到最终结果</li>
</ol>
<h1 id="experiments">Experiments</h1>
<p><img src="/20200724l1/image-20200724135044368.png"></p>
<p><img src="/20200724l1/image-20200724135108880.png"></p>
]]></content>
      <categories>
        <category>阅读笔记</category>
      </categories>
      <tags>
        <tag>2020</tag>
        <tag>图像分割</tag>
        <tag>code</tag>
        <tag>自监督</tag>
      </tags>
  </entry>
  <entry>
    <title>【2020 MICCAI】Self-Loop Uncertainty: A Novel Pseudo-Label for Semi-Supervised Medical Image Segmentation</title>
    <url>/20200731l1/</url>
    <content><![CDATA[<h1 id="motivation">Motivation</h1>
<ul>
<li>注释医学图像需要有经验的医生花费数小时或数天进行研究，不仅费力而且十分昂贵</li>
<li>使用未标记数据的伪标签（通过不确定性估计由分割算法自动生成）是潜在的解决方案之一，其中最受欢迎的方法是：
<ul>
<li>softmax概率图</li>
<li>蒙特卡洛drop</li>
<li>网络结构中的不确定性估计</li>
</ul></li>
</ul>
<h1 id="methods">Methods</h1>
<p>提出的自环不确定性是通过对具有自监督子任务（例如拼图）的完全卷积网络（FCN）的编码器进行循环优化而生成的。</p>
<p>训练集包含标注数据<span class="math inline">\(D_L\)</span>和无标注数据<span class="math inline">\(D_U\)</span>，提出的方法包含三个loss（<span class="math inline">\(L_{SEG}, L_{UG}, L_{SS}\)</span>），<span class="math inline">\(L_{SEG}\)</span>用户有标注数据的分割训练，<span class="math inline">\(L_{SS}\)</span>用户学习两个数据源的数据信息并生成自循环不确定性图，<span class="math inline">\(L_{UG}\)</span>用于增强无标注数据的分割性能。</p>
<p><img src="/20200731l1/image-20200731095630976.png"></p>
<p><img src="/20200731l1/image-20200731095742223.png"></p>
<h2 id="self-supervised-sub-task">Self-supervised Sub-task</h2>
<p>自监督损失<span class="math inline">\(L_{SS}\)</span>旨在利用原始数据中包含的丰富信息并产生自环不确定性。</p>
<p>本文使用由平移和旋转变换组成的拼图作为自我监督的子任务来反复优化FCN的编码器并产生自环不确定性。例如将图像切分为<span class="math inline">\(3 \times 3\)</span>的拼图，然后进行排序。进行K次排序组合，选择最大汉明距离图像放入排序池中<span class="math inline">\(P=(P_1, P_2, ..., P_{9!})\)</span>。</p>
<p>每次迭代训练选取从<span class="math inline">\(K=100\)</span>中选取<span class="math inline">\(Q \ll K, Q=10\)</span>次，从中选取一个作为替代。同时，使用过K组图像对FCN的编码器进行更新，可以看做K个类别的分类任务。</p>
<h2 id="uncertainty-guided-loss">Uncertainty-guided Loss</h2>
<p>本文采用均方误差损失作为不确定性指导，<span class="math inline">\(y_{sl}\)</span>为伪标签 <span class="math display">\[
\mathcal{L}_{U G}\left(S_{x}, y_{s l}\right)=\frac{\sum_{H \times W} \mathbb{I}\left(y_{s l}&gt;t h\right)\left\|S_{x}-y_{s l}\right\|^{2}}{\sum_{H \times W} \mathbb{I}\left(y_{s l}&gt;t h\right)}
\]</span></p>
<h2 id="目标函数">目标函数</h2>
<p><span class="math display">\[
\mathcal{L}=\sum_{j=1}^{N} \mathcal{L}_{S E G}\left(x_{j}, y_{j}\right)+\sum_{j=N+1}^{N+M} \mathcal{L}_{U G}\left(x_{j}, y_{s l}\right)+\sum_{j=1}^{N+M} \sum_{i=1}^{Q} \mathcal{L}_{S S}\left(T_{P_{i}^{\prime}}\left(x_{j}\right), g_{i}\right)
\]</span></p>
<h1 id="experiments">Experiments</h1>
<p><img src="/20200731l1/image-20200731111559107.png"></p>
<p><img src="/20200731l1/image-20200731111621604.png"></p>
]]></content>
      <categories>
        <category>阅读笔记</category>
      </categories>
      <tags>
        <tag>2020</tag>
        <tag>MICCAI</tag>
        <tag>腾讯</tag>
        <tag>图像分割</tag>
        <tag>自监督学习</tag>
      </tags>
  </entry>
  <entry>
    <title>【2020 ECCV】Semantic Flow for Fast and Accurate Scene Parsing</title>
    <url>/20200809l1/</url>
    <content><![CDATA[<p>Code: <a href="https://github.com/donnyyou/torchcv" target="_blank" rel="external nofollow noopener noreferrer">https://github.com/donnyyou/torchcv</a></p>
<h1 id="motivation">Motivation</h1>
<p>场景解析通常需要得到高分辨率且具有丰富语义特征的特征图，如利用空洞卷积和特征金字塔，但这些方法需要较多的计算量，不够有效。</p>
<p>从利用光流对相邻视频帧进行运动对齐的方法受到启发，本文提出一种FAW（Flow Alignment Module）的方法，来学习相邻尺度特征图之间的语义流。和FPN结构融合在一起，实现了快速又精确的结果。</p>
<p><img src="/20200809l1/image-20200809170732851.png"></p>
<h1 id="methods">Methods</h1>
<p><img src="/20200809l1/image-20200809170809299.png"></p>
<h2 id="flow-alignment-module">Flow Alignment Module</h2>
<p>为了更灵活，更动态地对齐，光流法对于在视频处理任务中对齐两个相邻的视频帧特征非常有效且灵活。</p>
<p>首先给出相邻两层特征图<span class="math inline">\(F_l\)</span>和<span class="math inline">\(F_{l-1}\)</span>，首先将特征图<span class="math inline">\(F_l\)</span>进行双线性插值上采样，得到的特征图与<span class="math inline">\(F_{l-1}\)</span>相同尺寸。然后将两个特征图concat，再经过一个<span class="math inline">\(3 \times 3\)</span>卷积，预测出语义流场。 <span class="math display">\[
\Delta_{l-1}= conv_l (cat(F_l, F_{l-1}))
\]</span> 由于输入特征与流场之间存在分辨率差距，因此将偏移量减半 <span class="math display">\[
p_l = \frac{p_{l-1} + \Delta_{l-1}(p_{l-1})}{2}
\]</span> <img src="/20200809l1/image-20200809171425537.png"></p>
<h2 id="network-architectures">Network Architectures</h2>
<ul>
<li><p>Backbone</p>
<p>resnet系列、ShuffleNet V2、DF系列</p></li>
<li><p>Contextual Module</p>
<p>采用Pyramid Pooling Module(PPM)用于捕获长距离的上下文信息</p></li>
<li><p>Aligned FPN Decoder</p>
<p>从Encoder中得到特征图，使用对齐的特征金字塔为最后一层的场景解析。使用FAM替换传统的双线性插值模块</p></li>
<li><p>Cascaded Deeply Supervised Learning</p>
<p>使用深度监督损失来监督中间输出，使得解码器更容易优化。</p></li>
</ul>
<p><img src="/20200809l1/image-20200809171538930.png"></p>
<h1 id="experiments">Experiments</h1>
<p><img src="/20200809l1/image-20200809195815405.png"></p>
<p><img src="/20200809l1/image-20200809195839873.png"></p>
<p><img src="/20200809l1/image-20200809195858502.png"></p>
<p><img src="/20200809l1/image-20200809195915264.png"></p>
<p><img src="/20200809l1/image-20200809195931646.png"></p>
<p><img src="/20200809l1/image-20200809195948221.png"></p>
]]></content>
      <categories>
        <category>阅读笔记</category>
      </categories>
      <tags>
        <tag>2020</tag>
        <tag>图像分割</tag>
        <tag>code</tag>
        <tag>ECCV</tag>
        <tag>北京大学</tag>
      </tags>
  </entry>
  <entry>
    <title>【2020 CVPR】Strip Pooling: Rethinking Spatial Pooling for Scene Parsing</title>
    <url>/20200731l5/</url>
    <content><![CDATA[<p>Code: <a href="https://github.com/Andrew-Qibin/SPNet" target="_blank" rel="external nofollow noopener noreferrer">https://github.com/Andrew-Qibin/SPNet</a></p>
<h1 id="motivation">Motivation</h1>
<p>解决长距离依赖的常见方法是注意力机制和非局部模块，但这些方法太消耗内存。</p>
<p>金字塔网络一般只应用在骨干网络的上层，不能灵活使用或直接应用到网络中学习。</p>
<p>这篇文章提出条状Pooling，相比于方形卷积，可以较好检测到长条形物体，提出的方法可以直接插入在已有的网络当中。</p>
<h2 id="本文贡献">本文贡献</h2>
<ul>
<li>提出了Strip Pooling，能够感知大范围依赖以及局部细节</li>
<li>设计了Strip Pooling Module和Mixed Pooling Module，可以随意插入任意网络中</li>
<li>设计了SPNet，提高分割性能</li>
</ul>
<p><img src="/20200731l5/image-20200420115653369.png"></p>
<h1 id="methods">Methods</h1>
<p><img src="/20200731l5/image-20200420115704116.png"></p>
<p><img src="/20200731l5/image-20200420115715342.png"></p>
<h1 id="experiments">Experiments</h1>
<p><img src="/20200731l5/image-20200420115724747.png"></p>
<p><img src="/20200731l5/image-20200420115733985.png"></p>
]]></content>
      <categories>
        <category>阅读笔记</category>
      </categories>
      <tags>
        <tag>2020</tag>
        <tag>图像分割</tag>
        <tag>CVPR</tag>
        <tag>code</tag>
      </tags>
  </entry>
  <entry>
    <title>【2020 CVPR】Structure Boundary Preserving Segmentation for Medical Image with Ambiguous Boundary</title>
    <url>/20200627l2/</url>
    <content><![CDATA[<h1 id="motivation">Motivation</h1>
<p>图像分割在医学影像中存在两个关键问题：</p>
<ul>
<li>分割区域边界模糊，存在歧义性</li>
<li>没有专业领域知识，分割区域不确定</li>
</ul>
<p>针对以上问题，提出了能够保存目标区域边界信息的医学图像分割框架：</p>
<ul>
<li>首先，提出了一种新颖的关键点选择算法。</li>
<li>然后，使用一种称为边界保留块（Boundary Preserving Block，BPB）的新型结构边界保留模型将这些点编码到分割网络中。</li>
<li>为了将专家知识嵌入到全自动分割模型中，在没有用户交互的情况下，以对抗性方式引入一种新型的基于结构边界信息的判别网络，成为形状边界感知评估器（Shape Boundary-aware Evaluator，SBE）。</li>
</ul>
<p><img src="/20200627l2/image-20200627201825789.png"></p>
<h1 id="methods">Methods</h1>
<p>整体框架分为三部分：</p>
<ul>
<li>Segmentation network</li>
<li>Boundary Preserving Block</li>
<li>Shape Boundary-aware Evaluator</li>
</ul>
<p><img src="/20200627l2/image-20200627201903381.png"></p>
<h2 id="boundary-key-point-selection-algorithm">Boundary Key Point Selection Algorithm</h2>
<p>首先提出一个关键点选择算法，找出符合目标区域边界的关键点。</p>
<ul>
<li><p>首先，使用传统的边缘检测算法从GT中找出目标物体的边界。</p>
<p>在目标边界上随机选取n个点，表示为<span class="math inline">\(P_{n}^{t}=\{(x_1^t, y_1^t),(x_2^t, y_2^t),...,(x_n^t, y_n^t)\}\)</span></p></li>
<li><p>然后，我们通过n轮后的n个点构造边界区域<span class="math inline">\(S_n^t\)</span>。</p></li>
<li><p>最后，选取使得IOU最大时的结构边界点为最终的边界点。</p>
<p><span class="math display">\[\tilde{P}=P_{n}^{\tilde{t}}, \text { where } \tilde{t}=\underset{t \in\{1, \cdots, T\}}{\arg \max } \operatorname{IOU}\left(\mathrm{S}_{n}^{t}, \mathrm{S}_{G T}\right)\]</span></p></li>
<li><p>选取的关键点集<span class="math inline">\(\tilde{P}\)</span>转换为2维的边界点图（boundary point map）。以这些点为中心，周围半径为R内置为1，其余为0（可以看做二维概率图或heapmap，更适合卷积网络训练和预测）</p></li>
</ul>
<p><img src="/20200627l2/image-20200627203024706.png"></p>
<h2 id="boundary-preserving-block-bpb">Boundary Preserving Block (BPB)</h2>
<p>通过生成的边界关键点图，分割模型可以在无需交互的情况下将结构边界信息进行编码。</p>
<p><img src="/20200627l2/image-20200627203826128.png"></p>
<h2 id="shape-boundary-aware-evaluator-sbe">Shape Boundary-aware Evaluator (SBE)</h2>
<p>模型最终产生的分割图预测与中间卷积模块中产生的边界点图可以通过形状边界判别器来约束它们之间表达的一致性。</p>
<p>将边界关键点图和给定的分割图像作为输入concat后输入到SBE结构中。</p>
<p>当边界与分割图一致时得分高，不一致时得分低。</p>
<p>通过损失<span class="math inline">\(L_{SBE}\)</span>训练SBE网络。</p>
<p><span class="math display">\[L_{SBE}=-log(D(S_{GT};M_{GT}))-log(1-D(\hat{S}_{Pred};M_{GT}))\]</span></p>
<h2 id="training-segmentation-network">Training Segmentation Network</h2>
<p>通过三个损失进行训练整个网络。</p>
<p><span class="math display">\[L_{Total}=L_{Seg}+L_{BA}+\sum_{i=1}^{l}{L_{Map}^{i}}\]</span></p>
<p>分割损失为：</p>
<p><span class="math display">\[\begin{aligned}
L_{S e g}=&amp;-\mathrm{S}_{G T} \cdot \log \left(\hat{\mathrm{S}}_{P r e d}\right)-\left(1-\mathrm{S}_{G T}\right) \cdot \log \left(1-\hat{\mathrm{S}}_{P r e d}\right)
\end{aligned}\]</span></p>
<p>关键点损失：</p>
<p><span class="math display">\[L_{M a p}^{i}=-\mathrm{M}_{G T}^{i} \cdot \log \hat{\mathrm{M}}^{i}-\left(1-\mathrm{M}_{G T}^{i}\right) \cdot \log \left(1-\hat{\mathrm{M}}^{i}\right)\]</span></p>
<p>边界感知损失：</p>
<p><span class="math display">\[L_{BA}=-log{(D(\hat{S}_{Pred};M_{GT}))}\]</span></p>
<p>其中，<span class="math inline">\(D(\cdot)\)</span>表示SBE接受分割图和边界关键点图作为输入并产生预测分数。</p>
<h1 id="experiments">Experiments</h1>
<h2 id="datasets">Datasets</h2>
<ul>
<li>PH2+ISBI 2016 Skin Lesion Challenge dataset</li>
<li>(私有) Transvaginal Ultrasound (TVUS) dataset</li>
</ul>
<h2 id="results">Results</h2>
<p>文中有提到一句话：</p>
<blockquote>
<p>Even the images have ambiguous structure boundary or heterogeneous texture, our method preserves structure boundary of target region; on the other hand, segmentation results obtained from U-Net failed to preserve boundary.</p>
</blockquote>
<p>这篇文章对U-Net不起作用？</p>
<p>还有个疑问，在与其他方法对比时，不同数据集使用了不同主干网络，没有提及其他主干网络搭配自己的方法，是结果不好吗？</p>
<p><img src="/20200627l2/image-20200627205528925.png"></p>
<p><img src="/20200627l2/image-20200627205540306.png"></p>
<p><img src="/20200627l2/image-20200627205607865.png"></p>
<p><img src="/20200627l2/image-20200627205624499.png"></p>
<p><img src="/20200627l2/image-20200627205633733.png"></p>
<p><img src="/20200627l2/image-20200627205646059.png"></p>
<p><img src="/20200627l2/image-20200627205704449.png"></p>
<p><img src="/20200627l2/image-20200627205725397.png"></p>
]]></content>
      <categories>
        <category>阅读笔记</category>
      </categories>
      <tags>
        <tag>2020</tag>
        <tag>图像分割</tag>
        <tag>CVPR</tag>
      </tags>
  </entry>
  <entry>
    <title>【2020 CVPR】Synthetic Learning: Learn From Distributed Asynchronized Discriminator GAN Without Sharing Medical Image Data</title>
    <url>/20200627l1/</url>
    <content><![CDATA[<p>Code: <a href="https://github.com/tommy-qichang/AsynDGAN" target="_blank" rel="external nofollow noopener noreferrer">https://github.com/tommy-qichang/AsynDGAN</a></p>
<h1 id="motivation">Motivation</h1>
<ul>
<li>隐私问题在每个领域都很重要，尤其是在医疗领域。</li>
<li>由于政策的限制，每年产生的医疗影像很多，然而能够获得的数据太少。</li>
</ul>
<h1 id="methods">Methods</h1>
<h2 id="贡献">贡献</h2>
<p>没有针对特定任务训练模型，而是分布式学习判别器，集中学习生成器，生成新数据。</p>
<ul>
<li>能够在不共享患者原始数据的情况下，从多个数据集中了解真实图像的分布情况。</li>
<li>与其他分布式深度学习方法相比，更高效，所需带宽更低。</li>
<li>与一个真实数据集训练的模型相比，具有更高的性能，与所有真实数据集训练的模型相比，性能几乎相同。</li>
<li>证明保证了生成器能够以一种非常重要的方式学习分布的分布，从而是无偏的。</li>
</ul>
<h2 id="网络结构">网络结构</h2>
<p>所有结构共用同一个生成器，在不同医院实体中采用不同参数的判别器。</p>
<p>对于分割任务，采用UNet来测试性能。</p>
<p><img src="/20200627l1/image-20200627141939598.png"></p>
<h2 id="中央生成器">中央生成器</h2>
<ul>
<li>对于分割任务，采用自编码器（Encoder-Decoder）。</li>
<li>降采样使用stride为2的卷积，9个残差块，两个反卷积。</li>
<li>非残差块采用BN和ReLU。</li>
<li>第一层和最后一层使用<span class="math inline">\(7 \times 7\)</span>卷积核外，其他卷积使用<span class="math inline">\(3 \times 3\)</span>卷积核。</li>
</ul>
<h2 id="判别器">判别器</h2>
<p>每一个医院使用不同参数的判别器，分割任务中采用PatchGAN</p>
<h2 id="优化过程">优化过程</h2>
<p>依次优化判别器后，最后再优化生成器。</p>
<p>更新G时，使用判别损失<span class="math inline">\(\sum_{j=1}^{N}{loss(D_{j})}\)</span>。</p>
<p><img src="/20200627l1/image-20200627142646116.png"></p>
<p><img src="/20200627l1/image-20200627142714654.png"></p>
<h1 id="experiments">Experiments</h1>
<h2 id="datasets">Datasets</h2>
<ul>
<li>BraTS2018</li>
<li>Multi-Organ</li>
</ul>
<h2 id="results">Results</h2>
<p><img src="/20200627l1/image-20200627143613840.png"></p>
<p><img src="/20200627l1/image-20200627143633392.png"></p>
<p><img src="/20200627l1/image-20200627143648298.png"></p>
<p><img src="/20200627l1/image-20200627143720935.png"></p>
<p><img src="/20200627l1/image-20200627143736546.png"></p>
<p><img src="/20200627l1/image-20200627143747684.png"></p>
]]></content>
      <categories>
        <category>阅读笔记</category>
      </categories>
      <tags>
        <tag>2020</tag>
        <tag>CVPR</tag>
        <tag>code</tag>
        <tag>GAN</tag>
        <tag>联邦学习</tag>
      </tags>
  </entry>
  <entry>
    <title>【2020 MICCAI】Towards Emergent Language Symbolic SemanticSegmentation and Model Interpretability</title>
    <url>/20200803l1/</url>
    <content><![CDATA[<h1 id="motivation">Motivation</h1>
<p>当前最先进的机器学习（ML）和人工智能（AI）的局限性包括缺乏可解释性和可解释能力；即利用深度神经网络的经典黑盒方法无法提供模型行为的证据。</p>
<p>受symbol grounding problem启发，本文研究了深度学习语义分割与Emergent Language（EL）模型之间的协同作用。通过描述如何扩展黑盒语义分割以提供符号语义<span class="math inline">\((S^2)\)</span>输出，进一步利用EL体系结构的一般属性来促进模型的解释能力。相应的句子（从分类分布中提取）是通过将符号组件集成到传统UNet-Like结构中生成的。</p>
<h1 id="methods">Methods</h1>
<p>本文模型假定如下：</p>
<ul>
<li>分割模型提供分割的输出<span class="math inline">\(x\)</span></li>
<li>提供词汇<span class="math inline">\(V={w_1, w_2, ..., w_{N_V}}\)</span>；句子<span class="math inline">\(S_{N_w}\)</span>的长度<span class="math inline">\(N_w\)</span>为一系列词组的长度<span class="math inline">\({w_1, w_2, ..., w_{N_w}}\)</span></li>
<li>Sender agent接受分割模型的输出<span class="math inline">\(x\)</span>并生成一个长度为<span class="math inline">\(N_w\)</span>的句子<span class="math inline">\(S_{N_w}\)</span></li>
<li>Receiver agent获得符号句子<span class="math inline">\(S_{N_w}\)</span>并生成输出<span class="math inline">\(x&#39;=Receiver(S_{N_w})\)</span></li>
<li>最后将<span class="math inline">\(x\)</span>和<span class="math inline">\(x&#39;\)</span>混合生成最终结果</li>
</ul>
<p><img src="/20200803l1/image-20200803104822471.png"></p>
<h2 id="sender-and-receiver-network">Sender and Receiver Network</h2>
<h3 id="sender-network">Sender Network</h3>
<ul>
<li>将<span class="math inline">\(x\)</span>输入到线性变换中(Linear transformation)</li>
<li>将变换后的结果输入到stacked LSTM网络中</li>
</ul>
<h3 id="receiver-network">Receiver Network</h3>
<p>Receiver由一个标准的LSTM模型构成</p>
<ul>
<li>初始状态设置为0</li>
<li>为Receiver最后一层隐藏层应用线性变换</li>
</ul>
<h3 id="semantic-symbolic-segmentation">Semantic Symbolic Segmentation</h3>
<ul>
<li>将输入<span class="math inline">\(x\)</span>和<span class="math inline">\(x&#39;\)</span>合并（Concat）</li>
<li>应用一个卷积操作及batch normalization使得tensor具有相同维度</li>
<li>最终结果应用Sigmoid操作</li>
</ul>
<h1 id="experiments">Experiments</h1>
<p><img src="/20200803l1/image-20200803110452480.png"></p>
<p><img src="/20200803l1/image-20200803110517654.png"></p>
<p><img src="/20200803l1/image-20200803110548548.png"></p>
]]></content>
      <categories>
        <category>阅读笔记</category>
      </categories>
      <tags>
        <tag>2020</tag>
        <tag>MICCAI</tag>
        <tag>图像分割</tag>
        <tag>模型可解释性</tag>
      </tags>
  </entry>
  <entry>
    <title>【2020 ICLR】U-GAT-IT: Unsupervised Generative Attentional Networks with Adaptive Layer-Instance Normalization for Image-to-Image Translation</title>
    <url>/20200801l10/</url>
    <content><![CDATA[<p>Code: <a href="https://github.com/znxlwm/UGATIT-pytorch" target="_blank" rel="external nofollow noopener noreferrer">https://github.com/znxlwm/UGATIT-pytorch</a></p>
<p><a href="http://www.twistedwg.com/2019/08/07/UGATIT.html" target="_blank" rel="external nofollow noopener noreferrer">UGATIT-自适应图层实例归一化下图像到图像转换</a> <a href="http://39.108.217.36/index.php/archives/392/" target="_blank" rel="external nofollow noopener noreferrer">论文笔记</a></p>
<p>引入<code>注意力机制</code>，这里采用全局和平均池化的类激活图（Class Activation Map-CAM）来实现的，通过CNN确定分类依据的位置。 加入<code>自适应图层实例归一化</code>（AdaLIN），帮助注意力引导模型灵活控制形状和纹理变化量。</p>
<p>CAM 的意义就是以热力图的形式告诉我们，模型通过哪些像素点得知图片属于某个类别。 特征图经过 GAP 处理后每一个特征图包含了不同类别的信息，权重 w 对应分类时的权重。绘制热力图时，提取出所有的权重，往回找到对应的特征图，然后进行加权求和即可。 <img src="/20200801l10/image-20200420120144704.png"></p>
<p>网络结构由一个生成器和两个判别器组成。 判别器的设计采用一个全局判别器(Global Discriminator)以及一个局部判别器(Local Discriminator)结合实现，所谓的全局判别器和局部判别器的区别就在于全局判别器对输入的图像进行了更深层次的特征压缩，最后输出的前一层。 <img src="/20200801l10/image-20200420120158234.png"> <img src="/20200801l10/image-20200420120213245.png"></p>
<p>此处要提一下，在判别器中也加入了CAM模块，虽然在判别器下CAM并没有做域的分类，但是加入注意力模块对于判别图像真伪是有益的，文中给出的解释是注意力图通过关注目标域中的真实图像和伪图像之间的差异来帮助进行微调。</p>
<p>损失函数：</p>
<ul>
<li>GAN的对抗损失</li>
<li>循环一致性损失</li>
<li>身份损失（相同域之间不希望进行转换）</li>
<li>CAM损失（生成器中对图像域进行分类，希望源域和目标域尽可能分开）</li>
</ul>
<p><img src="/20200801l10/image-20200420120247952.png"></p>
]]></content>
      <categories>
        <category>阅读笔记</category>
      </categories>
      <tags>
        <tag>2020</tag>
        <tag>code</tag>
        <tag>图像翻译</tag>
        <tag>ICLR</tag>
      </tags>
  </entry>
  <entry>
    <title>【2020】Uncertainty-based graph convolutional networks for organ segmentation refinement</title>
    <url>/20200713l1/</url>
    <content><![CDATA[<h1 id="motivation">Motivation</h1>
<h2 id="存在问题">存在问题</h2>
<ul>
<li>器官的形状与背景相似性较高，为器官的分割带来了挑战，存在很多False Negative和False Positive区域，可以考虑通过模型的不确定性分析找出潜在错误分类元素的有用信息</li>
<li>不确定性已被证明是半监督学习中的一种有用的注意力机制，计算机视觉的最新工作已经开始探索不确定性的能力，以寻找潜在的错误分类区域用于精细化分割结果</li>
</ul>
<h2 id="本文贡献">本文贡献</h2>
<p>本文是在医学图像单器官分割任务中第一个提出半监督GCN学习的方法</p>
<h1 id="methods">Methods</h1>
<p>本文提出的方法主要用于分割结果的后处理，并且假设无法获取到真实的分割结果。</p>
<ul>
<li>首先，选汇总一个二进制的volume <span class="math inline">\(U_b\)</span>，用于突出显示潜在的false positives和false negatives <span class="math inline">\(Y\)</span>。</li>
<li>其次，使用<span class="math inline">\(U_b\)</span>和<span class="math inline">\(Y，g， V\)</span>，进一步精细分割结果<span class="math inline">\(Y\)</span>。</li>
</ul>
<p><img src="/20200713l1/image-20200713134429510.png"></p>
<h2 id="uncertainty-analysis-finding-incorrect-elements">Uncertainty Analysis: Finding Incorrect Elements</h2>
<p>使用MCDO来评估CNN模型的不确定性。该策略需要应用于带有dropout层的卷积网络中。</p>
<p>首先，获得模型的期望：</p>
<p><span class="math display">\[
\mathbb{E} \approx \frac{1}{T}\sum_{t=1}^{T}{g(V(x), \theta_t)}
\]</span></p>
<p>然后，使用熵来表示模型不确定性<span class="math inline">\(\mathbb{U}\)</span>，使用<span class="math inline">\(\mathbb{E}\)</span>作为P的概率值来计算熵： <span class="math display">\[
\mathbb{U}(x)=H(x) = - \sum_{c=1}^{M}{P(x)^clogP(x)^2}
\]</span> 最终，定义潜在不确定性元素为<span class="math inline">\(U_b\)</span>： <span class="math display">\[
U_b(x)=\mathbb{1}[\mathbb{U}(x) &gt; \tau]
\]</span></p>
<h2 id="graph-learning-for-segmentation-refinement">Graph Learning for Segmentation Refinement</h2>
<p>使用来自不确定性分析的信息，可以定义一个部分标记的图，其中将体素映射到节点，并将邻域关系映射到边。这样refinement问题化为一个半监督图学习问题。</p>
<h3 id="partially-labeled-nodes">Partially-Labeled Nodes</h3>
<p>volume中大多数voxel与refinement问题无关，且图像不限于矩形结构表示，因此定义一个ROI表示目标结果。<span class="math inline">\(ROI(x)=dilation(U_b(x))\bigcup \mathbb{E}_b(x)\)</span></p>
<p>此ROI减少了图的节点数，从而减少了内存需求。最后使用下一条规则根据不确定性在图中标记每一个节点： <span class="math display">\[
l(x)=\left\{\begin{array}{ll}
Y(x) &amp; \text { if } U_{b}(x)=0 \\
\text { unlabeled } &amp; \text { if } U_{b}(x)=1
\end{array}\right.
\]</span></p>
<h3 id="edges-and-weighting">Edges and Weighting</h3>
<p>标记元素和未标记元素的联系通常发生在不确定区域的边界部分。对于选取的一个点，将其坐标系相邻的六个点进行连接，另外再随机选取图中16个节点进行连接。</p>
<p>权重选择使用高斯核进行定义： <span class="math display">\[
w(x_i, x_j)=\lambda div(x_i,x_j)+exp(-\frac{||V(x)-V(x_j)||^2}{2\sigma_1})+exp(-\frac{||V(x_i)-V(x_j)||^2}{2\sigma_2})
\]</span></p>
<p><span class="math display">\[
div(x_i, x_j)=\sum_{c=1}^{M}(P^c(x_i)-P^c(x_j))log\frac{P^c(x_i)}{P^c(x_j)}, M=2
\]</span></p>
<p><span class="math display">\[
P^1(x_i)=\mathbb{E}(x_i), P^2(x_i) = 1-P^1(x_i)
\]</span></p>
<h1 id="experiments">Experiments</h1>
<p><img src="/20200713l1/image-20200713135643651.png"></p>
<p><img src="/20200713l1/image-20200713135658898.png"></p>
<p><img src="/20200713l1/image-20200713135721506.png"></p>
<p><img src="/20200713l1/image-20200713135737475.png"></p>
<p><img src="/20200713l1/image-20200713135749729.png"></p>
]]></content>
      <categories>
        <category>阅读笔记</category>
      </categories>
      <tags>
        <tag>2020</tag>
        <tag>图像分割</tag>
        <tag>图卷积</tag>
        <tag>半监督学习</tag>
        <tag>德国慕尼黑工业大学</tag>
      </tags>
  </entry>
  <entry>
    <title>【2020 MICCAI】Unified cross-modality feature disentangler for unsupervised multi-domain MRI abdomen organs segmentation</title>
    <url>/20200801l11/</url>
    <content><![CDATA[<h1 id="motivation">Motivation</h1>
<p>本文方法使用变分自编码器（VAE）将图像内容与样式分开。</p>
<ul>
<li>VAE约束样式特征编码以匹配通用（高斯）先验，该先验加总后可以覆盖所有源域和目标域的样式</li>
<li>提取的图像样式被转换为潜在样式缩放代码，该代码将生成器可以根据目标域图像内容特征的代码生成多模态图像</li>
<li>介绍了一种联合分布匹配判别器，能够将翻译后的图像与任务相关的分割概率图相结合，以进一步约束和规范化图像到图像（I2I）的翻译</li>
</ul>
<h1 id="methods">Methods</h1>
<p><img src="/20200801l11/image-20200801154439206.png"></p>
<h2 id="feature-disentanglement-and-image-translation">Feature disentanglement and image translation</h2>
<p>本文主要使用VAE将图像的内容和样式进行分离。假设所有域的潜在样式分布为高斯分布，Encoder <span class="math inline">\(E_s\)</span>使用KL散度提取与此先验匹配的样式代码。样式代码之后通过latent scale (LS)层转换为潜在风格<span class="math inline">\(f_s\)</span>。域代码是通过与内容代码按通道进行级联注入的。</p>
<p>VAE中，自我重建损失与KL散度损失合并为： <span class="math display">\[
L_{V A E}=\sum_{i} E_{x_{i} \sim X_{i}}\left[D_{K L}\left(E_{s}\left(x_{i}\right) \| z\right)\right]+\left\|\hat{x}_{i}-x_{i}\right\|_{1}
\]</span> 领域不变内容特征是通过对抗训练由： <span class="math display">\[
L_{a d v}^{c}=\sum_{i} \sum_{j ; j \neq i} \underset{x_{i} \sim X_{i} \atop {x_j \sim X_j}}{\mathbb{E}}\left[\log \left(D_{c}\left(E_{c}\left(x_{i}\right)\right)\right)\right]+\mathbb{E}\left[1-\log \left(D_{c}\left(E_{c}\left(x_{j}\right)\right)\right)\right]
\]</span> Content reconstruction loss: <span class="math display">\[
L_{c}=\sum_{i} \sum_{j ; j \neq i} \underset{x_{i} \sim X_{i} \atop {x_j \sim X_j}}{\mathbb{E}}\left|\left| E_c(x_i)-E_c(\hat{x}_j ) \right|\right|
\]</span> Latent code regression loss <span class="math display">\[
L_{l r}=\sum_{i} \sum_{j ; j \neq i} \underset{x \sim X_{i}}{\mathbb{E}}\left\|z-E_{s}\left(G\left(E_{c}(x), L_{s}(z), d_{j}\right)\right)\right\|_{1}
\]</span> Domain adversarial loss <span class="math display">\[
\begin{aligned}
\min _{G} \max _{D} L_{G A N}=&amp; \sum_{i} \sum_{j ; j \neq i}\left\{\underset{x_{j} \sim X_{j}}{\mathbb{E}}\left[\log \left(D\left(x_{j}, d_{j}\right)\right)\right]+\right.\\
&amp; \underset{x_{i} \sim X_{i}}{\mathbb{E}}\left[\frac{1}{2} \log \left(1-D\left(G\left(E_{c}\left(x_{i}\right), L_{s}\left(E_{s}\left(x_{j}\right)\right), d_{j}\right), d_{j}\right)\right)\right]+\\
&amp; \underset{z \sim N(0,1)}{\mathbb{E}}\left[\frac{1}{2} \log \left(1-D\left(G\left(E_{c}\left(x_{i}\right), L_{s}(z), d_{j}\right), d_{j}\right)\right)\right\}
\end{aligned}
\]</span> Mode seeking loss <span class="math display">\[
L_{m s}=\max _{G}\left(\sum_{i} \sum_{j ; j \neq i} \frac{d_{I}\left(G\left(E_{c}\left(x_{i}\right), L_{s}\left(z_{1}\right), d_{j}\right), G\left(E_{c}\left(x_{i}\right), L_{s}\left(z_{2}\right), d_{j}\right)\right)}{d_{z}\left(z_{1}, z_{2}\right)}\right)
\]</span></p>
<h2 id="segmentation">Segmentation</h2>
<p><span class="math display">\[
L_{s e g}=\sum_{j, j \neq i} \underset{\hat{x}_{j} \sim \hat{X}_{j}, l_{i} \sim Y_{i}}{\mathbb{E}}\left[\log P\left(l_{i} \mid S_{j}\left(\hat{x}_{j}\right)\right)\right]
\]</span></p>
<p>Joint distribution structure discriminator <span class="math display">\[
L_{s t}=\sum_{j ; j \neq i}\left\{\mathbb{E}_{x_{j} \sim X_{j}}\left[\log \left(D_{s}\left(x_{j}, \psi_{j}, d_{j}\right)\right)\right]+\mathbb{E}_{\hat{x}_{j} \approx X_{j}}\left[\log \left(1-D_{s}\left(\hat{x}_{j}, \hat{\psi}_{j}, d_{j}\right)\right)\right]\right\}
\]</span> Total Loss <span class="math display">\[
L_{\text {total}}=L_{G A N}+\lambda_{\text {vae}} L_{V A E}+\lambda_{c} L_{\text {adv}}^{c}+\lambda_{\text {lr}} L_{l r}+\lambda_{\text {ms}} L_{m s}+\lambda_{\text {st}} L_{s t}+\lambda_{\text {seg}} L_{\text {seg}}
\]</span></p>
<h1 id="experiments">Experiments</h1>
<p><img src="/20200801l11/image-20200801162040843.png"></p>
<p><img src="/20200801l11/image-20200801162055263.png"></p>
<p><img src="/20200801l11/image-20200801162108036.png"></p>
<p><img src="/20200801l11/image-20200801162117165.png"></p>
]]></content>
      <categories>
        <category>阅读笔记</category>
      </categories>
      <tags>
        <tag>2020</tag>
        <tag>MICCAI</tag>
        <tag>图像分割</tag>
        <tag>无监督学习</tag>
      </tags>
  </entry>
  <entry>
    <title>【2020 MICCCAI】Universal Loss Reweighting to Balance Lesion Size Inequality in 3D Medical Image Segmentation</title>
    <url>/20200722l1/</url>
    <content><![CDATA[<p>Code: <a href="https://github.com/neuro-ml/inverse_weighting" target="_blank" rel="external nofollow noopener noreferrer">https://github.com/neuro-ml/inverse_weighting</a></p>
<h1 id="motivation">Motivation</h1>
<p>医学图像分割任务中，目标不平衡问题非常常见，主要分为两类：</p>
<ul>
<li>类别不平衡：正样本（lesion）与负样本（no-lesion）的pixel数量存在极大不平衡</li>
<li>病灶大小不平衡：一个图像中有多个病灶时，大的病灶会遮盖住小的病灶</li>
</ul>
<p>本文针对第二点进行改进。</p>
<p>已有主要思想是为损失函数增加权重以平等地代表每个类别，这种方法主要关注病变类型，而不关注病变的大小。</p>
<p><img src="/20200722l1/image-20200722110155722.png"></p>
<h1 id="methods">Methods</h1>
<p>提出一种称为反向加权inverse weighting (iw)的方法。 <span class="math display">\[
w_{j}=\frac{\sum_{k=0}^{K}\left|L_{k}\right|}{(K+1) \cdot\left|L_{j}\right|}
\]</span> 为每一个incoming patch生成一个向量权重，将相应的ground-truth patch分出K+1个连接块，<span class="math inline">\(L_0, ..., L_K\)</span>，其中<span class="math inline">\(L_0\)</span>为无病变区域（背景），K为当前病灶的数量。<span class="math inline">\(w_j\)</span>为权重，分配给相应分量<span class="math inline">\(L_j\)</span>内每个voxel。分母中的常量确保权重之和等于相同大小的单位张量的总和。</p>
<p><img src="/20200722l1/image-20200722111301929.png"></p>
<p><img src="/20200722l1/image-20200722111320372.png"></p>
<h1 id="experiments">Experiments</h1>
<p><img src="/20200722l1/image-20200722111342820.png"></p>
<p><img src="/20200722l1/image-20200722111356764.png"></p>
]]></content>
      <categories>
        <category>阅读笔记</category>
      </categories>
      <tags>
        <tag>2020</tag>
        <tag>MICCAI</tag>
        <tag>code</tag>
        <tag>损失函数</tag>
        <tag>目标不平衡</tag>
      </tags>
  </entry>
  <entry>
    <title>【2020 CVPR】Unsupervised Instance Segmentation in Microscopy Images via Panoptic Domain Adaptation and Task Re-weighting</title>
    <url>/20200629l1/</url>
    <content><![CDATA[<h1 id="motivation">Motivation</h1>
<p>基于无监督域自适应的数字病理学核实例分割，能够减轻标注的工作量和跨数据集的域迁移负担。</p>
<p>通过学习荧光显微镜图像，提出一种用于组织病理学图像的无监督核分割的Cycle Consistency Panoptic Domain Adaptive Mask R-CNN (CyC-PDAM)</p>
<p><img src="/20200629l1/image-20200629122922119.png"></p>
<p>将目标检测中的UDA方法直接用于实例分割任务主要存在两个问题：</p>
<ul>
<li>现有UDA方法侧重于减轻图像级（图像对比度、亮度等）和示例级别（对象比例、样式等）的域偏差。忽视了语义级别的域迁移（如前景和背景之间的关系以及对象的空间分布）。</li>
<li>UDA目标检测方法是多任务学习，可同时优化不同的损失函数。如果没有学习到域不变特征，则损失函数将使得模型偏向源域。</li>
</ul>
<p>本文贡献：</p>
<ul>
<li>为组织病理学图像中的UDA核实例分割提出了CyC-PDAM模型。</li>
<li>提出了一个简单的原子核修复机制，以去除合成图像中的假阳性对象。</li>
<li>CyC-PDAM通过融合实例级别适应模块和语义级别适应模块，生成了基于全景级别的域不变性特征。</li>
<li>提出了一种任务重新加权机制，减轻对源域的偏置。</li>
<li>与最新的UDA方法相比，CyC-PDAM具有竞争优势。</li>
</ul>
<h1 id="methods">Methods</h1>
<p>提出的方法主要基于CyCADA，将CyCADA与Mask R-CNN相结合。</p>
<p><img src="/20200629l1/image-20200629123041727.png"></p>
<p><img src="/20200629l1/image-20200629125624560.png"></p>
<h2 id="cycada-with-mask-r-cnn">CyCADA with Mask R-CNN</h2>
<p>目前没有用于实例分割的UDA模型，于是设计了域自适应的Mask-R-CNN。</p>
<p>Backbone主要使用ResNet101和Feature Pyramid Network（FPN）。</p>
<p>在FPN后加入一个判别器用于图像级别自适应，实例分支后加入另一个判别器进行实例级别自适应。</p>
<p>图像级判别器由4个卷积层和一个gradient reversal layer（GRL）组成，具体参数如下表。</p>
<p>实例级判别器由3个全连接层和一个GRL组成，输入为<span class="math inline">\(14 \times 14 \times 256\)</span>，平均池化降采样至<span class="math inline">\(2 \times 2 \times 256\)</span>，然后调整为<span class="math inline">\(1024 \times 1\)</span>，再与bounding box分支的<span class="math inline">\(1024 \times 1\)</span>特征相加。</p>
<p><img src="/20200629l1/image-20200629123906184.png"></p>
<h2 id="nuclei-inpainting-mechanism">Nuclei Inpainting Mechanism</h2>
<p>CycleGAN能够很有效的合成目标域图像，但生成图像的标签空间可能会发生变化。合成图像中可能存在多余的和不需要的原子核，这些将被视为背景类。</p>
<p><img src="/20200629l1/image-20200629124923143.png"></p>
<p>首先计算出辅助生成的核的Mask（<span class="math inline">\(M_{aux}\)</span>）：</p>
<p><span class="math display">\[M_{aux}=(otsu(S_{raw})\cup M)-M\]</span></p>
<p>其中，<span class="math inline">\(S_{raw}\)</span>为初始生成的图像，<span class="math inline">\(M\)</span>为对应的Mask，<span class="math inline">\(ostu(S_{raw})\)</span>为使用Otsu二值化的结果。</p>
<p>去除多余的核，得到合成图像<span class="math inline">\(S_{inp}\)</span>。</p>
<p><span class="math display">\[S_{inp}=inp(S_{raw},M_{aux})\]</span></p>
<p>将<span class="math inline">\(M_{aux}\)</span>标记的多余的核替换为未标记的背景像素值，实现原子核修复的视觉效果。直接修复使得合成图像的文理和外观不太真实。但，image-level adaptation能够解决这个问题。</p>
<h2 id="panoptic-level-domain-adaptation">Panoptic Level Domain Adaptation</h2>
<blockquote>
<p>In addition to the image- and feature-level domain bias, the domain shift at the semantic level also exists.</p>
</blockquote>
<p>在FPN的输出后加入语义分支，用于分割预测。</p>
<p>由于荧光显微镜图像和组织病理学图像均可以从组织样品中获取，并且它们可以显示互补和相关的信息，因此合成的和真实的组织病理学图像的这些语义分割标签空间具有很强的相似性。</p>
<p><img src="/20200629l1/image-20200629130643939.png"></p>
<h2 id="task-re-weighting-mechanism">Task Re-weighting Mechanism</h2>
<p>提出了一种任务重新加权机制，以根据域鉴别符的预测为每个特定于任务的损失函数增加一个权衡权重。</p>
<p>设定最终预测任务的源域目标域的特征概率图分别为<span class="math inline">\(p_s\)</span>和<span class="math inline">\(p_t\)</span>，特定任务的损失为<span class="math inline">\(L\)</span>，重新加权的损失为<span class="math inline">\(L_{rw}\)</span>。</p>
<p><span class="math display">\[L_{r w}=\min \left(\frac{p_{t}}{p_{s}}, \beta\right) L=\min \left(\frac{1-p_{s}}{p_{s}}, \beta\right) L\]</span></p>
<p>如果源域趋向1，则减小损失，以减轻模型对源域的偏向。</p>
<p>网络结构中分别对RPN、语义分支、实例分支的损失进行加权。</p>
<h2 id="network-overview-and-training-details">Network Overview and Training Details</h2>
<p><span class="math display">\[\begin{aligned}
L_{\text {pdam}} &amp;=\alpha_{\text {img}} L_{\text {rpn}}+\alpha_{\text {ins}} L_{\text {det}}+\alpha_{\text {sem}} L_{(\text {sem}-s e g)} \\
&amp;+\alpha_{\text {da}}\left(L_{(\text {img}-d a)}+L_{(\text {sem}-d a)}+L_{(\text {ins}-d a)}\right)
\end{aligned}\]</span></p>
<h1 id="experiments">Experiments</h1>
<p><img src="/20200629l1/image-20200629131606545.png"></p>
<p><img src="/20200629l1/image-20200629131623282.png"></p>
<p><img src="/20200629l1/image-20200629131649659.png"></p>
<p><img src="/20200629l1/image-20200629131727655.png"></p>
<p><img src="/20200629l1/image-20200629131740380.png"></p>
]]></content>
      <categories>
        <category>阅读笔记</category>
      </categories>
      <tags>
        <tag>2020</tag>
        <tag>CVPR</tag>
        <tag>无监督学习</tag>
        <tag>实例分割</tag>
      </tags>
  </entry>
  <entry>
    <title>【2016】VoxResNet: Deep Voxelwise Residual Networks for Volumetric Brain Segmentation</title>
    <url>/20200825l1/</url>
    <content><![CDATA[<h1 id="motivation">Motivation</h1>
<p>用于脑部分割的方法主要有3类：</p>
<ul>
<li>具有手工制作功能的机器学习方法，准确识别的能力有限</li>
<li>具有自动学习功能的深度学习方法，需要更优雅的架构来进一步提高性能</li>
<li>基于多图集配准的方法，计算量大，限制了要求快速的应用中的能力</li>
</ul>
<h1 id="methods">Methods</h1>
<p>将ResNet应用于3D MRI脑部图像分割上</p>
<p>网络结构卷积使用conv3d，使用额外的三个辅助分类器</p>
<p><img src="/20200825l1/image-20200825105117693.png"></p>
<p>将不同模态数据concat一起，隐式学习不同模态间的相互关系，实现互补。</p>
<p><img src="/20200825l1/image-20200825105133748.png"></p>
<h1 id="experiments">Experiments</h1>
<p><img src="/20200825l1/image-20200825105148909.png"></p>
<p><img src="/20200825l1/image-20200825105201177.png"></p>
<p><img src="/20200825l1/image-20200825105213636.png"></p>
]]></content>
      <categories>
        <category>阅读笔记</category>
      </categories>
      <tags>
        <tag>图像分割</tag>
        <tag>2016</tag>
        <tag>香港中文大学</tag>
      </tags>
  </entry>
  <entry>
    <title>【2020】YOLOv4: Optimal Speed and Accuracy of Object Detection</title>
    <url>/20200811l1/</url>
    <content><![CDATA[<p>Code: <a href="https://github.com/AlexeyAB/darknet" target="_blank" rel="external nofollow noopener noreferrer">https://github.com/AlexeyAB/darknet</a></p>
<h1 id="motivation">Motivation</h1>
<p><img src="/20200811l1/image-20200811210144034.png"></p>
<p><img src="/20200811l1/image-20200811210213028.png"></p>
<h1 id="methods">Methods</h1>
<p><img src="/20200811l1/image-20200811210240326.png"></p>
<p><img src="/20200811l1/image-20200811210318112.png"></p>
<p><img src="/20200811l1/image-20200811210340602.png"></p>
<h1 id="experiments">Experiments</h1>
<p><img src="/20200811l1/image-20200811210358335.png"></p>
<p><img src="/20200811l1/image-20200811210412043.png"></p>
<p><img src="/20200811l1/image-20200811210424849.png"></p>
<p><img src="/20200811l1/image-20200811210437316.png"></p>
<p><img src="/20200811l1/image-20200811210449096.png"></p>
<p><img src="/20200811l1/image-20200811210511096.png"></p>
<h1 id="acknowledgements"><strong>Acknowledgements</strong></h1>
<p>The authors wish to thank Glenn Jocher for the ideas of Mosaic data augmentation, the selection of hyper-parameters by using genetic algorithms and solving the grid sensitivity problem https://github.com/ultralytics/yolov3.</p>
]]></content>
      <categories>
        <category>阅读笔记</category>
      </categories>
      <tags>
        <tag>2020</tag>
        <tag>code</tag>
        <tag>目标检测</tag>
      </tags>
  </entry>
  <entry>
    <title>Latex: 引入已有PDF文件</title>
    <url>/20200603l2/</url>
    <content><![CDATA[<p>引入包： <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">\usepackage&#123;pdfpages&#125;</span><br></pre></td></tr></table></figure> 引入需要加载的PDF文件 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">\includepdf[pages&#x3D;&#123;1&#125;]&#123;first.pdf&#125;</span><br></pre></td></tr></table></figure></p>
<p>加入页面后想加入空白页可以使用： <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">\cleardoublepage</span><br></pre></td></tr></table></figure></p>
]]></content>
      <categories>
        <category>Latex</category>
      </categories>
      <tags>
        <tag>Latex</tag>
        <tag>PDF</tag>
        <tag>空白页</tag>
      </tags>
  </entry>
  <entry>
    <title>【2020 CVPR】What Can Be Transferred: Unsupervised Domain Adaptation for Endoscopic Lesions Segmentation</title>
    <url>/20200801l3/</url>
    <content><![CDATA[<p>CycleGAN作为Baseline Network</p>
<p><img src="/20200801l3/image-20200429150212008.png"></p>
<p><img src="/20200801l3/image-20200429150308372.png"></p>
]]></content>
      <categories>
        <category>阅读笔记</category>
      </categories>
      <tags>
        <tag>2020</tag>
        <tag>图像分割</tag>
        <tag>CVPR</tag>
      </tags>
  </entry>
  <entry>
    <title>【2018 ICLR】mixup: Beyond Empirical Risk Minimization</title>
    <url>/20200714l1/</url>
    <content><![CDATA[<p>Code: <a href="https://github.com/facebookresearch/mixup-cifar10" target="_blank" rel="external nofollow noopener noreferrer">https://github.com/facebookresearch/mixup-cifar10</a></p>
<h1 id="motivation">Motivation</h1>
<p>在大多数成功的案例中，网络模型有两处共同点：</p>
<ul>
<li>训练神经网络主要依靠经验风险最小化（ERM）</li>
<li>网络的规模和数据集规模成正比</li>
</ul>
<p>ERM收敛的重要保证是模型规模不随数据集规模增加而增加。然而目前的模型大多随着数据集规模发生变化，网络只是单纯“记住了”这些数据。</p>
<h1 id="methods">Methods</h1>
<p>机器学习的目标是为了期望风险最小化：</p>
<p><span class="math display">\[
R(f)=\int \ell(f(x), y) \mathrm{d} P(x, y)
\]</span></p>
<p>其中<span class="math inline">\(\ell\)</span>表示损失函数。实际问题中，<span class="math inline">\(P\)</span>的分布是未知的，通常使用经验分布来近似表示： <span class="math display">\[
P_{\delta}(x, y)=\frac{1}{n} \sum_{i=1}^{n} \delta\left(x=x_{i}, y=y_{i}\right)
\]</span> 由此得到： <span class="math display">\[
R_{\delta}(f)=\int \ell(f(x), y) \mathrm{d} P_{\delta}(x, y)=\frac{1}{n} \sum_{i=1}^{n} \ell\left(f\left(x_{i}\right), y_{i}\right)
\]</span> 经验风险最小化虽然可以计算效率很高，但经验风险只关注有限的n个样例。当数据超出训练样例范围时，可能会出现一些不可知的错误。</p>
<p>本文提出一种通用的邻域分布，称为mixup： <span class="math display">\[
\mu\left(\tilde{x}, \tilde{y} \mid x_{i}, y_{i}\right)=\frac{1}{n} \sum_{j}^{n} \underset{\lambda}{\mathbb{E}}\left[\delta\left(\tilde{x}=\lambda \cdot x_{i}+(1-\lambda) \cdot x_{j}, \tilde{y}=\lambda \cdot y_{i}+(1-\lambda) \cdot y_{j}\right)\right]
\]</span> 其数据为直接将x和y进行混合： <span class="math display">\[
\tilde{x}=\lambda x_i+(1-\lambda)x_j
\]</span></p>
<p><span class="math display">\[
\tilde{y} = \lambda y_i + (1-\lambda) y_i
\]</span></p>
<p><img src="/20200714l1/image-20200714171323416.png"></p>
<p>mixup实现方式非常直观，实现过程中有考虑以下几点：</p>
<ul>
<li>将三个或三个以上融合，效果几乎和两个一样，而且增加了mixup的时间</li>
<li>将mixup应用于单个loader，对同一个minibatch中的数据进行混合，这样的策略和整个数据集随机打乱的效果是一样的，还能减少IO的开销</li>
<li>相同标签的数据进行mixup作用不大</li>
</ul>
<p>mixup的作用：</p>
<p>mixup可以理解为一种数据增强形式，告诉训练器尽可能训练出线性的边界。这样能减少过拟合，而且线性模型最简单，符合奥卡姆剃刀。</p>
<p><img src="/20200714l1/image-20200714171930590.png"></p>
<h1 id="experiments">Experiments</h1>
<h2 id="imagenet-分类">ImageNet 分类</h2>
<p><img src="/20200714l1/image-20200714171952853.png"></p>
<h2 id="cifar-10-与-cifar-100">CIFAR-10 与 CIFAR-100</h2>
<p><img src="/20200714l1/image-20200714172207692.png"></p>
<h2 id="语音数据">语音数据</h2>
<p><img src="/20200714l1/image-20200714172230878.png"></p>
<h2 id="memorization-of-corrupted-labels">Memorization of corrupted labels</h2>
<p><img src="/20200714l1/image-20200714172337180.png"></p>
<h2 id="robustness-to-adversarial-expamples">Robustness to Adversarial Expamples</h2>
<p><img src="/20200714l1/image-20200714172426333.png"></p>
<h2 id="tabular-data">Tabular Data</h2>
<p><img src="/20200714l1/image-20200714195427460.png"></p>
<h2 id="stabilization-of-generative-adversarial-networks">Stabilization of Generative Adversarial Networks</h2>
<p><img src="/20200714l1/image-20200714195502996.png"></p>
<h2 id="ablation-studies">Ablation Studies</h2>
<p><img src="/20200714l1/image-20200714195541847.png"></p>
<h1 id="discussion">Discussion</h1>
<ul>
<li>还没有一个很好的理论来理解这种偏差-方差折中的“最佳点”</li>
<li>作者通过实验推测，增加模型容量将使训练误差对大的<span class="math inline">\(\alpha\)</span>的敏感度降低，因此mixup的优势将更加明显</li>
<li>是否有可能使类似的想法适用于其他类型的监督学习问题，例如回归和结构化预测？尽管讲mixup泛化到回归问题很简单，但应用于结构化预测问题（如图像分割）的问题还不是很明显</li>
<li>类似的方法在监督学习之外可以用吗？插值原理是合理的归纳偏差，也可能有助于无监督、半监督和强化学习。</li>
<li>是否能够将mixup拓展到特征-标签之外来保证远离训练数据时模型的健壮性？</li>
</ul>
]]></content>
      <categories>
        <category>阅读笔记</category>
      </categories>
      <tags>
        <tag>2018</tag>
        <tag>数据增强</tag>
        <tag>code</tag>
        <tag>ICLR</tag>
        <tag>MIT</tag>
      </tags>
  </entry>
  <entry>
    <title>nn-UNet 使用</title>
    <url>/20200812l1/</url>
    <content><![CDATA[<h1 id="github地址">Github地址</h1>
<p><a href="https://github.com/MIC-DKFZ/nnUNet" target="_blank" rel="external nofollow noopener noreferrer">https://github.com/MIC-DKFZ/nnUNet</a></p>
<h1 id="安装">安装</h1>
<p>作者Github上说最好不要用conda,但自己机器已有环境都是用的conda,看其他人说使用conda没问题,那自己也试一下</p>
<h2 id="安装虚拟环境">安装虚拟环境</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">conda create -n nnunet python==3.7</span><br><span class="line">conda activate nnunet</span><br></pre></td></tr></table></figure>
<h2 id="安装pytorch">安装pytorch</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">pip install torch==1.6.0+cu101 torchvision==0.7.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html</span><br></pre></td></tr></table></figure>
<h2 id="安装nvidia-apex">安装Nvidia-Apex</h2>
<p>这是英伟达的一个用于混合精度训练的插件，请不要直接pip，跟着下面的操作来：</p>
<ul>
<li><p>打开<a href="https://github.com/NVIDIA/apex" target="_blank" rel="external nofollow noopener noreferrer">Apex所在的项目网站</a>,翻页到QuickStart</p></li>
<li><p>在用来安装环境的目录下打开终端</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/NVIDIA/apex</span><br></pre></td></tr></table></figure></li>
<li><p>切换工作目录到apex</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> apex</span><br></pre></td></tr></table></figure></li>
<li><p>安装apex</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">pip install -v --no-cache-dir --global-option=<span class="string">"--cpp_ext"</span> --global-option=<span class="string">"--cuda_ext"</span> ./</span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="安装hiddenlayer">安装hiddenlayer</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">pip install --upgrade git+https://github.com/nanohanno/hiddenlayer.git@bugfix/get_trace_graph<span class="comment">#egg=hiddenlayer</span></span><br></pre></td></tr></table></figure>
<h2 id="安装nnunet">安装nnunet</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/MIC-DKFZ/nnUNet.git</span><br><span class="line"><span class="built_in">cd</span> nnUNet</span><br><span class="line">pip install -e .</span><br></pre></td></tr></table></figure>
<p>遇到问题:</p>
<blockquote>
<p>ERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.</p>
<p>We recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.</p>
<p>batchgenerators 0.20.1 requires pillow&lt;7.1, but you'll have pillow 7.2.0 which is incompatible.</p>
</blockquote>
<p>解决方案: 对pillow进行降级处理</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">pip install pillow==7.0.0</span><br></pre></td></tr></table></figure>
<h1 id="配置kits">配置KiTS</h1>
<h2 id="数据预处理">数据预处理</h2>
<p>nnunet已写好kits19的脚本,修改<code>nnunet/dataset_conversion/Task040_KITS.py</code>中数据的输入输出位置即可</p>
<p>运行该python脚本对数据进行预处理</p>
<h3 id="在nnunet目录下创建数据集文件夹">在nnunet目录下创建数据集文件夹</h3>
<ul>
<li>nnUNet_preprocessed</li>
<li>nnUNet_raw/nnUNet_cropped_data</li>
<li>nnUNet_raw/nnUNet_raw_data</li>
<li>nnUNet_trained_models</li>
</ul>
<p>在<code>nnUNet_raw/nnUNet_raw_data</code>目录下创建比赛数据的文件夹(如Task040_kits)</p>
<h3 id="将数据集路径填写到配置文件中">将数据集路径填写到配置文件中</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">vim ~/.bashrc</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">export nnUNet_raw_data_base&#x3D;&quot;&#x2F;home&#x2F;nnUNet&#x2F;DATASET&#x2F;nnUNet_raw&quot;</span><br><span class="line">export nnUNet_preprocessed&#x3D;&quot;&#x2F;home&#x2F;nnUNet&#x2F;DATASET&#x2F;nnUNet_preprocessed&quot;</span><br><span class="line">export RESULTS_FOLDER&#x3D;&quot;&#x2F;home&#x2F;nnUNet&#x2F;DATASET&#x2F;nnUNet_trained_models&quot;</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">source</span> ~/.bashrc</span><br></pre></td></tr></table></figure>
<h3 id="预处理">预处理</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">nnUNet_plan_and_preprocess -t 40</span><br></pre></td></tr></table></figure>
<h1 id="参考文献">参考文献</h1>
<ul>
<li><a href="https://blog.csdn.net/weixin_42061636/article/details/107623757" target="_blank" rel="external nofollow noopener noreferrer">（四：2020.07.28）nnUNet最舒服的训练教程（让我的奶奶也会用nnUNet（上））（8.02更新）</a></li>
</ul>
]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>nnunet</tag>
      </tags>
  </entry>
  <entry>
    <title>【2020】ResNeSt: Split-Attention Networks</title>
    <url>/20200424l1/</url>
    <content><![CDATA[<h1 id="引言">引言</h1>
<h2 id="概述">概述</h2>
<p>本文主要介绍ResNet的变体：ResNeSt。</p>
<p>目前代码已经提供PyTorch和MXNet两个版本。</p>
<p>【Code】<a href="https://github.com/zhanghang1989/ResNeSt" target="_blank" rel="external nofollow noopener noreferrer">https://github.com/zhanghang1989/ResNeSt</a></p>
<p>性能显著提升，参数量并没有显著增加。</p>
<p>借鉴了：Multi-path 和 Feature-map Attention思想。</p>
<p>其中：</p>
<ul>
<li>GoogleNet 采用了Multi-path机制，其中每个网络块均由不同的卷积kernels组成。</li>
<li>ResNeXt在ResNet bottle模块中采用组卷积，将multi-path结构转换为统一操作。</li>
<li>SE-Net 通过自适应地重新校准通道特征响应来引入通道注意力（channel-attention）机制。</li>
<li>SK-Net 通过两个网络分支引入特征图注意力（feature-map attention）。</li>
</ul>
<h2 id="主要贡献">主要贡献</h2>
<ul>
<li>在ResNet的基础上进行了修改，结合了feature-map split attention机制。</li>
<li>在图像分类和迁移学习上都有很大提升，可以作为这些领域的benchmark。</li>
</ul>
<h1 id="方法">方法</h1>
<h2 id="概述-1">概述</h2>
<p>ResNeSt主要引入了Split-Attention block，由feature-map group和split attention operation组成。</p>
<p>SE-Net、SK-Net、ResNeSt的结构图如下：</p>
<p><img src="/20200424l1/image-20200424121613880.png"></p>
<p>其中Split Attention的结构如下：</p>
<p><img src="/20200424l1/image-20200424121729777.png"></p>
<p>从图1和图2可知，都有split的影子。比如图1中的 <span class="math inline">\(K(k)\)</span> 和图2中的 <span class="math inline">\(R(r)\)</span> 都是超参数，也就是共计 <span class="math inline">\(G = K*R\)</span> 组。</p>
<h2 id="feature-map-group">Feature-map Group</h2>
<p>如图 2，仿照ResNeXt block，将特征分为多个组，特征组的数量由超参数K控制。</p>
<p>每一个组由分成多个cardinal组，数量由R控制。</p>
<p>因此所有组的数量为<span class="math inline">\(G=K\times R\)</span>。</p>
<p>每个小组应用一系列变化<span class="math inline">\(\{F_1, F2, ..., F_G\}\)</span>，每个组的中间表示使用<span class="math inline">\(U_i=F_i(X)\)</span>，其中<span class="math inline">\(i \in \{1,2,...,G\}\)</span>。</p>
<h2 id="split-attention-in-cardinal-groups">Split Attention in Cardinal Groups</h2>
<p>受SENet和SKNet启发，每一个cardinal组的表达通过像素级相加的方式融合起来，公式表示为<span class="math inline">\(\hat{U}^k=\sum_{j=R(k-1)+1}^{Rk}{U_j}\)</span>，其中<span class="math inline">\(\hat{U}^k \in R^{H \times H \times C/K}, k \in 1,2,...,K\)</span>。</p>
<p>通过空间维度的全局平均池化实现聚合通道维度的全局上下文信息，第c个组件具体公式如下： <span class="math display">\[
s_c^k=\frac{1}{H \times W}\sum_{i-1}^{H}\sum_{j=1}^{W}{\hat{U}_c^k(i,j)}
\]</span> cardinal组表示的加权融合使用通道维度soft注意力实现，其中每个channel的特征图使用加权组合产生。 <span class="math display">\[
V_{c}^{k}=\sum_{i=1}^{R} a_{i}^{k}(c) U_{R(k-1)+i}
\]</span></p>
<p><span class="math display">\[
a_{i}^{k}(c)=\left\{\begin{array}{ll}
\frac{\exp \left(\mathcal{G}_{i}^{c}\left(s^{k}\right)\right)}{\sum_{j=0}^{R} \exp \left(\mathcal{G}_{j}^{c}\left(s^{k}\right)\right)} &amp; \text { if } R&gt;1 \\
\frac{1}{1+\exp \left(-\mathcal{G}_{i}^{c}\left(s^{k}\right)\right)} &amp; \text { if } R=1
\end{array}\right.
\]</span></p>
<h2 id="resnest-block">ResNeSt Block</h2>
<p>最终结果通过short cut连接：<span class="math inline">\(Y = V+X\)</span></p>
<p>当特征图大小不匹配时加入相应的变换：<span class="math inline">\(Y=V+T(X)\)</span>，<span class="math inline">\(T\)</span>可以使用strided convolution或combined convolution-with-pooling。</p>
<h2 id="network-tweaks">Network Tweaks</h2>
<h3 id="average-downsampling">Average Downsampling</h3>
<p>对于某些要求空间信息的拓展任务来说保存空间信息使必要的。在此之前，ResNet通过带有stride的<span class="math inline">\(3 \times 3\)</span>来实现降采样，但是这样对于边界的处理需要采用padding操作。而在本文中直接使用<span class="math inline">\(3 \times 3\)</span>平均池化操作。</p>
<h3 id="tweaks-from-resnet-d">Tweaks from ResNet-D</h3>
<ol type="1">
<li>将原始的<span class="math inline">\(7 \times 7\)</span>卷积操作替换成3个<span class="math inline">\(3 \times 3\)</span>的卷积操作，他们具有相同的感受野，计算量也较为相似。</li>
<li>在shortcut中<span class="math inline">\(1 \times 1\)</span>卷积层前加入<span class="math inline">\(2 \times 2\)</span>平均池化。</li>
</ol>
<h2 id="训练策略">训练策略</h2>
<p>这个对大家目前的工作应该具有很大的参考价值（涨点tricks）。</p>
<ul>
<li>Large Mini-batch Distributed Training</li>
<li>Label Smoothing</li>
<li>Auto Augmentation</li>
<li>Mixup Training</li>
<li>Large Crop Size</li>
<li>Regularization</li>
</ul>
<h1 id="实验结果">实验结果</h1>
<h2 id="图像分类">图像分类</h2>
<p><img src="/20200424l1/image-20200424141553855.png"></p>
<p><img src="/20200424l1/image-20200424141606687.png"></p>
<h2 id="目标检测">目标检测</h2>
<p><img src="/20200424l1/image-20200424141715775.png"></p>
<h2 id="实例分割">实例分割</h2>
<p><img src="/20200424l1/image-20200424141737925.png"></p>
<h2 id="语义分割">语义分割</h2>
<p><img src="/20200424l1/image-20200424141754790.png"></p>
<h1 id="作者的次要结论">作者的次要结论</h1>
<ul>
<li>depth-wise convolution is not optimal for training and inference efficiency on GPU,</li>
<li>model accuracy get saturated on ImageNet with a fixed input image size,</li>
<li>increasing input image size can get better accuracy and FLOPS trade-off.</li>
<li>bicubic upsampling strategy is needed for large crop-size (≥ 320).</li>
</ul>
<h1 id="作者解答">作者解答</h1>
<h2 id="这里的attention和sknet挺像">这里的attention和sknet挺像</h2>
<p>问题来自<a href="https://www.zhihu.com/question/388637660" target="_blank" rel="external nofollow noopener noreferrer">知乎</a></p>
<blockquote>
<p>1). 我们相当于把 SKNet 的思想做到 ResNeXt 的 group 里更有效，而且相对于多分支的 SKNet 更模块化，易于优化。</p>
<p>2). 这篇 paper 其实并不是一个什么开创性的工作，感觉大家有点过高期待了，只是几个小伙伴在繁忙工作中抽出3个多星期的时间一起赶出来的，为了全面提高 gluoncv 项目里的所有模型的性能，最简单的方法就是提高 backbone。仅此而已。</p>
</blockquote>
<h1 id="参考文献">参考文献</h1>
<ol type="1">
<li><a href="https://zhuanlan.zhihu.com/p/132655457" target="_blank" rel="external nofollow noopener noreferrer">ResNet最强改进版来了！ResNeSt：Split-Attention Networks</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/133496926" target="_blank" rel="external nofollow noopener noreferrer">ResNeSt: Split-Attention Networks阅读笔记</a></li>
<li><a href="https://www.zhihu.com/question/388637660" target="_blank" rel="external nofollow noopener noreferrer">如何评价ResNeSt：Split-Attention Networks？</a></li>
</ol>
]]></content>
      <categories>
        <category>阅读笔记</category>
      </categories>
      <tags>
        <tag>2020</tag>
        <tag>code</tag>
        <tag>阅读笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>论文阅读</title>
    <url>/20200101l1/</url>
    <content><![CDATA[<h1 id="section">2020</h1>
<h2 id="eur-j-nucl-med-mol-i">EUR J NUCL MED MOL I</h2>
<h3 id="section-1"><a href="/20200807l1/" title="CTumorGAN: a unified framework for automatic computed tomography tumor segmentation">CTumorGAN: a unified framework for automatic computed tomography tumor segmentation</a></h3>
<p><img src="/20200101l1/image-20200807172020185.png"></p>
<p><img src="/20200101l1/image-20200807172752184.png"></p>
<h2 id="mia">MIA</h2>
<a href="/20200914l1/" title="Marginal loss and exclusion loss for partially supervised multi-organ segmentation">Marginal loss and exclusion loss for partially supervised multi-organ segmentation</a>
<p>部分监督的多器官分割的边际损失和排斥损失</p>
<p><img src="/20200101l1/image-20200914143606638.png"></p>
<p><img src="/20200101l1/image-20200914150719683.png"></p>
<h2 id="miccai">MICCAI</h2>
<h3 id="section-2"><a href="/20200824l1/" title="Multi-organ Segmentation via Co-training Weight-averaged Models from Few-organ Datasets">Multi-organ Segmentation via Co-training Weight-averaged Models from Few-organ Datasets</a></h3>
<p>基于联合训练加权平均模型的多器官分割</p>
<p><img src="/20200101l1/image-20200824143837472.png"></p>
<h3 id="section-3"><a href="/20200805l1/" title="Learning Semantics-enriched Representation via Self-discovery, Self-classification, and Self-restoration">Learning Semantics-enriched Representation via Self-discovery, Self-classification, and Self-restoration</a></h3>
<p>通过自我发现、自我分类和自我修复来学习语义丰富的表征</p>
<p><img src="/20200101l1/image-20200805103106974.png"></p>
<h3 id="section-4"><a href="/20200804l1/" title="Cross-Domain Medical Image Translation by Shared Latent Gaussian Mixture Model">Cross-Domain Medical Image Translation by Shared Latent Gaussian Mixture Model</a></h3>
<p>基于共享潜在高斯混合模型的跨域医学图像翻译</p>
<p><img src="/20200101l1/image-20200804144948326.png"></p>
<h3 id="section-5"><a href="/20200803l1/" title="Towards Emergent Language Symbolic SemanticSegmentation and Model Interpretability">Towards Emergent Language Symbolic SemanticSegmentation and Model Interpretability</a></h3>
<p>面向新兴语言的符号语义分段与模型可解释性</p>
<p><img src="/20200101l1/image-20200803104822471.png"></p>
<h3 id="section-6"><a href="/20200801l11/" title="Unified cross-modality feature disentangler for unsupervised multi-domain MRI abdomen organs segmentation">Unified cross-modality feature disentangler for unsupervised multi-domain MRI abdomen organs segmentation</a></h3>
<p>统一的跨模态特征分解器用于无监督的多域MRI腹部器官分割</p>
<p><img src="/20200101l1/image-20200801154439206.png"></p>
<h3 id="section-7"><a href="/20200728l1/" title="Instance-aware Self-supervised Learning for Nuclei Segmentation">Instance-aware Self-supervised Learning for Nuclei Segmentation</a></h3>
<p>实例级自监督学习核分割</p>
<p><img src="/20200101l1/image-20200728091137247.png"></p>
<p><img src="/20200101l1/image-20200728091215708.png"></p>
<h3 id="section-8"><a href="/20200729l1/" title="A Macro-Micro Weakly-supervised Framework for AS-OCT Tissue Segmentation">A Macro-Micro Weakly-supervised Framework for AS-OCT Tissue Segmentation</a></h3>
<p>用于AS-OCT组织分割的宏观-微观弱监督学习框架</p>
<p><img src="/20200101l1/image-20200729162055083.png"></p>
<h3 id="section-9"><a href="/20200727l1/" title="Learning Directional Feature Maps for Cardiac MRI Segmentation">Learning Directional Feature Maps for Cardiac MRI Segmentation</a></h3>
<p>用于心脏MRI分割的方向特征图学习</p>
<p>Code: <a href="https://github.com/c-feng/DirectionalFeature" target="_blank" rel="external nofollow noopener noreferrer">https://github.com/c-feng/DirectionalFeature</a></p>
<p><img src="/20200101l1/image-20200727150542488.png"></p>
<p><img src="/20200101l1/image-20200727150935272.png"></p>
<h3 id="section-10"><a href="/20200628l1/" title="PraNet: Parallel Reverse Attention Network for Polyp Segmentation">PraNet: Parallel Reverse Attention Network for Polyp Segmentation</a></h3>
<p>PraNet：用于息肉分割的并行反向注意网络</p>
<p>Code: <a href="https://github.com/DengPingFan/PraNet" target="_blank" rel="external nofollow noopener noreferrer">https://github.com/DengPingFan/PraNet</a></p>
<p><img src="/20200101l1/image-20200628204135114.png"></p>
<h3 id="section-11"><a href="/20200731l1/" title="Self-Loop Uncertainty: A Novel Pseudo-Label for Semi-Supervised Medical Image Segmentation">Self-Loop Uncertainty: A Novel Pseudo-Label for Semi-Supervised Medical Image Segmentation</a></h3>
<p>自环不确定性：一种用于半监督医学图像分割的新型伪标签</p>
<p><img src="/20200101l1/image-20200731095630976.png"></p>
<p><img src="/20200101l1/image-20200731095742223.png"></p>
<h3 id="section-12"><a href="/20200722l1/" title="Universal Loss Reweighting to Balance Lesion Size Inequality in 3D Medical Image Segmentation">Universal Loss Reweighting to Balance Lesion Size Inequality in 3D Medical Image Segmentation</a></h3>
<p>通用损失重新加权以平衡3D医学图像分割中的病变大小不平等</p>
<p>Code: <a href="https://github.com/neuro-ml/inverse_weighting" target="_blank" rel="external nofollow noopener noreferrer">https://github.com/neuro-ml/inverse_weighting</a></p>
<p><img src="/20200101l1/image-20200722110155722.png"></p>
<h2 id="cvpr">CVPR</h2>
<h3 id="section-13"><a href="/20200712l1/" title="C2FNAS: Coarse-to-Fine Neural Architecture Search for 3D Medical Image Segmentation">C2FNAS: Coarse-to-Fine Neural Architecture Search for 3D Medical Image Segmentation</a></h3>
<p>C2FNAS：从粗到细的神经体系结构搜索3D医学图像分割</p>
<p><img src="/20200101l1/image-20200712091058072.png"></p>
<p><img src="/20200101l1/image-20200712093605764.png"></p>
<h3 id="section-14"><a href="/20200731l4/" title="CenterMask: single shot instance segmentation with point representation">CenterMask: single shot instance segmentation with point representation</a></h3>
<p>CenterMask：具有点表示的single-shot实例分割</p>
<p><img src="/20200101l1/image-20200731203940571.png"></p>
<p><img src="/20200101l1/image-20200731204006555.png"></p>
<h3 id="section-15"><a href="/20200630l1/" title="CPR-GCN:Conditional Partial-Residual Graph Convolutional Network in Automated Anatomical Labeling of Coronary Arteries">CPR-GCN:Conditional Partial-Residual Graph Convolutional Network in Automated Anatomical Labeling of Coronary Arteries</a></h3>
<p>CPR-GCN：条件部分残差图卷积网络在灌装动脉自动解剖标记中的应用</p>
<p><img src="/20200101l1/image-20200630142318333.png"></p>
<p><img src="/20200101l1/image-20200630145438310.png"></p>
<h3 id="section-16"><a href="/20200702l1/" title="FOAL: Fast Online Adaptive Learning for cardiac motion estimation">FOAL: Fast Online Adaptive Learning for cardiac motion estimation</a></h3>
<p>FOAL：用于心脏运动估计的快速在线自适应学习</p>
<p><img src="/20200101l1/image-20200703111837160.png"></p>
<p><img src="/20200101l1/image-20200703112008806.png"></p>
<h3 id="section-17"><a href="/20200701l1/" title="FocalMix: Semi-Supervised Learning for 3D Medical Image Detection">FocalMix: Semi-Supervised Learning for 3D Medical Image Detection</a></h3>
<p>FocalMix:用于3D医学图像检测的半监督学习</p>
<p><img src="/20200101l1/image-20200701221231901.png"></p>
<h3 id="section-18"><a href="/20200731l3/" title="PolarMask: Single Shot Instance Segmentation with Polar Representation">PolarMask: Single Shot Instance Segmentation with Polar Representation</a></h3>
<p>PolarMask：具有极坐标表示形式的单发实例分割</p>
<p>Code: <a href="https://github.com/xieenze/PolarMask" target="_blank" rel="external nofollow noopener noreferrer">https://github.com/xieenze/PolarMask</a></p>
<p><img src="/20200101l1/image-20200731173037090.png"></p>
<h3 id="section-19"><a href="/20200731l5/" title="Strip Pooling: Rethinking Spatial Pooling for Scene Parsing">Strip Pooling: Rethinking Spatial Pooling for Scene Parsing</a></h3>
<p>Strip Pooling: 重新考虑空间池以进行场景解析</p>
<p>Code: <a href="https://github.com/Andrew-Qibin/SPNet" target="_blank" rel="external nofollow noopener noreferrer">https://github.com/Andrew-Qibin/SPNet</a></p>
<p><img src="/20200101l1/image-20200420115653369.png"></p>
<p><img src="/20200101l1/image-20200420115704116.png"></p>
<p><img src="/20200101l1/image-20200420115715342.png"></p>
<h3 id="section-20"><a href="/20200627l2/" title="Structure Boundary Preserving Segmentation for Medical Image with Ambiguous Boundary">Structure Boundary Preserving Segmentation for Medical Image with Ambiguous Boundary</a></h3>
<p>具有模糊边界的医学图像保持结构边界分割</p>
<p><img src="/20200101l1/image-20200627201903381.png"></p>
<p><img src="/20200101l1/image-20200627203826128.png"></p>
<h3 id="section-21"><a href="/20200627l1/" title="Synthetic Learning: Learn From Distributed Asynchronized Discriminator GAN Without Sharing Medical Image Data">Synthetic Learning: Learn From Distributed Asynchronized Discriminator GAN Without Sharing Medical Image Data</a></h3>
<p>综合学习：在不共享医学图像数据的情况下从分布式异步学习判别器GAN</p>
<p>Code: <a href="https://github.com/tommy-qichang/AsynDGAN" target="_blank" rel="external nofollow noopener noreferrer">https://github.com/tommy-qichang/AsynDGAN</a></p>
<p><img src="/20200101l1/image-20200627141939598.png"></p>
<h3 id="section-22"><a href="/20200629l1/" title="Unsupervised Instance Segmentation in Microscopy Images via Panoptic Domain Adaptation and Task Re-weighting">Unsupervised Instance Segmentation in Microscopy Images via Panoptic Domain Adaptation and Task Re-weighting</a></h3>
<p>基于全景域自适应和任务重加权的显微图像无监督实例分割</p>
<p><img src="/20200101l1/image-20200629123041727.png"></p>
<p><img src="/20200101l1/image-20200629125624560.png"></p>
<h3 id="section-23"><a href="/20200801l3/" title="What Can Be Transferred: Unsupervised Domain Adaptation for Endoscopic Lesions Segmentation">What Can Be Transferred: Unsupervised Domain Adaptation for Endoscopic Lesions Segmentation</a></h3>
<p>What Can Be Transferred：用于内窥镜病变分割的无监督域自适应</p>
<p><img src="/20200101l1/image-20200429150212008.png"></p>
<p><img src="/20200101l1/image-20200429150308372.png"></p>
<h2 id="eccv">ECCV</h2>
<h3 id="section-24"><a href="/20200810l1/" title="Conditional Convolutions for Instance Segmentation">Conditional Convolutions for Instance Segmentation</a></h3>
<p>用于实例分割的条件卷积</p>
<p>Code: <a href="https://github.com/aim-uofa/AdelaiDet/" target="_blank" rel="external nofollow noopener noreferrer">https://github.com/aim-uofa/AdelaiDet/</a></p>
<p><img src="/20200101l1/image-20200810164721230.png"></p>
<h3 id="section-25"><a href="/20200809l1/" title="Semantic Flow for Fast and Accurate Scene Parsing">Semantic Flow for Fast and Accurate Scene Parsing</a></h3>
<p>语义流用于快速准确进行场景解析</p>
<p>Code: <a href="https://github.com/donnyyou/torchcv" target="_blank" rel="external nofollow noopener noreferrer">https://github.com/donnyyou/torchcv</a></p>
<p><img src="/20200101l1/image-20200809171425537.png"></p>
<p><img src="/20200101l1/image-20200809171538930.png"></p>
<h3 id="section-26"><a href="/20200805l2/" title="Feature Pyramid Transformer">Feature Pyramid Transformer</a></h3>
<p>特征金字塔 Transformer</p>
<p><img src="/20200101l1/image-20200805210246785.png"></p>
<h3 id="section-27"><a href="/20200711l1/" title="Mining Cross-Image Semantics for Weakly Supervised Semantic Segmentation">Mining Cross-Image Semantics for Weakly Supervised Semantic Segmentation</a></h3>
<p>挖掘跨图像语义的弱监督语义分割</p>
<p>Code: <a href="https://github.com/GuoleiSun/MCIS_wsss" target="_blank" rel="external nofollow noopener noreferrer">https://github.com/GuoleiSun/MCIS_wsss</a></p>
<p><img src="/20200101l1/image-20200711192732553.png"></p>
<p><img src="/20200101l1/image-20200801123537924.png"></p>
<h2 id="tip">TIP</h2>
<h3 id="section-28"><a href="/20200519l1/" title="Coarse-to-Fine Semantic Segmentation From Image-Level Labels">Coarse-to-Fine Semantic Segmentation From Image-Level Labels</a></h3>
<p>图像级标签的粗到细语义分割</p>
<p><img src="/20200101l1/image-20200520091439726.png"></p>
<p><img src="/20200101l1/image-20200520092358141.png"></p>
<h2 id="cmpb">CMPB</h2>
<h3 id="section-29"><a href="/20200801l6/" title="Hybrid Attention for Automatic Segmentation of Whole Fetal Head in Prenatal Ultrasound Volumes">Hybrid Attention for Automatic Segmentation of Whole Fetal Head in Prenatal Ultrasound Volumes</a></h3>
<p>混合注意在产前超声检查中自动分割整个胎儿头</p>
<p><img src="/20200101l1/image-20200510124602869.png"></p>
<p><img src="/20200101l1/image-20200510124632480.png"></p>
<h2 id="isbi">ISBI</h2>
<h3 id="section-30"><a href="/20200801l5/" title="DRU-net: An Efficient Deep Convolutional Neural Network for Medical Image Segmentation">DRU-net: An Efficient Deep Convolutional Neural Network for Medical Image Segmentation</a></h3>
<p>DRU-net：用于医学图像的高效深度卷积神经网络</p>
<p><img src="/20200101l1/image-20200510105245070.png"></p>
<h2 id="ijcai">IJCAI</h2>
<h3 id="section-31"><a href="/20200801l2/" title="Rectifying Pseudo Label Learning via Uncertainty Estimation for Domain Adaptive Semantic Segmentation">Rectifying Pseudo Label Learning via Uncertainty Estimation for Domain Adaptive Semantic Segmentation</a></h3>
<p>利用Uncertainty修正Domain Adaptation中的伪标签</p>
<p><img src="/20200101l1/image-20200427180645555.png"></p>
<p><img src="/20200101l1/image-20200427180807836.png"></p>
<h2 id="iclr">ICLR</h2>
<h3 id="section-32"><a href="/20200801l10/" title="U-GAT-IT: Unsupervised Generative Attentional Networks with Adaptive Layer-Instance Normalization for Image-to-Image Translation">U-GAT-IT: Unsupervised Generative Attentional Networks with Adaptive Layer-Instance Normalization for Image-to-Image Translation</a></h3>
<p>U-GAT-IT：具有图像到图像转换的自适应层实例规范化的无监督生成注意网络</p>
<p><img src="/20200101l1/image-20200420120144704.png"></p>
<p><img src="/20200101l1/image-20200420120158234.png"></p>
<p><img src="/20200101l1/image-20200420120213245.png"></p>
<h2 id="arxiv">Arxiv</h2>
<h3 id="section-33"><a href="/20201031l1/" title="Big Self-Supervised Models are Strong Semi-Supervised Learners">Big Self-Supervised Models are Strong Semi-Supervised Learners</a></h3>
<p>大型自我监督模型是强大的半监督学习者</p>
<p>Code: <a href="https://github.com/google-research/simclr" target="_blank" rel="external nofollow noopener noreferrer">https://github.com/google-research/simclr</a></p>
<p>Pdf: <a href="https://arxiv.org/abs/2006.10029" target="_blank" rel="external nofollow noopener noreferrer">https://arxiv.org/abs/2006.10029</a></p>
<p><img src="/20200101l1/image-20201031184149183.png"></p>
<h3 id="section-34"><a href="/20201031l1/" title="EfficientSeg: An Efficient Semantic Segmentation Network">EfficientSeg: An Efficient Semantic Segmentation Network</a></h3>
<p>EfficientSeg：高效的语义分割网络</p>
<p>Pdf: <a href="https://arxiv.org/abs/2009.06469" target="_blank" rel="external nofollow noopener noreferrer">https://arxiv.org/abs/2009.06469</a></p>
<p><img src="/20200101l1/image-20201031182939034.png"></p>
<h3 id="section-35"><a href="/20201031l1/" title="Selective Information Passing for MR&#x2F;CT Image Segmentation">Selective Information Passing for MR&#x2F;CT Image Segmentation</a></h3>
<p>MR/CT图像分割的选择性信息传递</p>
<p>Code: <a href="https://github.com/ahukui/SIPNet" target="_blank" rel="external nofollow noopener noreferrer">https://github.com/ahukui/SIPNet</a></p>
<p>Pdf: <a href="https://arxiv.org/abs/2010.04920" target="_blank" rel="external nofollow noopener noreferrer">https://arxiv.org/abs/2010.04920</a></p>
<p><img src="/20200101l1/image-20201031182240321.png"></p>
<h3 id="section-36"><a href="/20201030l1/" title="AbdomenCT-1K: Is Abdominal Organ Segmentation A Solved Problem?">AbdomenCT-1K: Is Abdominal Organ Segmentation A Solved Problem?</a></h3>
<p>AbdomenCT-1K：腹部器官分割是解决问题吗？</p>
<p>Code: <a href="https://github.com/JunMa11/AbdomenCT-1K" target="_blank" rel="external nofollow noopener noreferrer">https://github.com/JunMa11/AbdomenCT-1K</a></p>
<p>Pdf: <a href="https://arxiv.org/abs/2010.14808" target="_blank" rel="external nofollow noopener noreferrer">https://arxiv.org/abs/2010.14808</a></p>
<h3 id="section-37"><a href="/20201022l1/" title="PseudoSeg: Designing pseudo labels for semantic segmentation">PseudoSeg: Designing pseudo labels for semantic segmentation</a></h3>
<p>pseudo-seg：为语义分割设计伪标签</p>
<p>Code: <a href="https://github.com/googleinterns/wss" target="_blank" rel="external nofollow noopener noreferrer">https://github.com/googleinterns/wss</a></p>
<p><img src="/20200101l1/image-20201022192002650.png"></p>
<h3 id="section-38"><a href="/20200811l1/" title="YOLOv4: Optimal Speed and Accuracy of Object Detection">YOLOv4: Optimal Speed and Accuracy of Object Detection</a></h3>
<p>YOLOv4：目标检测的最佳速度和准确性</p>
<p>Code: <a href="https://github.com/AlexeyAB/darknet" target="_blank" rel="external nofollow noopener noreferrer">https://github.com/AlexeyAB/darknet</a></p>
<p><img src="/20200101l1/image-20200811210340602.png"></p>
<h3 id="section-39"><a href="/20200801l7/" title="Medical Image Segmentation Using a U-Net type of Architecture">Medical Image Segmentation Using a U-Net type of Architecture</a></h3>
<p>使用U-Net类型的架构进行医学图像分割</p>
<p><img src="/20200101l1/image-20200516170336554.png"></p>
<h3 id="section-40"><a href="/20200623l1/" title="PointRend: Image Segmentation as Rendering">PointRend: Image Segmentation as Rendering</a></h3>
<p>PointRend：将图像分割看作渲染</p>
<p>Code: <a href="https://github.com/facebookresearch/detectron2/tree/master/projects/PointRend" target="_blank" rel="external nofollow noopener noreferrer">https://github.com/facebookresearch/detectron2/tree/master/projects/PointRend</a></p>
<p><img src="/20200101l1/image-20200623160128310.png"></p>
<p><img src="/20200101l1/image-20200623160207304.png"></p>
<p><img src="/20200101l1/image-20200623160231982.png"></p>
<h3 id="section-41"><a href="/20200801l8/" title="Progressive Adversarial Semantic Segmentation">Progressive Adversarial Semantic Segmentation</a></h3>
<p>渐进式对抗语义分割</p>
<p><img src="/20200101l1/image-20200517102005015.png"></p>
<h3 id="section-42"><a href="/20200424l1/" title="ResNeSt: Split-Attention Networks">ResNeSt: Split-Attention Networks</a></h3>
<p>ResNeSt：注意力分散网络</p>
<p>Code: <a href="https://github.com/zhanghang1989/ResNeSt" target="_blank" rel="external nofollow noopener noreferrer">https://github.com/zhanghang1989/ResNeSt</a></p>
<p><img src="/20200101l1/image-20200424121613880.png"></p>
<p><img src="/20200101l1/image-20200424121729777.png"></p>
<h3 id="section-43"><a href="/20200724l1/" title="Self-Supervised Nuclei Segmentation in Histopathological Images Using Attention">Self-Supervised Nuclei Segmentation in Histopathological Images Using Attention</a></h3>
<p>基于注意力的组织病理图像自监督细胞核分割</p>
<p>Code: <a href="https://github.com/msahasrabudhe/miccai2020_self_sup_nuclei_seg" target="_blank" rel="external nofollow noopener noreferrer">https://github.com/msahasrabudhe/miccai2020_self_sup_nuclei_seg</a></p>
<p><img src="/20200101l1/image-20200724111338865.png"></p>
<h3 id="section-44"><a href="/20200720l2/" title="ClassMix: Segmentation-Based Data Augmentation for Semi-Supervised Learning">ClassMix: Segmentation-Based Data Augmentation for Semi-Supervised Learning</a></h3>
<p>ClassMix：基于分割的半监督学习数据增强</p>
<p>Code: <a href="https://github.com/WilhelmT/ClassMix" target="_blank" rel="external nofollow noopener noreferrer">https://github.com/WilhelmT/ClassMix</a></p>
<p><img src="/20200101l1/image-20200720170305343.png"></p>
<p><img src="/20200101l1/image-20200720170207551.png"></p>
<h3 id="section-45"><a href="/20200713l1/" title="Uncertainty-based graph convolutional networks for organ segmentation refinement">Uncertainty-based graph convolutional networks for organ segmentation refinement</a></h3>
<p>基于不确定度的图卷积网络用于器官分割细化</p>
<p><img src="/20200101l1/image-20200713134429510.png"></p>
<h3 id="section-46"><a href="/20200801l4/" title="A novel Region of Interest Extraction Layer for Instance Segmentation">A novel Region of Interest Extraction Layer for Instance Segmentation</a></h3>
<p>用于实例分割的新型兴趣区域提取层</p>
<p>Code: <a href="https://github.com/IMPLabUniPr/mmdetection-groie" target="_blank" rel="external nofollow noopener noreferrer">https://github.com/IMPLabUniPr/mmdetection-groie</a></p>
<p><img src="/20200101l1/image-20200508205358554.png"></p>
<h3 id="section-47"><a href="/20200801l1/" title="An automatic COVID-19 CT segmentation based on U-Net with attention mechanism">An automatic COVID-19 CT segmentation based on U-Net with attention mechanism</a></h3>
<p>基于注意力机制的基于U-Net的COVID-19 CT自动分割</p>
<p><img src="/20200101l1/image-20200801091847025.png"></p>
<p><img src="/20200101l1/image-20200801091916347.png"></p>
<p><img src="/20200101l1/image-20200801091943733.png"></p>
<h1 id="section-48">2019</h1>
<h2 id="miccai-1">MICCAI</h2>
<h3 id="section-49"><a href="/20200619l1/" title="Improving RetinaNet for CT Lesion Detection with Dense Masks from Weak RECIST Labels">Improving RetinaNet for CT Lesion Detection with Dense Masks from Weak RECIST Labels</a></h3>
<p>用弱RECIST标记的致密膜改善RetinaNet对CT病变的检测</p>
<p><img src="/20200101l1/image-20200619111759526.png"></p>
<h2 id="cvpr-1">CVPR</h2>
<h3 id="section-50"><a href="/20200801l9/" title="Dual Attention Network for Scene Segmentation">Dual Attention Network for Scene Segmentation</a></h3>
<p>双注意力网络用于场景分割</p>
<p>Code: <a href="https://github.com/junfu1115/DANet/" target="_blank" rel="external nofollow noopener noreferrer">https://github.com/junfu1115/DANet/</a></p>
<p><img src="/20200101l1/image-20200420120019703.png"></p>
<p><img src="/20200101l1/image-20200420120052772.png"></p>
<h2 id="iccv">ICCV</h2>
<h3 id="section-51"><a href="/20200917l1/" title="Prior-aware Neural Network for Partially-Supervised Multi-Organ Segmentation">Prior-aware Neural Network for Partially-Supervised Multi-Organ Segmentation</a></h3>
<p>用于部分监管的多组织分割的先验感知神经网络</p>
<p><img src="/20200101l1/image-20200917214449754.png"></p>
<h3 id="section-52"><a href="/20200720l1/" title="CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features">CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features</a></h3>
<p>CutMix：正则化策略来训练具有可局部化的强大分类器</p>
<p>Code: <a href="https://github.com/clovaai/CutMix-PyTorch" target="_blank" rel="external nofollow noopener noreferrer">https://github.com/clovaai/CutMix-PyTorch</a></p>
<p><img src="/20200101l1/image-20200720154731427.png"></p>
<h3 id="section-53"><a href="/20200731l2/" title="FCOS: Fully Convolutional One-Stage Object Detection">FCOS: Fully Convolutional One-Stage Object Detection</a></h3>
<p>FCOS：完全卷积一阶段对象检测</p>
<p>Code: <a href="https://github.com/tianzhi0549/FCOS" target="_blank" rel="external nofollow noopener noreferrer">https://github.com/tianzhi0549/FCOS</a></p>
<p><img src="/20200101l1/image-20200731165416695.png"></p>
<p><img src="/20200101l1/image-20200731165642605.png"></p>
<h3 id="section-54"><a href="/20200723l1/" title="Learning a Mixture of Granularity-Specific Experts for Fine-Grained Categorization">Learning a Mixture of Granularity-Specific Experts for Fine-Grained Categorization</a></h3>
<p>学习特定粒度专家的混合物以进行细粒度分类</p>
<p><img src="/20200101l1/image-20200727151845765.png"></p>
<p><img src="/20200101l1/image-20200727151930340.png"></p>
<h2 id="icml">ICML</h2>
<h3 id="section-55"><a href="/20200706l1/" title="EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks">EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks</a></h3>
<p>EfficientNet: 对卷积神经网络模型缩放的再思考</p>
<p>Code: <a href="https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet" target="_blank" rel="external nofollow noopener noreferrer">https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet</a></p>
<p><img src="/20200101l1/image-20200706155430777.png"></p>
<p><img src="/20200101l1/image-20200706155556921.png"></p>
<h2 id="neurips">NeurIPS</h2>
<h3 id="section-56"><a href="/20200715l1/" title="MixMatch: A Holistic Approach to Semi-Supervised Learning">MixMatch: A Holistic Approach to Semi-Supervised Learning</a></h3>
<p>MixMatch：半监督学习的整体方法</p>
<p>Code: <a href="https://github.com/google-research/mixmatch" target="_blank" rel="external nofollow noopener noreferrer">https://github.com/google-research/mixmatch</a></p>
<p><img src="/20200101l1/image-20200715210313015.png"></p>
<p><img src="/20200101l1/image-20200715210333773.png"></p>
<h2 id="arxiv-1">Arxiv</h2>
<h3 id="section-57"><a href="/20200624l2/" title="See Better Before Looking Closer: Weakly Supervised Data Augmentation Network for Fine-Grained Visual Classification">See Better Before Looking Closer: Weakly Supervised Data Augmentation Network for Fine-Grained Visual Classification</a></h3>
<p>看得更清楚再看得更近：用于细粒度视觉分类的弱监督数据增强网络</p>
<p>Code: <a href="https://github.com/GuYuc/WS-DAN.PyTorch" target="_blank" rel="external nofollow noopener noreferrer">https://github.com/GuYuc/WS-DAN.PyTorch</a></p>
<p><img src="/20200101l1/image-20200624213638315.png"></p>
<p><img src="/20200101l1/image-20200624214610530.png"></p>
<h1 id="section-58">2018</h1>
<h2 id="eccv-1">ECCV</h2>
<h3 id="section-59"><a href="/20200707l1/" title="DeepLabv3+:Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation">DeepLabv3+:Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation</a></h3>
<p>DeepLab v3+</p>
<p>Code: <a href="https://github.com/tensorflow/models/tree/master/research/deeplab" target="_blank" rel="external nofollow noopener noreferrer">https://github.com/tensorflow/models/tree/master/research/deeplab</a></p>
<p><img src="/20200101l1/image-20200709104913290.png"></p>
<p><img src="/20200101l1/image-20200709105003493.png"></p>
<h2 id="iclr-1">ICLR</h2>
<h3 id="section-60"><a href="/20200714l1/" title="mixup: Beyond Empirical Risk Minimization">mixup: Beyond Empirical Risk Minimization</a></h3>
<p>mixup: 超越经验风险最小化</p>
<p>Code: <a href="https://github.com/facebookresearch/mixup-cifar10" target="_blank" rel="external nofollow noopener noreferrer">https://github.com/facebookresearch/mixup-cifar10</a></p>
<p><img src="/20200101l1/image-20200714171323416.png"></p>
<h2 id="arxiv-2">Arxiv</h2>
<h3 id="section-61"><a href="/20200710l1/" title="Self-Attention Generative Adversarial Networks">Self-Attention Generative Adversarial Networks</a></h3>
<p>自注意力生成对抗网络</p>
<p>Code: <a href="https://github.com/brain-research/self-attention-gan" target="_blank" rel="external nofollow noopener noreferrer">https://github.com/brain-research/self-attention-gan</a></p>
<p><img src="/20200101l1/image-20200710170445992.png"></p>
<p><img src="/20200101l1/image-20200710170828360.png"></p>
<h3 id="section-62"><a href="/20200721l1/" title="Bag of Tricks for Image Classification with Convolutional Neural Networks">Bag of Tricks for Image Classification with Convolutional Neural Networks</a></h3>
<p>使用卷积神经网络进行图像分类的技巧包</p>
<p><img src="/20200101l1/image-20200721105523992.png"></p>
<h1 id="section-63">2017</h1>
<h2 id="tpami">TPAMI</h2>
<h3 id="section-64"><a href="/20200709l1/" title="DeepLabv2: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs">DeepLabv2: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs</a></h3>
<p>DeepLab v2</p>
<p>Code: <a href="https://bitbucket.org/aquariusjay/deeplab-public-ver2/src/master/" target="_blank" rel="external nofollow noopener noreferrer">https://bitbucket.org/aquariusjay/deeplab-public-ver2/src/master/</a></p>
<p><img src="/20200101l1/image-20200709091315408.png"></p>
<p><img src="/20200101l1/image-20200709091744292.png"></p>
<h2 id="arxiv-3">Arxiv</h2>
<h3 id="section-65"><a href="/20200709l2/" title="DeepLabv3:Rethinking Atrous Convolution for Semantic Image Segmentation">DeepLabv3:Rethinking Atrous Convolution for Semantic Image Segmentation</a></h3>
<p>DeepLab v3</p>
<p>Code: <a href="https://github.com/eveningdong/DeepLabV3-Tensorflow" target="_blank" rel="external nofollow noopener noreferrer">https://github.com/eveningdong/DeepLabV3-Tensorflow</a></p>
<p><img src="/20200101l1/image-20200709103023997.png"></p>
<p><img src="/20200101l1/image-20200709104325142.png"></p>
<h3 id="section-66"><a href="/20200712l2/" title="Large Kernel Matters — Improve Semantic Segmentation by Global Convolutional Network">Large Kernel Matters — Improve Semantic Segmentation by Global Convolutional Network</a></h3>
<p>大型内核问题—通过全局卷积网络改进语义分割</p>
<p><img src="/20200101l1/image-20200712202943356.png"></p>
<p><img src="/20200101l1/image-20200801130339163.png"></p>
<h1 id="section-67">2016</h1>
<h2 id="arxiv-4">Arxiv</h2>
<h3 id="section-68"><a href="/20200825l1/" title="VoxResNet: Deep Voxelwise Residual Networks for Volumetric Brain Segmentation">VoxResNet: Deep Voxelwise Residual Networks for Volumetric Brain Segmentation</a></h3>
<p>VoxResNet：用于体积脑分割的深体素残差网络</p>
<p><img src="/20200101l1/image-20200825105117693.png"></p>
<h1 id="section-69">2015</h1>
<h2 id="iclr-2">ICLR</h2>
<h3 id="section-70"><a href="/20200708l1/" title="DeepLabv1: Semantic image segmentation with deep convolutional nets and fully connected CRFs">DeepLabv1: Semantic image segmentation with deep convolutional nets and fully connected CRFs</a></h3>
<p>DeepLab v1</p>
<p>Code: <a href="https://github.com/TheLegendAli/DeepLab-Context" target="_blank" rel="external nofollow noopener noreferrer">https://github.com/TheLegendAli/DeepLab-Context</a></p>
<p><img src="/20200101l1/image-20200709084713666.png"></p>
]]></content>
      <categories>
        <category>阅读笔记</category>
      </categories>
      <tags>
        <tag>总结</tag>
      </tags>
  </entry>
  <entry>
    <title>【牛客网】数据库</title>
    <url>/20200422l1/</url>
    <content><![CDATA[<h1 id="数据库管理系统是数据库系统的核心">数据库管理系统是数据库系统的核心</h1>
<p>数据库系统由数据库（数据）、数据库管理系统（软件）、数据库管理员（人员）、硬件平台（硬件）、软件平台5个部分构成。</p>
<p><strong>数据库管理系统</strong>是数据库系统的核心，负责数据库中的数据组织、数据操作、数据维护、控制及保护和数据服务等工作。</p>
<h1 id="数据库的外模式模式映像保证了数据与程序的逻辑独立性">数据库的外模式/模式映像，保证了数据与程序的逻辑独立性。</h1>
<p>外模式/模式映像定义了数据库中不同用户的外模式与数据库逻辑模式之间的对应关系。当数据库模式发生变化时，通过调整外模式/模式映像间的映像关系，使得应用程序不必随之修改，从而保证数据与应用程序间的逻辑独立性，简称数据的逻辑独立性。</p>
<p>模式又称逻辑模式，模式/内模式映像定义了数据库中数据全局逻辑结构与这些数据在系统中的物理存储组织结构之间的对应关系，保证数据库中数据与应用程序间的物理独立性。</p>
<h1 id="若事务-t-对数据对象-a-加上-s-锁则">若事务 T 对数据对象 A 加上 S 锁，则（ ）。</h1>
<p>事务T可以读A但不能修改A，其它事务只能再对A加S锁，而不能加X 锁。</p>
<p>S锁为共享锁，X锁为排他锁。</p>
<p>共享锁又称为读锁，若事务T对数据对象A加上S锁，则事务T只能读A；其他事务只能再对A加S锁，而不能加X锁，直到T释放A上的S锁。这就保证了其他事务可以读A，但在T释放A上的S锁之前不能对A做任何修改。</p>
<h1 id="关系的5条性质">关系的5条性质</h1>
<ol type="1">
<li>分量必须取原子值，每个分量必须是不可再分的数据项。</li>
<li>列是同质的，每列中的分量必须是同一类型的数据，来自同一个域。</li>
<li>属性不能重名。</li>
<li>行列的顺序无关。</li>
<li>任何两个元组不能完全相同，这是由主码约束来保证的。但是有些数据库若用户没有定义完整性约束条件，允许有两行以上的相同的元组。</li>
</ol>
<h1 id="在关系模式-r分解成数据库模式ρ时谈论无损联接的先决条件是存在泛关系">在关系模式 R分解成数据库模式ρ时，谈论无损联接的先决条件是存在（泛关系）。</h1>
<h1 id="后援副本的用途是故障后的恢复">后援副本的用途是故障后的恢复</h1>
<h1 id="数据库恢复的基础是利用转储的冗余数据这些转储的冗余数据是指">数据库恢复的基础是利用转储的冗余数据。这些转储的冗余数据是指（ ）</h1>
<p>日志文件、数据库后备副本</p>
<p>数据库恢复的实现中可定期对整个数据库进行复制或转储 转储是数据库恢复中常用的基本技术，它是指DBA把数据库复制到另一个磁盘上的过程，可分为静态转储和动态转储 转储还可以分为海量存储和增量转储。 转储的冗余数据包通常包括 日志文件、数据库后备副本 等。</p>
<h1 id="索引的优缺点">索引的优缺点：</h1>
<h2 id="优点">优点：</h2>
<p>（1）通过创建索引,可以在查询的过程中,提高系统的性能 （2）通过创建唯一性索引,可以保证数据库表中每一行数据的唯一性 （3）在使用分组和排序子句进行数据检索时,可以减少查询中分组和排序的时间</p>
<h2 id="缺点">缺点</h2>
<p>（1）创建索引和维护索引要耗费时间,而且时间随着数据量的增加而增大 （2）索引需要占用物理空间,如果要建立聚簇索引,所需要的空间会更大 （3）在对表中的数据进行增加删除和修改时需要耗费较多的时间,因为索引也要动态地维护</p>
<h1 id="外模式-内模式-概念模式的关系">外模式 内模式 概念模式的关系</h1>
<p>三级模式结构：外模式、模式和内模式</p>
<p>一、模式（Schema）</p>
<p>定义：也称逻辑模式，是数据库中全体数据的逻辑结构和特征的描述，是所有用户的公共数据视图。</p>
<p>理解： ① 一个数据库只有一个模式； ② 是数据库数据在逻辑级上的视图； ③ 数据库模式以某一种数据模型为基础； ④ 定义模式时不仅要定义数据的逻辑结构（如数据记录由哪些数据项构成，数据项的名字、类型、取值范围等），而且要定义与数据有关的安全性、完整性要求，定义这些数据之间的联系。</p>
<p>二、外模式（External Schema）</p>
<p>定义：也称子模式（Subschema）或用户模式，是数据库用户（包括应用程序员和最终用户）能够看见和使用的局部数据的逻辑结构和特征的描述，是数据库用户的数据视图，是与某一应用有关的数据的逻辑表示。</p>
<p>理解： ① 一个数据库可以有多个外模式； ② 外模式就是用户视图； ③ 外模式是保证数据安全性的一个有力措施。</p>
<p>三、内模式（Internal Schema）</p>
<p>定义：也称存储模式（Storage Schema），它是数据物理结构和存储方式的描述，是数据在数据库内部的表示方式（例如，记录的存储方式是顺序存储、按照B树结构存储还是按hash方法存储；索引按照什么方式组织；数据是否压缩存储，是否加密；数据的存储记录结构有何规定）。</p>
<p>理解： ① 一个数据库只有一个内模式； ② 一个表可能由多个文件组成，如：数据文件、索引文件。 它是数据库管理系统(DBMS)对数据库中数据进行有效组织和管理的方法</p>
<p>其目的有： ① 为了减少数据冗余，实现数据共享； ② 为了提高存取效率，改善性能。</p>
<h1 id="事务的特性-acid特性">事务的特性 ACID特性</h1>
<p>A: 原子 C: 一致 I:隔离 D:持久</p>
<h1 id="复合索引可以只使用复合索引中的一部分但必须是由最左部分开始且可以存在常量">复合索引可以只使用复合索引中的一部分，但必须是由最左部分开始，且可以存在常量。</h1>
<h1 id="需求分析概念设计逻辑设计物理设计">需求分析、概念设计、逻辑设计、物理设计</h1>
<ul>
<li>需求分析：分析用户的需求，包括数据、功能和性能需求，确立系统所需要实现的功能模块</li>
<li>概念设计：主要采用E-R模型进行设计，包括画E-R图</li>
<li>逻辑设计：E-R图转换成关系模式，进行关系规范化</li>
<li>物理设计：为所设计的数据库选择合适的 存储结构 和存取路径</li>
</ul>
<h1 id="sql语言-四大类dql数据查询语言dml数据操纵语言dcl数据控制语言ddl数据定义语言">SQL语言 四大类：DQL（数据查询语言）、DML（数据操纵语言）、DCL（数据控制语言）、DDL（数据定义语言）</h1>
<p>DQL：数据查询语言DQL基本结构是由SELECT子句，FROM子句，WHERE子句组成的查询块</p>
<p>DML：插入、更新、删除</p>
<p>DCL：数据控制语言DCL用来授予或回收访问数据库的某种特权，并控制数据库操纵事务发生的时间及效果，对数据库实行监视等</p>
<p>DDL：数据定义语言DDL用来创建数据库中的各种对象-----表、视图、索引、同义词、聚簇等</p>
<p>查询：SELECT</p>
<p>操纵：UPDATE, INSERT, DELETE</p>
<p>定义：CREATE, DROP</p>
<p>控制：COMMIT, ROLLBACK, GRANT, REVOKE等</p>
<h1 id="i中的数据保护模式包括有">9i中的数据保护模式包括有？</h1>
<p><strong>数据库Oracle 9i</strong> ：</p>
<h2 id="最大保护maximum-protection">最大保护(Maximum protection )</h2>
<p>这种模式能够保证在primary Database发生故障保证数据不丢失。在这种模式下，事务提交前，要保证Redo数据已经写入到Primary Database的Online Redologs，同时写入Standby Database的Standby Redologs，并确保至少在一个Standby Database中可用。如果Standby Database不可用，Primary Database将会shutdown。</p>
<h2 id="最高可用性maximum-availability">最高可用性(Maximum availability)</h2>
<p>这种模式在不影响Primary Database可用的前提下，提供最高级别的数据保护策略，这种模式也能够确保数据不丢失。事务提交之前，要保证Redo数据已经写入到Primary Database的Online Redologs，同时写入Standby Database的Standby Redologs，确保至少在一个Standby Database中可用。与最大保护模式不同的是，如果Standby Database出现故障导致不可用，Primary Database并不会被shutdown，而是自动转换为最高性能模式，等Standby Database恢复正常后，Primary Database又会自动切换到最高可用性模式。</p>
<h2 id="最大性能maximum-performance">最大性能(Maximum performance)</h2>
<p>这是一种默认的保护模式。事务可以随时提交，当前Primary Database的Redo数据至少需要写入一个Standby Database，不过这种方式不会等待Standby Database是否写入的确认因此这种写入属于异步写入。</p>
<h1 id="数据库文件缓冲区日志文件后援文件">数据库文件、缓冲区、日志文件、后援文件</h1>
<p>数据库文件：电脑上储存数据的文件。</p>
<p>缓冲区：是用户前端用来存储、操纵数据的对象。</p>
<p>日志文件：用于记录系统操作事件的记录文件或文件集合，可分为事件日志和消息日志。具有处理历史数据、诊断问题的追踪以及理解系统的活动等重要作用。</p>
<p>后援副本：数据的转存，这样才能让数据库恢复到最近一次转存时的一致性状态。</p>
<h1 id="数据库系统的存储模式如有改变概念模式无需改动">数据库系统的存储模式如有改变，概念模式无需改动</h1>
<h1 id="视图设计的设计次序">视图设计的设计次序</h1>
<ol type="1">
<li><p>自顶向下。先全局框架，然后逐步细化</p></li>
<li><p>自底向上。先局部概念结构，再集成为全局结构</p></li>
<li><p>由里向外。先核心结构，再向外扩张</p></li>
<li><p>混合策略。1与2相结合，先自顶向下设计一个概念结构的框架，再自底向上为框架设计局部概念结构</p></li>
</ol>
<h1 id="范式">范式</h1>
<p>1NF</p>
<p>每个关系r的属性值为不可分的原子值</p>
<p>2NF</p>
<p>满足1NF，非主属性完全函数依赖于候选键(左部不可约)</p>
<p>3NF</p>
<p>满足2NF，消除非主属性对候选键的传递依赖</p>
<p>BCNF</p>
<p>满足3NF，消除每一属性对候选键的传递依赖</p>
<p>1NF + 消去非主属性对键的部分函数依赖 = 2NF。即2NF中，非主属性完全依赖于主关键字；</p>
<p>2NF + 消去非主属性对键的传递函数依赖 = 3NF。即3NF中，属性不依赖于其它非主属性。传递函数依赖，指的是如果存在"A → B → C"的决定关系，则C传递函数依赖于A；</p>
<p>3NF + 消去主属性对键的传递函数依赖 = BCNF。BCNF是3NF的改进形式，即在3NF的基础上，数据库表中如果不存在任何字段对任一候选关键字段的传递函数依赖则符合BCNF。</p>
<h1 id="存储过程的调用">存储过程的调用</h1>
<p>exec 是sql server的存储过程调用方式，call是mysql的存储过程调用方式，同时调用时必须有参数或者为null</p>
<p>MySQL的存储过程参数没有默认值，所以在调用MySQL存储过程时，不能省略参数，但是可以用null来代替</p>
<h1 id="nosql数据库">noSQL数据库</h1>
<p>基于K-V：Redis， Voldemort， Oracle BDB</p>
<p>基于列存储：Cassandra， HBase， Riak</p>
<p>基于文档型：CouchDB， MongoDB</p>
<p>图形（Graph）数据库：Neo4J， InfoGrid， Infinite Graph</p>
<h1 id="数据模型与逻辑模型">数据模型与逻辑模型</h1>
<p>常用的数据模型：概念模型、逻辑模型、物理模型</p>
<p>常用的逻辑模型：层次模型、网状模型、关系模型</p>
<h1 id="查询设计视图窗口分为上下部分">“查询”设计视图窗口分为上下部分</h1>
<p>上部分：“字段列表区”，用来显示所选择的所有字段。</p>
<p>下部分：“设计网络”，由一些字段列和一些已命名的列组成。</p>
<h1 id="sql语句执行顺序">SQL语句执行顺序</h1>
<p><code>FROM</code>&gt;<code>ON</code>&gt;<code>JOIN</code>&gt;<code>WHERE</code>&gt;<code>GROUP BY</code>&gt;<code>WITH CUBE</code> or <code>WITH ROLLUP</code>&gt;<code>HAVING</code>&gt;<code>SELECT</code>&gt;<code>DISTINCT</code>&gt;<code>ORDER BY</code>&gt;<code>TOP</code></p>
]]></content>
      <categories>
        <category>知识点</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>知识点</tag>
        <tag>牛客网</tag>
        <tag>刷题</tag>
      </tags>
  </entry>
  <entry>
    <title>白盒测试：语句覆盖、条件覆盖（分支覆盖）、判定覆盖、条件-判定覆盖、组合覆盖、路径覆盖的区别</title>
    <url>/20200421l1/</url>
    <content><![CDATA[<h1 id="语句覆盖">语句覆盖</h1>
<p><strong>每个可执行语句都走一遍即可，即测试用例要覆盖所有的语句</strong>（来源：软件开发的技术基础） <a id="more"></a></p>
<figure>
<img src="/20200421l1/5de1f30e76c66137ee061916.jpg" alt><figcaption>img</figcaption>
</figure>
<h1 id="判定覆盖分支覆盖">判定覆盖（分支覆盖）</h1>
<p>针对判断语句，在设定案例的时候，要设定True和False的两种案例；与语句覆盖不同的是增加了False的情况。</p>
<figure>
<img src="/20200421l1/5de1f30e76c66137ee061916-1587460107267.jpg" alt><figcaption>img</figcaption>
</figure>
<h1 id="条件覆盖">条件覆盖</h1>
<p>针对判断语句里面案例的取值都要去一次，不考虑条件的取值</p>
<p><strong>另注</strong>：条件覆盖保证判断中的每个条件都被覆盖（来源：软件开发的技术基础）</p>
<figure>
<img src="/20200421l1/5de1f30e76c66137ee061916-1587460180539.jpg" alt><figcaption>img</figcaption>
</figure>
<h1 id="判定条件覆盖">判定/条件覆盖</h1>
<p>判定覆盖各条件覆盖交叉，针对于判定中的条件取值</p>
<figure>
<img src="/20200421l1/5de1f30e76c66137ee061916-1587460218160.jpg" alt><figcaption>img</figcaption>
</figure>
<h1 id="条件组合覆盖">条件组合覆盖</h1>
<p>判定-条件覆盖的加强版</p>
<figure>
<img src="/20200421l1/5de1f30e76c66137ee061916-1587460241521.jpg" alt><figcaption>img</figcaption>
</figure>
<h1 id="路径覆盖">路径覆盖</h1>
<figure>
<img src="/20200421l1/5de1f30e76c66137ee061916-1587460268368.jpg" alt><figcaption>img</figcaption>
</figure>
]]></content>
      <categories>
        <category>知识点</category>
      </categories>
      <tags>
        <tag>软件测试</tag>
        <tag>白盒测试</tag>
      </tags>
  </entry>
  <entry>
    <title>【2020】PointRend: Image Segmentation as Rendering</title>
    <url>/20200623l1/</url>
    <content><![CDATA[<p>Code: <a href="https://github.com/facebookresearch/detectron2/tree/master/projects/PointRend" target="_blank" rel="external nofollow noopener noreferrer">https://github.com/facebookresearch/detectron2/tree/master/projects/PointRend</a></p>
<h1 id="motivation">Motivation</h1>
<ul>
<li>过采样（ oversample ）：对于图片中低频区域（ 属于同一个物体 ），没必要使用 太多的采样点，却使用太多采样点造成过采样；</li>
<li>欠采样（ undersample ） ：对于图片中高频区域（ 靠近物体边界 ），如果这些区域的采样过于稀疏，导致分割出的边界过于平滑，不大真实。</li>
</ul>
<p><img src="/20200623l1/image-20200623160128310.png"></p>
<h1 id="method">Method</h1>
<p>渲染：渲染器将模型（如 3D 网格）映射到点阵图像，即像素的规则网格。</p>
<p>计算机图形学的思路：对图像平面中被自适应选择点的不规则子集计算出像素值。从而高效渲染出抗锯齿的高分辨率图像。</p>
<p>图像分割，同样可以视作底层连续实体的占用图，然后从中输出预测标签的矩形网格。类比计算机图形学的思路，使用细分策略来自适应地选择一组非均匀点，进而计算标签。</p>
<p>图像分割步骤</p>
<ul>
<li>使用轻量级的分割头，对每个检测到的对象（红框）进行粗略的mask预测。</li>
<li>选择一组点（红色点），用小规模的多层感知器（MLP）为每个点进行独立预测。</li>
<li>对这样的细分算法进行迭代，以从粗到细的方式计算mask。</li>
</ul>
<p>PointRend模块主要由以下三部分组成：</p>
<ol type="1">
<li><p>点选择策略: 选择少数的一些点（<code>难点</code>）进行预测，避免在高分辨率的输出中过量的计算所有的像素；</p></li>
<li><p>point-wise: 特征表示：提取被选择出来的点的特征，这些特征通过双线性插值计算，然后沿着通道维度编码子像素信息预测分割；</p></li>
<li><p>point head: 小的网络，进行预测。</p></li>
</ol>
<p><img src="/20200623l1/image-20200623160207304.png"></p>
<h2 id="point-selection">Point Selection</h2>
<p>PointRend的核心思想是，在图像平面中自适应地选择预测分割标签的点。那么这些点主要分布在哪里呢？理论上讲，这些点应该在高频区域分布较广（比如说图像的边缘）。</p>
<p>推断：用于推断的点选择策略受到计算机图形学中自适应细分（adaptive subdivision）这一经典技术的启发。该技术通过计算与其近邻的值显著不同的位置，来高效渲染高分辨率图像（如通过光线追踪）；其他位置的值则通过内插已经计算好的输出值来获得（从粗糙网格开始）。</p>
<p><img src="/20200623l1/image-20200623160231982.png"></p>
<p><strong>训练：</strong>在训练过程中，PointRend 还需要选择点来构建训练 point head 所需的逐点特征。原则上，点选择策略类似于推断过程中使用的细分策略。但是，细分策略使用的顺序步骤对于利用反向传播训练神经网络不那么友好。因此，训练过程使用基于随机采样的非迭代策略。</p>
<p><img src="/20200623l1/image-20200623160247553.png"></p>
<h2 id="逐点表示">逐点表示</h2>
<p>PointRend通过组合低层特征 (fine-grained features) 和高层特征 (coarse prediction)，在选定的点上构造逐点特征。在细粒度特征（fine-grained features）方面，为了让PointRend呈现出精细的分割细节，研究人员为CNN特征图中的每个采样点提取了特征向量。</p>
<p>细粒度特征虽然可以解析细节，但也存在两方面的不足：</p>
<p>（1）不包含特定于区域的信息，对于实例分割任务，就可能在同一点上预测出不同的标签。比如两个重叠的实例中的一点具有相同的细粒度特征，故该点只会被预测到其中一个实例中。</p>
<p>（2）用于细粒度特征的特征映射，可能仅包含相对较低级别的信息。受通过哪张feature map提取细粒度特征的影响，提取到的细粒度特征可能只包含low level的特征，相比之下具有更多上下文和语义信息的feature map会更有价值。</p>
<p>基于上述讨论，第二种特征是从网络中抽取一个粗的分割预测，比如对每个点，预测一个k维的向量用来对应表示k个类别各自的概率。这种粗预测特征，类似现有的语义分割结构，同样使用与现有模型相同的监督方式进行监督。比如对实例分割，粗预测特征可以使用Mask R-CNN输出的轻量级7*7 mask head。再比如对于语义分割，它可以是预测一个原图下采样16倍的feature map.</p>
<p>这就需要粗略分割预测 (coarse prediction) 来进行补充，提供更多全局背景。</p>
<p>这样的粗略预测类似于现有架构的输出。以实例分割为例，coarse prediction可以是Mask R-CNN中 7×7 轻量级mask head的输出。</p>
<h2 id="point-head">Point Head</h2>
<p>对于每个选定点的逐点特征表示，PointRend使用简单的多层感知器进行逐点分割预测。</p>
<p>多层感知器在所有点（所有区域）上共享权重。</p>
<p>并且，由于多层感知器会针对每个点预测分割标签，可以通过特定任务的分割损失进行训练。</p>
<h1 id="参考文献">参考文献</h1>
<ol type="1">
<li><a href="https://blog.csdn.net/qq_41997920/article/details/104536014" target="_blank" rel="external nofollow noopener noreferrer">PointRend: Image Segmentation as Rendering（论文解读二十四）</a></li>
</ol>
]]></content>
      <categories>
        <category>阅读笔记</category>
      </categories>
      <tags>
        <tag>图像分割</tag>
        <tag>code</tag>
      </tags>
  </entry>
  <entry>
    <title>【2019 ICCV】FCOS: Fully Convolutional One-Stage Object Detection</title>
    <url>/20200731l2/</url>
    <content><![CDATA[<p>Code: <a href="https://github.com/tianzhi0549/FCOS" target="_blank" rel="external nofollow noopener noreferrer">https://github.com/tianzhi0549/FCOS</a></p>
<h1 id="motivation">Motivation</h1>
<p>以图像分割的方式解决目标检测问题。</p>
<ul>
<li>由于bbox的scale和ratio是固定的，所以有些 形状变化较大的物体无法检测，尤其是小目标。</li>
<li>为了较高召回率，需要密集的放置锚点。因此负样本较多，导致正负样本不平衡。</li>
</ul>
<p><img src="/20200731l2/image-20200731165416695.png"></p>
<h1 id="methods">Methods</h1>
<p><img src="/20200731l2/image-20200731165642605.png"></p>
<h1 id="experiments">Experiments</h1>
<p><img src="/20200731l2/image-20200731165819275.png"></p>
<p><img src="/20200731l2/image-20200731165835453.png"></p>
<p><img src="/20200731l2/image-20200731165849210.png"></p>
<p><img src="/20200731l2/image-20200731165902400.png"></p>
<p><img src="/20200731l2/image-20200731165915994.png"></p>
<p><img src="/20200731l2/image-20200731165927179.png"></p>
<p><img src="/20200731l2/image-20200731165937378.png"></p>
]]></content>
      <categories>
        <category>阅读笔记</category>
      </categories>
      <tags>
        <tag>code</tag>
        <tag>ICCV</tag>
        <tag>2019</tag>
        <tag>目标检测</tag>
      </tags>
  </entry>
  <entry>
    <title>设计模式</title>
    <url>/20200426l2/</url>
    <content><![CDATA[<h1 id="简介">简介</h1>
<p>设计模式（Design pattern）代表了最佳的实践，通常被有经验的面向对象的软件开发人员所采用。设计模式是软件开发人员在软件开发过程中面临的一般问题的解决方案。这些解决方案是众多软件开发人员经过相当长的一段时间的试验和错误总结出来的。</p>
<h2 id="类型">类型</h2>
<p>根据设计模式的参考书 <strong>Design Patterns - Elements of Reusable Object-Oriented Software（中文译名：设计模式 - 可复用的面向对象软件元素）</strong> 中所提到的，总共有 23 种设计模式。这些模式可以分为三大类：创建型模式（Creational Patterns）、结构型模式（Structural Patterns）、行为型模式（Behavioral Patterns）。</p>
<table>
<colgroup>
<col style="width: 3%">
<col style="width: 48%">
<col style="width: 48%">
</colgroup>
<thead>
<tr class="header">
<th>序号</th>
<th>模式 &amp; 描述</th>
<th>包括</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td><strong>创建型模式</strong><br>这些设计模式提供了一种在创建对象的同时隐藏创建逻辑的方式，而不是使用 new 运算符直接实例化对象。这使得程序在判断针对某个给定实例需要创建哪些对象时更加灵活。</td>
<td>- 工厂模式（Factory Pattern）<br> - 抽象工厂模式（Abstract Factory Pattern）<br>- 单例模式（Singleton Pattern） <br>- 建造者模式（Builder Pattern） <br>- 原型模式（Prototype Pattern）</td>
</tr>
<tr class="even">
<td>2</td>
<td><strong>结构型模式</strong><br>这些设计模式关注类和对象的组合。继承的概念被用来组合接口和定义组合对象获得新功能的方式。</td>
<td>适配器模式（Adapter Pattern）<br> -桥接模式（Bridge Pattern）<br> -过滤器模式（Filter、Criteria Pattern）<br> -组合模式（Composite Pattern）<br> -装饰器模式（Decorator Pattern）<br> -外观模式（Facade Pattern）<br> -享元模式（Flyweight Pattern）<br> -代理模式（Proxy Pattern）</td>
</tr>
<tr class="odd">
<td>3</td>
<td><strong>行为型模式</strong><br>这些设计模式特别关注对象之间的通信。</td>
<td>责任链模式（Chain of Responsibility Pattern）<br> -命令模式（Command Pattern） <br> -解释器模式（Interpreter Pattern） <br> -迭代器模式（Iterator Pattern） <br> -中介者模式（Mediator Pattern） <br> -备忘录模式（Memento Pattern） <br> -观察者模式（Observer Pattern） <br> -状态模式（State Pattern） <br> -空对象模式（Null Object Pattern）<br> - 策略模式（Strategy Pattern） <br> -模板模式（Template Pattern）<br> -访问者模式（Visitor Pattern）</td>
</tr>
</tbody>
</table>
<p><img src="/20200426l2/the-relationship-between-design-patterns.jpg"></p>
<h2 id="设计模式的六大原则">设计模式的六大原则</h2>
<p><strong>1、开闭原则（Open Close Principle）</strong></p>
<p>开闭原则的意思是：<strong>对扩展开放，对修改关闭</strong>。在程序需要进行拓展的时候，不能去修改原有的代码，实现一个热插拔的效果。简言之，是为了使程序的扩展性好，易于维护和升级。想要达到这样的效果，我们需要使用接口和抽象类，后面的具体设计中我们会提到这点。</p>
<p><strong>2、里氏代换原则（Liskov Substitution Principle）</strong></p>
<p>里氏代换原则是面向对象设计的基本原则之一。 里氏代换原则中说，任何基类可以出现的地方，子类一定可以出现。LSP 是继承复用的基石，只有当派生类可以替换掉基类，且软件单位的功能不受到影响时，基类才能真正被复用，而派生类也能够在基类的基础上增加新的行为。里氏代换原则是对开闭原则的补充。实现开闭原则的关键步骤就是抽象化，而基类与子类的继承关系就是抽象化的具体实现，所以里氏代换原则是对实现抽象化的具体步骤的规范。</p>
<p><strong>3、依赖倒转原则（Dependence Inversion Principle）</strong></p>
<p>这个原则是开闭原则的基础，具体内容：针对接口编程，依赖于抽象而不依赖于具体。</p>
<p><strong>4、接口隔离原则（Interface Segregation Principle）</strong></p>
<p>这个原则的意思是：使用多个隔离的接口，比使用单个接口要好。它还有另外一个意思是：降低类之间的耦合度。由此可见，其实设计模式就是从大型软件架构出发、便于升级和维护的软件设计思想，它强调降低依赖，降低耦合。</p>
<p><strong>5、迪米特法则，又称最少知道原则（Demeter Principle）</strong></p>
<p>最少知道原则是指：一个实体应当尽量少地与其他实体之间发生相互作用，使得系统功能模块相对独立。</p>
<p><strong>6、合成复用原则（Composite Reuse Principle）</strong></p>
<p>合成复用原则是指：尽量使用合成/聚合的方式，而不是使用继承。</p>
<h1 id="工厂模式factory-pattern">工厂模式（Factory Pattern）</h1>
<h2 id="简介-1">简介</h2>
<p>工厂模式（Factory Pattern）是 Java 中最常用的设计模式之一。这种类型的设计模式属于创建型模式，它提供了一种创建对象的最佳方式。</p>
<p>在工厂模式中，我们在创建对象时不会对客户端暴露创建逻辑，并且是通过使用一个共同的接口来指向新创建的对象。</p>
<p><strong>意图：</strong>定义一个创建对象的接口，让其子类自己决定实例化哪一个工厂类，工厂模式使其创建过程延迟到子类进行。</p>
<p><strong>主要解决：</strong>主要解决接口选择的问题。</p>
<p><strong>何时使用：</strong>我们明确地计划不同条件下创建不同实例时。</p>
<p><strong>如何解决：</strong>让其子类实现工厂接口，返回的也是一个抽象的产品。</p>
<p><strong>关键代码：</strong>创建过程在其子类执行。</p>
<p><strong>应用实例：</strong> 1、您需要一辆汽车，可以直接从工厂里面提货，而不用去管这辆汽车是怎么做出来的，以及这个汽车里面的具体实现。 2、Hibernate 换数据库只需换方言和驱动就可以。</p>
<p><strong>优点：</strong> 1、一个调用者想创建一个对象，只要知道其名称就可以了。 2、扩展性高，如果想增加一个产品，只要扩展一个工厂类就可以。 3、屏蔽产品的具体实现，调用者只关心产品的接口。</p>
<p><strong>缺点：</strong>每次增加一个产品时，都需要增加一个具体类和对象实现工厂，使得系统中类的个数成倍增加，在一定程度上增加了系统的复杂度，同时也增加了系统具体类的依赖。这并不是什么好事。</p>
<p><strong>使用场景：</strong> 1、日志记录器：记录可能记录到本地硬盘、系统事件、远程服务器等，用户可以选择记录日志到什么地方。 2、数据库访问，当用户不知道最后系统采用哪一类数据库，以及数据库可能有变化时。 3、设计一个连接服务器的框架，需要三个协议，"POP3"、"IMAP"、"HTTP"，可以把这三个作为产品类，共同实现一个接口。</p>
<p><strong>注意事项：</strong>作为一种创建类模式，在任何需要生成复杂对象的地方，都可以使用工厂方法模式。有一点需要注意的地方就是复杂对象适合使用工厂模式，而简单对象，特别是只需要通过 new 就可以完成创建的对象，无需使用工厂模式。如果使用工厂模式，就需要引入一个工厂类，会增加系统的复杂度。</p>
<h2 id="实现">实现</h2>
<p>我们将创建一个 <em>Shape</em> 接口和实现 <em>Shape</em> 接口的实体类。下一步是定义工厂类 <em>ShapeFactory</em>。</p>
<p><em>FactoryPatternDemo</em>，我们的演示类使用 <em>ShapeFactory</em> 来获取 <em>Shape</em> 对象。它将向 <em>ShapeFactory</em> 传递信息（<em>CIRCLE / RECTANGLE / SQUARE</em>），以便获取它所需对象的类型。</p>
<p><img src="/20200426l2/factory_pattern_uml_diagram.jpg"></p>
<h1 id="抽象工厂模式abstract-factory-pattern">抽象工厂模式（Abstract Factory Pattern）</h1>
<h2 id="简介-2">简介</h2>
<p>抽象工厂模式（Abstract Factory Pattern）是围绕一个超级工厂创建其他工厂。该超级工厂又称为其他工厂的工厂。这种类型的设计模式属于创建型模式，它提供了一种创建对象的最佳方式。</p>
<p>在抽象工厂模式中，接口是负责创建一个相关对象的工厂，不需要显式指定它们的类。每个生成的工厂都能按照工厂模式提供对象。</p>
<p><strong>意图：</strong>提供一个创建一系列相关或相互依赖对象的接口，而无需指定它们具体的类。</p>
<p><strong>主要解决：</strong>主要解决接口选择的问题。</p>
<p><strong>何时使用：</strong>系统的产品有多于一个的产品族，而系统只消费其中某一族的产品。</p>
<p><strong>如何解决：</strong>在一个产品族里面，定义多个产品。</p>
<p><strong>关键代码：</strong>在一个工厂里聚合多个同类产品。</p>
<p><strong>应用实例：</strong>工作了，为了参加一些聚会，肯定有两套或多套衣服吧，比如说有商务装（成套，一系列具体产品）、时尚装（成套，一系列具体产品），甚至对于一个家庭来说，可能有商务女装、商务男装、时尚女装、时尚男装，这些也都是成套的，即一系列具体产品。假设一种情况（现实中是不存在的，要不然，没法进入共产主义了，但有利于说明抽象工厂模式），在您的家中，某一个衣柜（具体工厂）只能存放某一种这样的衣服（成套，一系列具体产品），每次拿这种成套的衣服时也自然要从这个衣柜中取出了。用 OOP 的思想去理解，所有的衣柜（具体工厂）都是衣柜类的（抽象工厂）某一个，而每一件成套的衣服又包括具体的上衣（某一具体产品），裤子（某一具体产品），这些具体的上衣其实也都是上衣（抽象产品），具体的裤子也都是裤子（另一个抽象产品）。</p>
<p><strong>优点：</strong>当一个产品族中的多个对象被设计成一起工作时，它能保证客户端始终只使用同一个产品族中的对象。</p>
<p><strong>缺点：</strong>产品族扩展非常困难，要增加一个系列的某一产品，既要在抽象的 Creator 里加代码，又要在具体的里面加代码。</p>
<p><strong>使用场景：</strong> 1、QQ 换皮肤，一整套一起换。 2、生成不同操作系统的程序。</p>
<p><strong>注意事项：</strong>产品族难扩展，产品等级易扩展。</p>
<h2 id="实现-1">实现</h2>
<p>我们将创建 <em>Shape</em> 和 <em>Color</em> 接口和实现这些接口的实体类。下一步是创建抽象工厂类 <em>AbstractFactory</em>。接着定义工厂类 <em>ShapeFactory</em> 和 <em>ColorFactory</em>，这两个工厂类都是扩展了 <em>AbstractFactory</em>。然后创建一个工厂创造器/生成器类 <em>FactoryProducer</em>。</p>
<p><em>AbstractFactoryPatternDemo</em>，我们的演示类使用 <em>FactoryProducer</em> 来获取 <em>AbstractFactory</em> 对象。它将向 <em>AbstractFactory</em> 传递形状信息 <em>Shape</em>（<em>CIRCLE / RECTANGLE / SQUARE</em>），以便获取它所需对象的类型。同时它还向 <em>AbstractFactory</em> 传递颜色信息 <em>Color</em>（<em>RED / GREEN / BLUE</em>），以便获取它所需对象的类型。</p>
<p><img src="/20200426l2/abstractfactory_pattern_uml_diagram.jpg"></p>
<h1 id="单例模式singleton-pattern">单例模式（Singleton Pattern）</h1>
<h1 id="简介-3">简介</h1>
<p>单例模式（Singleton Pattern）是 Java 中最简单的设计模式之一。这种类型的设计模式属于创建型模式，它提供了一种创建对象的最佳方式。</p>
<p>这种模式涉及到一个单一的类，该类负责创建自己的对象，同时确保只有单个对象被创建。这个类提供了一种访问其唯一的对象的方式，可以直接访问，不需要实例化该类的对象。</p>
<p><strong>注意：</strong></p>
<ul>
<li>1、单例类只能有一个实例。</li>
<li>2、单例类必须自己创建自己的唯一实例。</li>
<li>3、单例类必须给所有其他对象提供这一实例。</li>
</ul>
<p><strong>意图：</strong>保证一个类仅有一个实例，并提供一个访问它的全局访问点。</p>
<p><strong>主要解决：</strong>一个全局使用的类频繁地创建与销毁。</p>
<p><strong>何时使用：</strong>当您想控制实例数目，节省系统资源的时候。</p>
<p><strong>如何解决：</strong>判断系统是否已经有这个单例，如果有则返回，如果没有则创建。</p>
<p><strong>关键代码：</strong>构造函数是私有的。</p>
<p><strong>应用实例：</strong></p>
<ul>
<li>1、一个班级只有一个班主任。</li>
<li>2、Windows 是多进程多线程的，在操作一个文件的时候，就不可避免地出现多个进程或线程同时操作一个文件的现象，所以所有文件的处理必须通过唯一的实例来进行。</li>
<li>3、一些设备管理器常常设计为单例模式，比如一个电脑有两台打印机，在输出的时候就要处理不能两台打印机打印同一个文件。</li>
</ul>
<p><strong>优点：</strong></p>
<ul>
<li>1、在内存里只有一个实例，减少了内存的开销，尤其是频繁的创建和销毁实例（比如管理学院首页页面缓存）。</li>
<li>2、避免对资源的多重占用（比如写文件操作）。</li>
</ul>
<p><strong>缺点：</strong>没有接口，不能继承，与单一职责原则冲突，一个类应该只关心内部逻辑，而不关心外面怎么样来实例化。</p>
<p><strong>使用场景：</strong></p>
<ul>
<li>1、要求生产唯一序列号。</li>
<li>2、WEB 中的计数器，不用每次刷新都在数据库里加一次，用单例先缓存起来。</li>
<li>3、创建的一个对象需要消耗的资源过多，比如 I/O 与数据库的连接等。</li>
</ul>
<p><strong>注意事项：</strong>getInstance() 方法中需要使用同步锁 synchronized (Singleton.class) 防止多线程同时进入造成 instance 被多次实例化。</p>
<h2 id="实现-2">实现</h2>
<p>我们将创建一个 <em>SingleObject</em> 类。<em>SingleObject</em> 类有它的私有构造函数和本身的一个静态实例。</p>
<p><em>SingleObject</em> 类提供了一个静态方法，供外界获取它的静态实例。<em>SingletonPatternDemo</em>，我们的演示类使用 <em>SingleObject</em> 类来获取 <em>SingleObject</em> 对象。</p>
<p><img src="/20200426l2/singleton_pattern_uml_diagram.jpg"></p>
<h1 id="建造者模式builder-pattern">建造者模式（Builder Pattern）</h1>
<h2 id="简介-4">简介</h2>
<p>建造者模式（Builder Pattern）使用多个简单的对象一步一步构建成一个复杂的对象。这种类型的设计模式属于创建型模式，它提供了一种创建对象的最佳方式。</p>
<p>一个 Builder 类会一步一步构造最终的对象。该 Builder 类是独立于其他对象的。</p>
<p><strong>意图：</strong>将一个复杂的构建与其表示相分离，使得同样的构建过程可以创建不同的表示。</p>
<p><strong>主要解决：</strong>主要解决在软件系统中，有时候面临着"一个复杂对象"的创建工作，其通常由各个部分的子对象用一定的算法构成；由于需求的变化，这个复杂对象的各个部分经常面临着剧烈的变化，但是将它们组合在一起的算法却相对稳定。</p>
<p><strong>何时使用：</strong>一些基本部件不会变，而其组合经常变化的时候。</p>
<p><strong>如何解决：</strong>将变与不变分离开。</p>
<p><strong>关键代码：</strong>建造者：创建和提供实例，导演：管理建造出来的实例的依赖关系。</p>
<p><strong>应用实例：</strong> 1、去肯德基，汉堡、可乐、薯条、炸鸡翅等是不变的，而其组合是经常变化的，生成出所谓的"套餐"。 2、JAVA 中的 StringBuilder。</p>
<p><strong>优点：</strong> 1、建造者独立，易扩展。 2、便于控制细节风险。</p>
<p><strong>缺点：</strong> 1、产品必须有共同点，范围有限制。 2、如内部变化复杂，会有很多的建造类。</p>
<p><strong>使用场景：</strong> 1、需要生成的对象具有复杂的内部结构。 2、需要生成的对象内部属性本身相互依赖。</p>
<p><strong>注意事项：</strong>与工厂模式的区别是：建造者模式更加关注与零件装配的顺序。</p>
<h2 id="实现-3">实现</h2>
<p>我们假设一个快餐店的商业案例，其中，一个典型的套餐可以是一个汉堡（Burger）和一杯冷饮（Cold drink）。汉堡（Burger）可以是素食汉堡（Veg Burger）或鸡肉汉堡（Chicken Burger），它们是包在纸盒中。冷饮（Cold drink）可以是可口可乐（coke）或百事可乐（pepsi），它们是装在瓶子中。</p>
<p>我们将创建一个表示食物条目（比如汉堡和冷饮）的 <em>Item</em> 接口和实现 <em>Item</em> 接口的实体类，以及一个表示食物包装的 <em>Packing</em> 接口和实现 <em>Packing</em> 接口的实体类，汉堡是包在纸盒中，冷饮是装在瓶子中。</p>
<p>然后我们创建一个 <em>Meal</em> 类，带有 <em>Item</em> 的 <em>ArrayList</em> 和一个通过结合 <em>Item</em> 来创建不同类型的 <em>Meal</em> 对象的 <em>MealBuilder</em>。<em>BuilderPatternDemo</em>，我们的演示类使用 <em>MealBuilder</em> 来创建一个 <em>Meal</em>。</p>
<p><img src="/20200426l2/builder_pattern_uml_diagram.jpg"></p>
<h1 id="原型模式prototype-pattern">原型模式（Prototype Pattern）</h1>
<h2 id="简介-5">简介</h2>
<p>原型模式（Prototype Pattern）是用于创建重复的对象，同时又能保证性能。这种类型的设计模式属于创建型模式，它提供了一种创建对象的最佳方式。</p>
<p>这种模式是实现了一个原型接口，该接口用于创建当前对象的克隆。当直接创建对象的代价比较大时，则采用这种模式。例如，一个对象需要在一个高代价的数据库操作之后被创建。我们可以缓存该对象，在下一个请求时返回它的克隆，在需要的时候更新数据库，以此来减少数据库调用。</p>
<p><strong>意图：</strong>用原型实例指定创建对象的种类，并且通过拷贝这些原型创建新的对象。</p>
<p><strong>主要解决：</strong>在运行期建立和删除原型。</p>
<p><strong>何时使用：</strong> 1、当一个系统应该独立于它的产品创建，构成和表示时。 2、当要实例化的类是在运行时刻指定时，例如，通过动态装载。 3、为了避免创建一个与产品类层次平行的工厂类层次时。 4、当一个类的实例只能有几个不同状态组合中的一种时。建立相应数目的原型并克隆它们可能比每次用合适的状态手工实例化该类更方便一些。</p>
<p><strong>如何解决：</strong>利用已有的一个原型对象，快速地生成和原型对象一样的实例。</p>
<p><strong>关键代码：</strong> 1、实现克隆操作，在 JAVA 继承 Cloneable，重写 clone()，在 .NET 中可以使用 Object 类的 MemberwiseClone() 方法来实现对象的浅拷贝或通过序列化的方式来实现深拷贝。 2、原型模式同样用于隔离类对象的使用者和具体类型（易变类）之间的耦合关系，它同样要求这些"易变类"拥有稳定的接口。</p>
<p><strong>应用实例：</strong> 1、细胞分裂。 2、JAVA 中的 Object clone() 方法。</p>
<p><strong>优点：</strong> 1、性能提高。 2、逃避构造函数的约束。</p>
<p><strong>缺点：</strong> 1、配备克隆方法需要对类的功能进行通盘考虑，这对于全新的类不是很难，但对于已有的类不一定很容易，特别当一个类引用不支持串行化的间接对象，或者引用含有循环结构的时候。 2、必须实现 Cloneable 接口。</p>
<p><strong>使用场景：</strong> 1、资源优化场景。 2、类初始化需要消化非常多的资源，这个资源包括数据、硬件资源等。 3、性能和安全要求的场景。 4、通过 new 产生一个对象需要非常繁琐的数据准备或访问权限，则可以使用原型模式。 5、一个对象多个修改者的场景。 6、一个对象需要提供给其他对象访问，而且各个调用者可能都需要修改其值时，可以考虑使用原型模式拷贝多个对象供调用者使用。 7、在实际项目中，原型模式很少单独出现，一般是和工厂方法模式一起出现，通过 clone 的方法创建一个对象，然后由工厂方法提供给调用者。原型模式已经与 Java 融为浑然一体，大家可以随手拿来使用。</p>
<p><strong>注意事项：</strong>与通过对一个类进行实例化来构造新对象不同的是，原型模式是通过拷贝一个现有对象生成新对象的。浅拷贝实现 Cloneable，重写，深拷贝是通过实现 Serializable 读取二进制流。</p>
<h2 id="实现-4">实现</h2>
<p>我们将创建一个抽象类 <em>Shape</em> 和扩展了 <em>Shape</em> 类的实体类。下一步是定义类 <em>ShapeCache</em>，该类把 shape 对象存储在一个 <em>Hashtable</em> 中，并在请求的时候返回它们的克隆。</p>
<p><em>PrototypePatternDemo</em>，我们的演示类使用 <em>ShapeCache</em> 类来获取 <em>Shape</em> 对象。</p>
<p><img src="/20200426l2/prototype_pattern_uml_diagram.jpg"></p>
<h1 id="适配器模式adapter-pattern">适配器模式（Adapter Pattern）</h1>
<h2 id="简介-6">简介</h2>
<p>适配器模式（Adapter Pattern）是作为两个不兼容的接口之间的桥梁。这种类型的设计模式属于结构型模式，它结合了两个独立接口的功能。</p>
<p>这种模式涉及到一个单一的类，该类负责加入独立的或不兼容的接口功能。举个真实的例子，读卡器是作为内存卡和笔记本之间的适配器。您将内存卡插入读卡器，再将读卡器插入笔记本，这样就可以通过笔记本来读取内存卡。</p>
<p>我们通过下面的实例来演示适配器模式的使用。其中，音频播放器设备只能播放 mp3 文件，通过使用一个更高级的音频播放器来播放 vlc 和 mp4 文件。</p>
<p><strong>意图：</strong>将一个类的接口转换成客户希望的另外一个接口。适配器模式使得原本由于接口不兼容而不能一起工作的那些类可以一起工作。</p>
<p><strong>主要解决：</strong>主要解决在软件系统中，常常要将一些"现存的对象"放到新的环境中，而新环境要求的接口是现对象不能满足的。</p>
<p><strong>何时使用：</strong> 1、系统需要使用现有的类，而此类的接口不符合系统的需要。 2、想要建立一个可以重复使用的类，用于与一些彼此之间没有太大关联的一些类，包括一些可能在将来引进的类一起工作，这些源类不一定有一致的接口。 3、通过接口转换，将一个类插入另一个类系中。（比如老虎和飞禽，现在多了一个飞虎，在不增加实体的需求下，增加一个适配器，在里面包容一个虎对象，实现飞的接口。）</p>
<p><strong>如何解决：</strong>继承或依赖（推荐）。</p>
<p><strong>关键代码：</strong>适配器继承或依赖已有的对象，实现想要的目标接口。</p>
<p><strong>应用实例：</strong> 1、美国电器 110V，中国 220V，就要有一个适配器将 110V 转化为 220V。 2、JAVA JDK 1.1 提供了 Enumeration 接口，而在 1.2 中提供了 Iterator 接口，想要使用 1.2 的 JDK，则要将以前系统的 Enumeration 接口转化为 Iterator 接口，这时就需要适配器模式。 3、在 LINUX 上运行 WINDOWS 程序。 4、JAVA 中的 jdbc。</p>
<p><strong>优点：</strong> 1、可以让任何两个没有关联的类一起运行。 2、提高了类的复用。 3、增加了类的透明度。 4、灵活性好。</p>
<p><strong>缺点：</strong> 1、过多地使用适配器，会让系统非常零乱，不易整体进行把握。比如，明明看到调用的是 A 接口，其实内部被适配成了 B 接口的实现，一个系统如果太多出现这种情况，无异于一场灾难。因此如果不是很有必要，可以不使用适配器，而是直接对系统进行重构。 2.由于 JAVA 至多继承一个类，所以至多只能适配一个适配者类，而且目标类必须是抽象类。</p>
<p><strong>使用场景：</strong>有动机地修改一个正常运行的系统的接口，这时应该考虑使用适配器模式。</p>
<p><strong>注意事项：</strong>适配器不是在详细设计时添加的，而是解决正在服役的项目的问题。</p>
<h2 id="实现-5">实现</h2>
<p>我们有一个 <em>MediaPlayer</em> 接口和一个实现了 <em>MediaPlayer</em> 接口的实体类 <em>AudioPlayer</em>。默认情况下，<em>AudioPlayer</em> 可以播放 mp3 格式的音频文件。</p>
<p>我们还有另一个接口 <em>AdvancedMediaPlayer</em> 和实现了 <em>AdvancedMediaPlayer</em> 接口的实体类。该类可以播放 vlc 和 mp4 格式的文件。</p>
<p>我们想要让 <em>AudioPlayer</em> 播放其他格式的音频文件。为了实现这个功能，我们需要创建一个实现了 <em>MediaPlayer</em> 接口的适配器类 <em>MediaAdapter</em>，并使用 <em>AdvancedMediaPlayer</em> 对象来播放所需的格式。</p>
<p><em>AudioPlayer</em> 使用适配器类 <em>MediaAdapter</em> 传递所需的音频类型，不需要知道能播放所需格式音频的实际类。<em>AdapterPatternDemo</em>，我们的演示类使用 <em>AudioPlayer</em> 类来播放各种格式。</p>
<p><img src="/20200426l2/adapter_pattern_uml_diagram.jpg"></p>
<h1 id="桥接模式bridge-pattern">桥接模式（Bridge Pattern）</h1>
<h2 id="简介-7">简介</h2>
<p>桥接（Bridge）是用于把抽象化与实现化解耦，使得二者可以独立变化。这种类型的设计模式属于结构型模式，它通过提供抽象化和实现化之间的桥接结构，来实现二者的解耦。</p>
<p>这种模式涉及到一个作为桥接的接口，使得实体类的功能独立于接口实现类。这两种类型的类可被结构化改变而互不影响。</p>
<p>我们通过下面的实例来演示桥接模式（Bridge Pattern）的用法。其中，可以使用相同的抽象类方法但是不同的桥接实现类，来画出不同颜色的圆。</p>
<p><strong>意图：</strong>将抽象部分与实现部分分离，使它们都可以独立的变化。</p>
<p><strong>主要解决：</strong>在有多种可能会变化的情况下，用继承会造成类爆炸问题，扩展起来不灵活。</p>
<p><strong>何时使用：</strong>实现系统可能有多个角度分类，每一种角度都可能变化。</p>
<p><strong>如何解决：</strong>把这种多角度分类分离出来，让它们独立变化，减少它们之间耦合。</p>
<p><strong>关键代码：</strong>抽象类依赖实现类。</p>
<p><strong>应用实例：</strong> 1、猪八戒从天蓬元帅转世投胎到猪，转世投胎的机制将尘世划分为两个等级，即：灵魂和肉体，前者相当于抽象化，后者相当于实现化。生灵通过功能的委派，调用肉体对象的功能，使得生灵可以动态地选择。 2、墙上的开关，可以看到的开关是抽象的，不用管里面具体怎么实现的。</p>
<p><strong>优点：</strong> 1、抽象和实现的分离。 2、优秀的扩展能力。 3、实现细节对客户透明。</p>
<p><strong>缺点：</strong>桥接模式的引入会增加系统的理解与设计难度，由于聚合关联关系建立在抽象层，要求开发者针对抽象进行设计与编程。</p>
<p><strong>使用场景：</strong> 1、如果一个系统需要在构件的抽象化角色和具体化角色之间增加更多的灵活性，避免在两个层次之间建立静态的继承联系，通过桥接模式可以使它们在抽象层建立一个关联关系。 2、对于那些不希望使用继承或因为多层次继承导致系统类的个数急剧增加的系统，桥接模式尤为适用。 3、一个类存在两个独立变化的维度，且这两个维度都需要进行扩展。</p>
<p><strong>注意事项：</strong>对于两个独立变化的维度，使用桥接模式再适合不过了。</p>
<h2 id="实现-6">实现</h2>
<p>我们有一个作为桥接实现的 <em>DrawAPI</em> 接口和实现了 <em>DrawAPI</em> 接口的实体类 <em>RedCircle</em>、<em>GreenCircle</em>。<em>Shape</em> 是一个抽象类，将使用 <em>DrawAPI</em> 的对象。<em>BridgePatternDemo</em>，我们的演示类使用 <em>Shape</em> 类来画出不同颜色的圆。</p>
<p><img src="/20200426l2/bridge_pattern_uml_diagram.jpg"></p>
<h1 id="过滤器模式filtercriteria-pattern">过滤器模式（Filter、Criteria Pattern）</h1>
<h2 id="简介-8">简介</h2>
<p>过滤器模式（Filter Pattern）或标准模式（Criteria Pattern）是一种设计模式，这种模式允许开发人员使用不同的标准来过滤一组对象，通过逻辑运算以解耦的方式把它们连接起来。这种类型的设计模式属于结构型模式，它结合多个标准来获得单一标准。</p>
<h2 id="实现-7">实现</h2>
<p>我们将创建一个 <em>Person</em> 对象、<em>Criteria</em> 接口和实现了该接口的实体类，来过滤 <em>Person</em> 对象的列表。<em>CriteriaPatternDemo</em>，我们的演示类使用 <em>Criteria</em> 对象，基于各种标准和它们的结合来过滤 <em>Person</em> 对象的列表。</p>
<p><img src="/20200426l2/filter_pattern_uml_diagram.jpg"></p>
<h1 id="组合模式composite-pattern">组合模式（Composite Pattern）</h1>
<h2 id="简介-9">简介</h2>
<p>组合模式（Composite Pattern），又叫部分整体模式，是用于把一组相似的对象当作一个单一的对象。组合模式依据树形结构来组合对象，用来表示部分以及整体层次。这种类型的设计模式属于结构型模式，它创建了对象组的树形结构。</p>
<p>这种模式创建了一个包含自己对象组的类。该类提供了修改相同对象组的方式。</p>
<p>我们通过下面的实例来演示组合模式的用法。实例演示了一个组织中员工的层次结构。</p>
<p><strong>意图：</strong>将对象组合成树形结构以表示"部分-整体"的层次结构。组合模式使得用户对单个对象和组合对象的使用具有一致性。</p>
<p><strong>主要解决：</strong>它在我们树型结构的问题中，模糊了简单元素和复杂元素的概念，客户程序可以像处理简单元素一样来处理复杂元素，从而使得客户程序与复杂元素的内部结构解耦。</p>
<p><strong>何时使用：</strong> 1、您想表示对象的部分-整体层次结构（树形结构）。 2、您希望用户忽略组合对象与单个对象的不同，用户将统一地使用组合结构中的所有对象。</p>
<p><strong>如何解决：</strong>树枝和叶子实现统一接口，树枝内部组合该接口。</p>
<p><strong>关键代码：</strong>树枝内部组合该接口，并且含有内部属性 List，里面放 Component。</p>
<p><strong>应用实例：</strong> 1、算术表达式包括操作数、操作符和另一个操作数，其中，另一个操作符也可以是操作数、操作符和另一个操作数。 2、在 JAVA AWT 和 SWING 中，对于 Button 和 Checkbox 是树叶，Container 是树枝。</p>
<p><strong>优点：</strong> 1、高层模块调用简单。 2、节点自由增加。</p>
<p><strong>缺点：</strong>在使用组合模式时，其叶子和树枝的声明都是实现类，而不是接口，违反了依赖倒置原则。</p>
<p><strong>使用场景：</strong>部分、整体场景，如树形菜单，文件、文件夹的管理。</p>
<p><strong>注意事项：</strong>定义时为具体类。</p>
<h2 id="实现-8">实现</h2>
<p>我们有一个类 <em>Employee</em>，该类被当作组合模型类。<em>CompositePatternDemo</em>，我们的演示类使用 <em>Employee</em> 类来添加部门层次结构，并打印所有员工。</p>
<p><img src="/20200426l2/composite_pattern_uml_diagram.jpg"></p>
<h1 id="装饰器模式decorator-pattern">装饰器模式（Decorator Pattern）</h1>
<h2 id="简介-10">简介</h2>
<p>装饰器模式（Decorator Pattern）允许向一个现有的对象添加新的功能，同时又不改变其结构。这种类型的设计模式属于结构型模式，它是作为现有的类的一个包装。</p>
<p>这种模式创建了一个装饰类，用来包装原有的类，并在保持类方法签名完整性的前提下，提供了额外的功能。</p>
<p>我们通过下面的实例来演示装饰器模式的用法。其中，我们将把一个形状装饰上不同的颜色，同时又不改变形状类。</p>
<p><strong>意图：</strong>动态地给一个对象添加一些额外的职责。就增加功能来说，装饰器模式相比生成子类更为灵活。</p>
<p><strong>主要解决：</strong>一般的，我们为了扩展一个类经常使用继承方式实现，由于继承为类引入静态特征，并且随着扩展功能的增多，子类会很膨胀。</p>
<p><strong>何时使用：</strong>在不想增加很多子类的情况下扩展类。</p>
<p><strong>如何解决：</strong>将具体功能职责划分，同时继承装饰者模式。</p>
<p><strong>关键代码：</strong> 1、Component 类充当抽象角色，不应该具体实现。 2、修饰类引用和继承 Component 类，具体扩展类重写父类方法。</p>
<p><strong>应用实例：</strong> 1、孙悟空有 72 变，当他变成"庙宇"后，他的根本还是一只猴子，但是他又有了庙宇的功能。 2、不论一幅画有没有画框都可以挂在墙上，但是通常都是有画框的，并且实际上是画框被挂在墙上。在挂在墙上之前，画可以被蒙上玻璃，装到框子里；这时画、玻璃和画框形成了一个物体。</p>
<p><strong>优点：</strong>装饰类和被装饰类可以独立发展，不会相互耦合，装饰模式是继承的一个替代模式，装饰模式可以动态扩展一个实现类的功能。</p>
<p><strong>缺点：</strong>多层装饰比较复杂。</p>
<p><strong>使用场景：</strong> 1、扩展一个类的功能。 2、动态增加功能，动态撤销。</p>
<p><strong>注意事项：</strong>可代替继承。</p>
<h2 id="实现-9">实现</h2>
<p>我们将创建一个 <em>Shape</em> 接口和实现了 <em>Shape</em> 接口的实体类。然后我们创建一个实现了 <em>Shape</em> 接口的抽象装饰类 <em>ShapeDecorator</em>，并把 <em>Shape</em> 对象作为它的实例变量。</p>
<p><em>RedShapeDecorator</em> 是实现了 <em>ShapeDecorator</em> 的实体类。</p>
<p><em>DecoratorPatternDemo</em>，我们的演示类使用 <em>RedShapeDecorator</em> 来装饰 <em>Shape</em> 对象。</p>
<p><img src="/20200426l2/decorator_pattern_uml_diagram.jpg"></p>
<h1 id="外观模式facade-pattern">外观模式（Facade Pattern）</h1>
<h2 id="简介-11">简介</h2>
<p>外观模式（Facade Pattern）隐藏系统的复杂性，并向客户端提供了一个客户端可以访问系统的接口。这种类型的设计模式属于结构型模式，它向现有的系统添加一个接口，来隐藏系统的复杂性。</p>
<p>这种模式涉及到一个单一的类，该类提供了客户端请求的简化方法和对现有系统类方法的委托调用。</p>
<p><strong>意图：</strong>为子系统中的一组接口提供一个一致的界面，外观模式定义了一个高层接口，这个接口使得这一子系统更加容易使用。</p>
<p><strong>主要解决：</strong>降低访问复杂系统的内部子系统时的复杂度，简化客户端与之的接口。</p>
<p><strong>何时使用：</strong> 1、客户端不需要知道系统内部的复杂联系，整个系统只需提供一个"接待员"即可。 2、定义系统的入口。</p>
<p><strong>如何解决：</strong>客户端不与系统耦合，外观类与系统耦合。</p>
<p><strong>关键代码：</strong>在客户端和复杂系统之间再加一层，这一层将调用顺序、依赖关系等处理好。</p>
<p><strong>应用实例：</strong> 1、去医院看病，可能要去挂号、门诊、划价、取药，让患者或患者家属觉得很复杂，如果有提供接待人员，只让接待人员来处理，就很方便。 2、JAVA 的三层开发模式。</p>
<p><strong>优点：</strong> 1、减少系统相互依赖。 2、提高灵活性。 3、提高了安全性。</p>
<p><strong>缺点：</strong>不符合开闭原则，如果要改东西很麻烦，继承重写都不合适。</p>
<p><strong>使用场景：</strong> 1、为复杂的模块或子系统提供外界访问的模块。 2、子系统相对独立。 3、预防低水平人员带来的风险。</p>
<p><strong>注意事项：</strong>在层次化结构中，可以使用外观模式定义系统中每一层的入口。</p>
<h2 id="实现-10">实现</h2>
<p>我们将创建一个 <em>Shape</em> 接口和实现了 <em>Shape</em> 接口的实体类。下一步是定义一个外观类 <em>ShapeMaker</em>。</p>
<p><em>ShapeMaker</em> 类使用实体类来代表用户对这些类的调用。<em>FacadePatternDemo</em>，我们的演示类使用 <em>ShapeMaker</em> 类来显示结果。</p>
<p><img src="/20200426l2/facade_pattern_uml_diagram.jpg"></p>
<h1 id="享元模式flyweight-pattern">享元模式（Flyweight Pattern）</h1>
<h2 id="简介-12">简介</h2>
<p>享元模式（Flyweight Pattern）主要用于减少创建对象的数量，以减少内存占用和提高性能。这种类型的设计模式属于结构型模式，它提供了减少对象数量从而改善应用所需的对象结构的方式。</p>
<p>享元模式尝试重用现有的同类对象，如果未找到匹配的对象，则创建新对象。我们将通过创建 5 个对象来画出 20 个分布于不同位置的圆来演示这种模式。由于只有 5 种可用的颜色，所以 color 属性被用来检查现有的 <em>Circle</em> 对象。</p>
<p><strong>意图：</strong>运用共享技术有效地支持大量细粒度的对象。</p>
<p><strong>主要解决：</strong>在有大量对象时，有可能会造成内存溢出，我们把其中共同的部分抽象出来，如果有相同的业务请求，直接返回在内存中已有的对象，避免重新创建。</p>
<p><strong>何时使用：</strong> 1、系统中有大量对象。 2、这些对象消耗大量内存。 3、这些对象的状态大部分可以外部化。 4、这些对象可以按照内蕴状态分为很多组，当把外蕴对象从对象中剔除出来时，每一组对象都可以用一个对象来代替。 5、系统不依赖于这些对象身份，这些对象是不可分辨的。</p>
<p><strong>如何解决：</strong>用唯一标识码判断，如果在内存中有，则返回这个唯一标识码所标识的对象。</p>
<p><strong>关键代码：</strong>用 HashMap 存储这些对象。</p>
<p><strong>应用实例：</strong> 1、JAVA 中的 String，如果有则返回，如果没有则创建一个字符串保存在字符串缓存池里面。 2、数据库的数据池。</p>
<p><strong>优点：</strong>大大减少对象的创建，降低系统的内存，使效率提高。</p>
<p><strong>缺点：</strong>提高了系统的复杂度，需要分离出外部状态和内部状态，而且外部状态具有固有化的性质，不应该随着内部状态的变化而变化，否则会造成系统的混乱。</p>
<p><strong>使用场景：</strong> 1、系统有大量相似对象。 2、需要缓冲池的场景。</p>
<p><strong>注意事项：</strong> 1、注意划分外部状态和内部状态，否则可能会引起线程安全问题。 2、这些类必须有一个工厂对象加以控制。</p>
<h2 id="实现-11">实现</h2>
<p>我们将创建一个 <em>Shape</em> 接口和实现了 <em>Shape</em> 接口的实体类 <em>Circle</em>。下一步是定义工厂类 <em>ShapeFactory</em>。</p>
<p><em>ShapeFactory</em> 有一个 <em>Circle</em> 的 <em>HashMap</em>，其中键名为 <em>Circle</em> 对象的颜色。无论何时接收到请求，都会创建一个特定颜色的圆。<em>ShapeFactory</em> 检查它的 <em>HashMap</em> 中的 circle 对象，如果找到 <em>Circle</em> 对象，则返回该对象，否则将创建一个存储在 hashmap 中以备后续使用的新对象，并把该对象返回到客户端。</p>
<p><em>FlyWeightPatternDemo</em>，我们的演示类使用 <em>ShapeFactory</em> 来获取 <em>Shape</em> 对象。它将向 <em>ShapeFactory</em> 传递信息（<em>red / green / blue/ black / white</em>），以便获取它所需对象的颜色。</p>
<p><img src="/20200426l2/flyweight_pattern_uml_diagram-1.jpg"></p>
<h1 id="代理模式proxy-pattern">代理模式（Proxy Pattern）</h1>
<h2 id="简介-13">简介</h2>
<p>在代理模式（Proxy Pattern）中，一个类代表另一个类的功能。这种类型的设计模式属于结构型模式。</p>
<p>在代理模式中，我们创建具有现有对象的对象，以便向外界提供功能接口。</p>
<p><strong>意图：</strong>为其他对象提供一种代理以控制对这个对象的访问。</p>
<p><strong>主要解决：</strong>在直接访问对象时带来的问题，比如说：要访问的对象在远程的机器上。在面向对象系统中，有些对象由于某些原因（比如对象创建开销很大，或者某些操作需要安全控制，或者需要进程外的访问），直接访问会给使用者或者系统结构带来很多麻烦，我们可以在访问此对象时加上一个对此对象的访问层。</p>
<p><strong>何时使用：</strong>想在访问一个类时做一些控制。</p>
<p><strong>如何解决：</strong>增加中间层。</p>
<p><strong>关键代码：</strong>实现与被代理类组合。</p>
<p><strong>应用实例：</strong> 1、Windows 里面的快捷方式。 2、猪八戒去找高翠兰结果是孙悟空变的，可以这样理解：把高翠兰的外貌抽象出来，高翠兰本人和孙悟空都实现了这个接口，猪八戒访问高翠兰的时候看不出来这个是孙悟空，所以说孙悟空是高翠兰代理类。 3、买火车票不一定在火车站买，也可以去代售点。 4、一张支票或银行存单是账户中资金的代理。支票在市场交易中用来代替现金，并提供对签发人账号上资金的控制。 5、spring aop。</p>
<p><strong>优点：</strong> 1、职责清晰。 2、高扩展性。 3、智能化。</p>
<p><strong>缺点：</strong> 1、由于在客户端和真实主题之间增加了代理对象，因此有些类型的代理模式可能会造成请求的处理速度变慢。 2、实现代理模式需要额外的工作，有些代理模式的实现非常复杂。</p>
<p><strong>使用场景：</strong>按职责来划分，通常有以下使用场景： 1、远程代理。 2、虚拟代理。 3、Copy-on-Write 代理。 4、保护（Protect or Access）代理。 5、Cache代理。 6、防火墙（Firewall）代理。 7、同步化（Synchronization）代理。 8、智能引用（Smart Reference）代理。</p>
<p><strong>注意事项：</strong> 1、和适配器模式的区别：适配器模式主要改变所考虑对象的接口，而代理模式不能改变所代理类的接口。 2、和装饰器模式的区别：装饰器模式为了增强功能，而代理模式是为了加以控制。</p>
<h2 id="实现-12">实现</h2>
<p>我们将创建一个 <em>Image</em> 接口和实现了 <em>Image</em> 接口的实体类。<em>ProxyImage</em> 是一个代理类，减少 <em>RealImage</em> 对象加载的内存占用。</p>
<p><em>ProxyPatternDemo</em>，我们的演示类使用 <em>ProxyImage</em> 来获取要加载的 <em>Image</em> 对象，并按照需求进行显示。</p>
<p><img src="/20200426l2/proxy_pattern_uml_diagram.jpg"></p>
<h1 id="责任链模式chain-of-responsibility-pattern">责任链模式（Chain of Responsibility Pattern）</h1>
<h2 id="简介-14">简介</h2>
<p>顾名思义，责任链模式（Chain of Responsibility Pattern）为请求创建了一个接收者对象的链。这种模式给予请求的类型，对请求的发送者和接收者进行解耦。这种类型的设计模式属于行为型模式。</p>
<p>在这种模式中，通常每个接收者都包含对另一个接收者的引用。如果一个对象不能处理该请求，那么它会把相同的请求传给下一个接收者，依此类推。</p>
<p><strong>意图：</strong>避免请求发送者与接收者耦合在一起，让多个对象都有可能接收请求，将这些对象连接成一条链，并且沿着这条链传递请求，直到有对象处理它为止。</p>
<p><strong>主要解决：</strong>职责链上的处理者负责处理请求，客户只需要将请求发送到职责链上即可，无须关心请求的处理细节和请求的传递，所以职责链将请求的发送者和请求的处理者解耦了。</p>
<p><strong>何时使用：</strong>在处理消息的时候以过滤很多道。</p>
<p><strong>如何解决：</strong>拦截的类都实现统一接口。</p>
<p><strong>关键代码：</strong>Handler 里面聚合它自己，在 HandlerRequest 里判断是否合适，如果没达到条件则向下传递，向谁传递之前 set 进去。</p>
<p><strong>应用实例：</strong> 1、红楼梦中的"击鼓传花"。 2、JS 中的事件冒泡。 3、JAVA WEB 中 Apache Tomcat 对 Encoding 的处理，Struts2 的拦截器，jsp servlet 的 Filter。</p>
<p><strong>优点：</strong> 1、降低耦合度。它将请求的发送者和接收者解耦。 2、简化了对象。使得对象不需要知道链的结构。 3、增强给对象指派职责的灵活性。通过改变链内的成员或者调动它们的次序，允许动态地新增或者删除责任。 4、增加新的请求处理类很方便。</p>
<p><strong>缺点：</strong> 1、不能保证请求一定被接收。 2、系统性能将受到一定影响，而且在进行代码调试时不太方便，可能会造成循环调用。 3、可能不容易观察运行时的特征，有碍于除错。</p>
<p><strong>使用场景：</strong> 1、有多个对象可以处理同一个请求，具体哪个对象处理该请求由运行时刻自动确定。 2、在不明确指定接收者的情况下，向多个对象中的一个提交一个请求。 3、可动态指定一组对象处理请求。</p>
<p><strong>注意事项：</strong>在 JAVA WEB 中遇到很多应用。</p>
<h2 id="实现-13">实现</h2>
<p>我们创建抽象类 <em>AbstractLogger</em>，带有详细的日志记录级别。然后我们创建三种类型的记录器，都扩展了 <em>AbstractLogger</em>。每个记录器消息的级别是否属于自己的级别，如果是则相应地打印出来，否则将不打印并把消息传给下一个记录器。</p>
<p><img src="/20200426l2/chain_pattern_uml_diagram.jpg"></p>
<h1 id="命令模式command-pattern">命令模式（Command Pattern）</h1>
<h2 id="简介-15">简介</h2>
<p>命令模式（Command Pattern）是一种数据驱动的设计模式，它属于行为型模式。请求以命令的形式包裹在对象中，并传给调用对象。调用对象寻找可以处理该命令的合适的对象，并把该命令传给相应的对象，该对象执行命令。</p>
<p><strong>意图：</strong>将一个请求封装成一个对象，从而使您可以用不同的请求对客户进行参数化。</p>
<p><strong>主要解决：</strong>在软件系统中，行为请求者与行为实现者通常是一种紧耦合的关系，但某些场合，比如需要对行为进行记录、撤销或重做、事务等处理时，这种无法抵御变化的紧耦合的设计就不太合适。</p>
<p><strong>何时使用：</strong>在某些场合，比如要对行为进行"记录、撤销/重做、事务"等处理，这种无法抵御变化的紧耦合是不合适的。在这种情况下，如何将"行为请求者"与"行为实现者"解耦？将一组行为抽象为对象，可以实现二者之间的松耦合。</p>
<p><strong>如何解决：</strong>通过调用者调用接受者执行命令，顺序：调用者→接受者→命令。</p>
<p><strong>关键代码：</strong>定义三个角色：1、received 真正的命令执行对象 2、Command 3、invoker 使用命令对象的入口</p>
<p><strong>应用实例：</strong>struts 1 中的 action 核心控制器 ActionServlet 只有一个，相当于 Invoker，而模型层的类会随着不同的应用有不同的模型类，相当于具体的 Command。</p>
<p><strong>优点：</strong> 1、降低了系统耦合度。 2、新的命令可以很容易添加到系统中去。</p>
<p><strong>缺点：</strong>使用命令模式可能会导致某些系统有过多的具体命令类。</p>
<p><strong>使用场景：</strong>认为是命令的地方都可以使用命令模式，比如： 1、GUI 中每一个按钮都是一条命令。 2、模拟 CMD。</p>
<p><strong>注意事项：</strong>系统需要支持命令的撤销(Undo)操作和恢复(Redo)操作，也可以考虑使用命令模式，见命令模式的扩展。</p>
<h2 id="实现-14">实现</h2>
<p>我们首先创建作为命令的接口 <em>Order</em>，然后创建作为请求的 <em>Stock</em> 类。实体命令类 <em>BuyStock</em> 和 <em>SellStock</em>，实现了 <em>Order</em> 接口，将执行实际的命令处理。创建作为调用对象的类 <em>Broker</em>，它接受订单并能下订单。</p>
<p><em>Broker</em> 对象使用命令模式，基于命令的类型确定哪个对象执行哪个命令。<em>CommandPatternDemo</em>，我们的演示类使用 <em>Broker</em> 类来演示命令模式。</p>
<p><img src="/20200426l2/command_pattern_uml_diagram.jpg"></p>
<h1 id="解释器模式interpreter-pattern">解释器模式（Interpreter Pattern）</h1>
<h2 id="简介-16">简介</h2>
<p>解释器模式（Interpreter Pattern）提供了评估语言的语法或表达式的方式，它属于行为型模式。这种模式实现了一个表达式接口，该接口解释一个特定的上下文。这种模式被用在 SQL 解析、符号处理引擎等。</p>
<p><strong>意图：</strong>给定一个语言，定义它的文法表示，并定义一个解释器，这个解释器使用该标识来解释语言中的句子。</p>
<p><strong>主要解决：</strong>对于一些固定文法构建一个解释句子的解释器。</p>
<p><strong>何时使用：</strong>如果一种特定类型的问题发生的频率足够高，那么可能就值得将该问题的各个实例表述为一个简单语言中的句子。这样就可以构建一个解释器，该解释器通过解释这些句子来解决该问题。</p>
<p><strong>如何解决：</strong>构建语法树，定义终结符与非终结符。</p>
<p><strong>关键代码：</strong>构建环境类，包含解释器之外的一些全局信息，一般是 HashMap。</p>
<p><strong>应用实例：</strong>编译器、运算表达式计算。</p>
<p><strong>优点：</strong> 1、可扩展性比较好，灵活。 2、增加了新的解释表达式的方式。 3、易于实现简单文法。</p>
<p><strong>缺点：</strong> 1、可利用场景比较少。 2、对于复杂的文法比较难维护。 3、解释器模式会引起类膨胀。 4、解释器模式采用递归调用方法。</p>
<p><strong>使用场景：</strong> 1、可以将一个需要解释执行的语言中的句子表示为一个抽象语法树。 2、一些重复出现的问题可以用一种简单的语言来进行表达。 3、一个简单语法需要解释的场景。</p>
<p><strong>注意事项：</strong>可利用场景比较少，JAVA 中如果碰到可以用 expression4J 代替。</p>
<h2 id="实现-15">实现</h2>
<p>我们将创建一个接口 <em>Expression</em> 和实现了 <em>Expression</em> 接口的实体类。定义作为上下文中主要解释器的 <em>TerminalExpression</em> 类。其他的类 <em>OrExpression</em>、<em>AndExpression</em> 用于创建组合式表达式。</p>
<p><em>InterpreterPatternDemo</em>，我们的演示类使用 <em>Expression</em> 类创建规则和演示表达式的解析。</p>
<p><img src="/20200426l2/interpreter_pattern_uml_diagram.jpg"></p>
<h1 id="迭代器模式iterator-pattern">迭代器模式（Iterator Pattern）</h1>
<h2 id="简介-17">简介</h2>
<p>迭代器模式（Iterator Pattern）是 Java 和 .Net 编程环境中非常常用的设计模式。这种模式用于顺序访问集合对象的元素，不需要知道集合对象的底层表示。</p>
<p>迭代器模式属于行为型模式。</p>
<p><strong>意图：</strong>提供一种方法顺序访问一个聚合对象中各个元素, 而又无须暴露该对象的内部表示。</p>
<p><strong>主要解决：</strong>不同的方式来遍历整个整合对象。</p>
<p><strong>何时使用：</strong>遍历一个聚合对象。</p>
<p><strong>如何解决：</strong>把在元素之间游走的责任交给迭代器，而不是聚合对象。</p>
<p><strong>关键代码：</strong>定义接口：hasNext, next。</p>
<p><strong>应用实例：</strong>JAVA 中的 iterator。</p>
<p><strong>优点：</strong> 1、它支持以不同的方式遍历一个聚合对象。 2、迭代器简化了聚合类。 3、在同一个聚合上可以有多个遍历。 4、在迭代器模式中，增加新的聚合类和迭代器类都很方便，无须修改原有代码。</p>
<p><strong>缺点：</strong>由于迭代器模式将存储数据和遍历数据的职责分离，增加新的聚合类需要对应增加新的迭代器类，类的个数成对增加，这在一定程度上增加了系统的复杂性。</p>
<p><strong>使用场景：</strong> 1、访问一个聚合对象的内容而无须暴露它的内部表示。 2、需要为聚合对象提供多种遍历方式。 3、为遍历不同的聚合结构提供一个统一的接口。</p>
<p><strong>注意事项：</strong>迭代器模式就是分离了集合对象的遍历行为，抽象出一个迭代器类来负责，这样既可以做到不暴露集合的内部结构，又可让外部代码透明地访问集合内部的数据。</p>
<h2 id="实现-16">实现</h2>
<p>我们将创建一个叙述导航方法的 <em>Iterator</em> 接口和一个返回迭代器的 <em>Container</em> 接口。实现了 <em>Container</em> 接口的实体类将负责实现 <em>Iterator</em> 接口。</p>
<p><em>IteratorPatternDemo</em>，我们的演示类使用实体类 <em>NamesRepository</em> 来打印 <em>NamesRepository</em> 中存储为集合的 <em>Names</em>。</p>
<p><img src="/20200426l2/iterator_pattern_uml_diagram.jpg"></p>
<h1 id="中介者模式mediator-pattern">中介者模式（Mediator Pattern）</h1>
<h2 id="简介-18">简介</h2>
<p>中介者模式（Mediator Pattern）是用来降低多个对象和类之间的通信复杂性。这种模式提供了一个中介类，该类通常处理不同类之间的通信，并支持松耦合，使代码易于维护。中介者模式属于行为型模式。</p>
<p><strong>意图：</strong>用一个中介对象来封装一系列的对象交互，中介者使各对象不需要显式地相互引用，从而使其耦合松散，而且可以独立地改变它们之间的交互。</p>
<p><strong>主要解决：</strong>对象与对象之间存在大量的关联关系，这样势必会导致系统的结构变得很复杂，同时若一个对象发生改变，我们也需要跟踪与之相关联的对象，同时做出相应的处理。</p>
<p><strong>何时使用：</strong>多个类相互耦合，形成了网状结构。</p>
<p><strong>如何解决：</strong>将上述网状结构分离为星型结构。</p>
<p><strong>关键代码：</strong>对象 Colleague 之间的通信封装到一个类中单独处理。</p>
<p><strong>应用实例：</strong> 1、中国加入 WTO 之前是各个国家相互贸易，结构复杂，现在是各个国家通过 WTO 来互相贸易。 2、机场调度系统。 3、MVC 框架，其中C（控制器）就是 M（模型）和 V（视图）的中介者。</p>
<p><strong>优点：</strong> 1、降低了类的复杂度，将一对多转化成了一对一。 2、各个类之间的解耦。 3、符合迪米特原则。</p>
<p><strong>缺点：</strong>中介者会庞大，变得复杂难以维护。</p>
<p><strong>使用场景：</strong> 1、系统中对象之间存在比较复杂的引用关系，导致它们之间的依赖关系结构混乱而且难以复用该对象。 2、想通过一个中间类来封装多个类中的行为，而又不想生成太多的子类。</p>
<p><strong>注意事项：</strong>不应当在职责混乱的时候使用。</p>
<h2 id="实现-17">实现</h2>
<p>我们通过聊天室实例来演示中介者模式。实例中，多个用户可以向聊天室发送消息，聊天室向所有的用户显示消息。我们将创建两个类 <em>ChatRoom</em> 和 <em>User</em>。<em>User</em> 对象使用 <em>ChatRoom</em> 方法来分享他们的消息。</p>
<p><em>MediatorPatternDemo</em>，我们的演示类使用 <em>User</em> 对象来显示他们之间的通信。</p>
<p><img src="/20200426l2/mediator_pattern_uml_diagram.jpg"></p>
<h1 id="备忘录模式memento-pattern">备忘录模式（Memento Pattern）</h1>
<h2 id="简介-19">简介</h2>
<p>备忘录模式（Memento Pattern）保存一个对象的某个状态，以便在适当的时候恢复对象。备忘录模式属于行为型模式。</p>
<p><strong>意图：</strong>在不破坏封装性的前提下，捕获一个对象的内部状态，并在该对象之外保存这个状态。</p>
<p><strong>主要解决：</strong>所谓备忘录模式就是在不破坏封装的前提下，捕获一个对象的内部状态，并在该对象之外保存这个状态，这样可以在以后将对象恢复到原先保存的状态。</p>
<p><strong>何时使用：</strong>很多时候我们总是需要记录一个对象的内部状态，这样做的目的就是为了允许用户取消不确定或者错误的操作，能够恢复到他原先的状态，使得他有"后悔药"可吃。</p>
<p><strong>如何解决：</strong>通过一个备忘录类专门存储对象状态。</p>
<p><strong>关键代码：</strong>客户不与备忘录类耦合，与备忘录管理类耦合。</p>
<p><strong>应用实例：</strong> 1、后悔药。 2、打游戏时的存档。 3、Windows 里的 ctri + z。 4、IE 中的后退。 4、数据库的事务管理。</p>
<p><strong>优点：</strong> 1、给用户提供了一种可以恢复状态的机制，可以使用户能够比较方便地回到某个历史的状态。 2、实现了信息的封装，使得用户不需要关心状态的保存细节。</p>
<p><strong>缺点：</strong>消耗资源。如果类的成员变量过多，势必会占用比较大的资源，而且每一次保存都会消耗一定的内存。</p>
<p><strong>使用场景：</strong> 1、需要保存/恢复数据的相关状态场景。 2、提供一个可回滚的操作。</p>
<p><strong>注意事项：</strong> 1、为了符合迪米特原则，还要增加一个管理备忘录的类。 2、为了节约内存，可使用原型模式+备忘录模式。</p>
<h2 id="实现-18">实现</h2>
<p>备忘录模式使用三个类 <em>Memento</em>、<em>Originator</em> 和 <em>CareTaker</em>。Memento 包含了要被恢复的对象的状态。Originator 创建并在 Memento 对象中存储状态。Caretaker 对象负责从 Memento 中恢复对象的状态。</p>
<p><em>MementoPatternDemo</em>，我们的演示类使用 <em>CareTaker</em> 和 <em>Originator</em> 对象来显示对象的状态恢复。</p>
<p><img src="/20200426l2/memento_pattern_uml_diagram.jpg"></p>
<h1 id="观察者模式observer-pattern">观察者模式（Observer Pattern）</h1>
<h2 id="简介-20">简介</h2>
<p>当对象间存在一对多关系时，则使用观察者模式（Observer Pattern）。比如，当一个对象被修改时，则会自动通知它的依赖对象。观察者模式属于行为型模式。</p>
<p><strong>意图：</strong>定义对象间的一种一对多的依赖关系，当一个对象的状态发生改变时，所有依赖于它的对象都得到通知并被自动更新。</p>
<p><strong>主要解决：</strong>一个对象状态改变给其他对象通知的问题，而且要考虑到易用和低耦合，保证高度的协作。</p>
<p><strong>何时使用：</strong>一个对象（目标对象）的状态发生改变，所有的依赖对象（观察者对象）都将得到通知，进行广播通知。</p>
<p><strong>如何解决：</strong>使用面向对象技术，可以将这种依赖关系弱化。</p>
<p><strong>关键代码：</strong>在抽象类里有一个 ArrayList 存放观察者们。</p>
<p><strong>应用实例：</strong> 1、拍卖的时候，拍卖师观察最高标价，然后通知给其他竞价者竞价。 2、西游记里面悟空请求菩萨降服红孩儿，菩萨洒了一地水招来一个老乌龟，这个乌龟就是观察者，他观察菩萨洒水这个动作。</p>
<p><strong>优点：</strong> 1、观察者和被观察者是抽象耦合的。 2、建立一套触发机制。</p>
<p><strong>缺点：</strong> 1、如果一个被观察者对象有很多的直接和间接的观察者的话，将所有的观察者都通知到会花费很多时间。 2、如果在观察者和观察目标之间有循环依赖的话，观察目标会触发它们之间进行循环调用，可能导致系统崩溃。 3、观察者模式没有相应的机制让观察者知道所观察的目标对象是怎么发生变化的，而仅仅只是知道观察目标发生了变化。</p>
<p><strong>使用场景：</strong></p>
<ul>
<li>一个抽象模型有两个方面，其中一个方面依赖于另一个方面。将这些方面封装在独立的对象中使它们可以各自独立地改变和复用。</li>
<li>一个对象的改变将导致其他一个或多个对象也发生改变，而不知道具体有多少对象将发生改变，可以降低对象之间的耦合度。</li>
<li>一个对象必须通知其他对象，而并不知道这些对象是谁。</li>
<li>需要在系统中创建一个触发链，A对象的行为将影响B对象，B对象的行为将影响C对象……，可以使用观察者模式创建一种链式触发机制。</li>
</ul>
<p><strong>注意事项：</strong> 1、JAVA 中已经有了对观察者模式的支持类。 2、避免循环引用。 3、如果顺序执行，某一观察者错误会导致系统卡壳，一般采用异步方式。</p>
<h2 id="实现-19">实现</h2>
<p>观察者模式使用三个类 Subject、Observer 和 Client。Subject 对象带有绑定观察者到 Client 对象和从 Client 对象解绑观察者的方法。我们创建 <em>Subject</em> 类、<em>Observer</em> 抽象类和扩展了抽象类 <em>Observer</em> 的实体类。</p>
<p><em>ObserverPatternDemo</em>，我们的演示类使用 <em>Subject</em> 和实体类对象来演示观察者模式。</p>
<p><img src="/20200426l2/observer_pattern_uml_diagram.jpg"></p>
<h1 id="状态模式state-pattern">状态模式（State Pattern）</h1>
<h2 id="简介-21">简介</h2>
<p>在状态模式（State Pattern）中，类的行为是基于它的状态改变的。这种类型的设计模式属于行为型模式。</p>
<p>在状态模式中，我们创建表示各种状态的对象和一个行为随着状态对象改变而改变的 context 对象。</p>
<p><strong>意图：</strong>允许对象在内部状态发生改变时改变它的行为，对象看起来好像修改了它的类。</p>
<p><strong>主要解决：</strong>对象的行为依赖于它的状态（属性），并且可以根据它的状态改变而改变它的相关行为。</p>
<p><strong>何时使用：</strong>代码中包含大量与对象状态有关的条件语句。</p>
<p><strong>如何解决：</strong>将各种具体的状态类抽象出来。</p>
<p><strong>关键代码：</strong>通常命令模式的接口中只有一个方法。而状态模式的接口中有一个或者多个方法。而且，状态模式的实现类的方法，一般返回值，或者是改变实例变量的值。也就是说，状态模式一般和对象的状态有关。实现类的方法有不同的功能，覆盖接口中的方法。状态模式和命令模式一样，也可以用于消除 if...else 等条件选择语句。</p>
<p><strong>应用实例：</strong> 1、打篮球的时候运动员可以有正常状态、不正常状态和超常状态。 2、曾侯乙编钟中，'钟是抽象接口','钟A'等是具体状态，'曾侯乙编钟'是具体环境（Context）。</p>
<p><strong>优点：</strong> 1、封装了转换规则。 2、枚举可能的状态，在枚举状态之前需要确定状态种类。 3、将所有与某个状态有关的行为放到一个类中，并且可以方便地增加新的状态，只需要改变对象状态即可改变对象的行为。 4、允许状态转换逻辑与状态对象合成一体，而不是某一个巨大的条件语句块。 5、可以让多个环境对象共享一个状态对象，从而减少系统中对象的个数。</p>
<p><strong>缺点：</strong> 1、状态模式的使用必然会增加系统类和对象的个数。 2、状态模式的结构与实现都较为复杂，如果使用不当将导致程序结构和代码的混乱。 3、状态模式对"开闭原则"的支持并不太好，对于可以切换状态的状态模式，增加新的状态类需要修改那些负责状态转换的源代码，否则无法切换到新增状态，而且修改某个状态类的行为也需修改对应类的源代码。</p>
<p><strong>使用场景：</strong> 1、行为随状态改变而改变的场景。 2、条件、分支语句的代替者。</p>
<p><strong>注意事项：</strong>在行为受状态约束的时候使用状态模式，而且状态不超过 5 个。</p>
<h2 id="实现-20">实现</h2>
<p>我们将创建一个 <em>State</em> 接口和实现了 <em>State</em> 接口的实体状态类。<em>Context</em> 是一个带有某个状态的类。</p>
<p><em>StatePatternDemo</em>，我们的演示类使用 <em>Context</em> 和状态对象来演示 Context 在状态改变时的行为变化。</p>
<p><img src="/20200426l2/state_pattern_uml_diagram.png"></p>
<h1 id="空对象模式null-object-pattern">空对象模式（Null Object Pattern）</h1>
<h2 id="简介-22">简介</h2>
<p>在空对象模式（Null Object Pattern）中，一个空对象取代 NULL 对象实例的检查。Null 对象不是检查空值，而是反应一个不做任何动作的关系。这样的 Null 对象也可以在数据不可用的时候提供默认的行为。</p>
<p>在空对象模式中，我们创建一个指定各种要执行的操作的抽象类和扩展该类的实体类，还创建一个未对该类做任何实现的空对象类，该空对象类将无缝地使用在需要检查空值的地方。</p>
<h2 id="实现-21">实现</h2>
<p>我们将创建一个定义操作（在这里，是客户的名称）的 <em>AbstractCustomer</em> 抽象类，和扩展了 <em>AbstractCustomer</em> 类的实体类。工厂类 <em>CustomerFactory</em> 基于客户传递的名字来返回 <em>RealCustomer</em> 或 <em>NullCustomer</em> 对象。</p>
<p><em>NullPatternDemo</em>，我们的演示类使用 <em>CustomerFactory</em> 来演示空对象模式的用法。</p>
<p><img src="/20200426l2/null_pattern_uml_diagram.jpg"></p>
<h1 id="策略模式strategy-pattern">策略模式（Strategy Pattern）</h1>
<h2 id="简介-23">简介</h2>
<p>在策略模式（Strategy Pattern）中，一个类的行为或其算法可以在运行时更改。这种类型的设计模式属于行为型模式。</p>
<p>在策略模式中，我们创建表示各种策略的对象和一个行为随着策略对象改变而改变的 context 对象。策略对象改变 context 对象的执行算法。</p>
<p><strong>意图：</strong>定义一系列的算法,把它们一个个封装起来, 并且使它们可相互替换。</p>
<p><strong>主要解决：</strong>在有多种算法相似的情况下，使用 if...else 所带来的复杂和难以维护。</p>
<p><strong>何时使用：</strong>一个系统有许多许多类，而区分它们的只是他们直接的行为。</p>
<p><strong>如何解决：</strong>将这些算法封装成一个一个的类，任意地替换。</p>
<p><strong>关键代码：</strong>实现同一个接口。</p>
<p><strong>应用实例：</strong> 1、诸葛亮的锦囊妙计，每一个锦囊就是一个策略。 2、旅行的出游方式，选择骑自行车、坐汽车，每一种旅行方式都是一个策略。 3、JAVA AWT 中的 LayoutManager。</p>
<p><strong>优点：</strong> 1、算法可以自由切换。 2、避免使用多重条件判断。 3、扩展性良好。</p>
<p><strong>缺点：</strong> 1、策略类会增多。 2、所有策略类都需要对外暴露。</p>
<p><strong>使用场景：</strong> 1、如果在一个系统里面有许多类，它们之间的区别仅在于它们的行为，那么使用策略模式可以动态地让一个对象在许多行为中选择一种行为。 2、一个系统需要动态地在几种算法中选择一种。 3、如果一个对象有很多的行为，如果不用恰当的模式，这些行为就只好使用多重的条件选择语句来实现。</p>
<p><strong>注意事项：</strong>如果一个系统的策略多于四个，就需要考虑使用混合模式，解决策略类膨胀的问题。</p>
<h2 id="实现-22">实现</h2>
<p>我们将创建一个定义活动的 <em>Strategy</em> 接口和实现了 <em>Strategy</em> 接口的实体策略类。<em>Context</em> 是一个使用了某种策略的类。</p>
<p><em>StrategyPatternDemo</em>，我们的演示类使用 <em>Context</em> 和策略对象来演示 Context 在它所配置或使用的策略改变时的行为变化。</p>
<p><img src="/20200426l2/strategy_pattern_uml_diagram.jpg"></p>
<h1 id="模板模式template-pattern">模板模式（Template Pattern）</h1>
<h2 id="简介-24">简介</h2>
<p>在模板模式（Template Pattern）中，一个抽象类公开定义了执行它的方法的方式/模板。它的子类可以按需要重写方法实现，但调用将以抽象类中定义的方式进行。这种类型的设计模式属于行为型模式。</p>
<p><strong>意图：</strong>定义一个操作中的算法的骨架，而将一些步骤延迟到子类中。模板方法使得子类可以不改变一个算法的结构即可重定义该算法的某些特定步骤。</p>
<p><strong>主要解决：</strong>一些方法通用，却在每一个子类都重新写了这一方法。</p>
<p><strong>何时使用：</strong>有一些通用的方法。</p>
<p><strong>如何解决：</strong>将这些通用算法抽象出来。</p>
<p><strong>关键代码：</strong>在抽象类实现，其他步骤在子类实现。</p>
<p><strong>应用实例：</strong> 1、在造房子的时候，地基、走线、水管都一样，只有在建筑的后期才有加壁橱加栅栏等差异。 2、西游记里面菩萨定好的 81 难，这就是一个顶层的逻辑骨架。 3、spring 中对 Hibernate 的支持，将一些已经定好的方法封装起来，比如开启事务、获取 Session、关闭 Session 等，程序员不重复写那些已经规范好的代码，直接丢一个实体就可以保存。</p>
<p><strong>优点：</strong> 1、封装不变部分，扩展可变部分。 2、提取公共代码，便于维护。 3、行为由父类控制，子类实现。</p>
<p><strong>缺点：</strong>每一个不同的实现都需要一个子类来实现，导致类的个数增加，使得系统更加庞大。</p>
<p><strong>使用场景：</strong> 1、有多个子类共有的方法，且逻辑相同。 2、重要的、复杂的方法，可以考虑作为模板方法。</p>
<p><strong>注意事项：</strong>为防止恶意操作，一般模板方法都加上 final 关键词。</p>
<h2 id="实现-23">实现</h2>
<p>我们将创建一个定义操作的 <em>Game</em> 抽象类，其中，模板方法设置为 final，这样它就不会被重写。<em>Cricket</em> 和 <em>Football</em> 是扩展了 <em>Game</em> 的实体类，它们重写了抽象类的方法。</p>
<p><em>TemplatePatternDemo</em>，我们的演示类使用 <em>Game</em> 来演示模板模式的用法。</p>
<p><img src="https://www.runoob.com/wp-content/uploads/2014/08/template_pattern_uml_diagram.jpg"></p>
<h1 id="访问者模式visitor-pattern">访问者模式（Visitor Pattern）</h1>
<h2 id="简介-25">简介</h2>
<p>在访问者模式（Visitor Pattern）中，我们使用了一个访问者类，它改变了元素类的执行算法。通过这种方式，元素的执行算法可以随着访问者改变而改变。这种类型的设计模式属于行为型模式。根据模式，元素对象已接受访问者对象，这样访问者对象就可以处理元素对象上的操作。</p>
<p><strong>意图：</strong>主要将数据结构与数据操作分离。</p>
<p><strong>主要解决：</strong>稳定的数据结构和易变的操作耦合问题。</p>
<p><strong>何时使用：</strong>需要对一个对象结构中的对象进行很多不同的并且不相关的操作，而需要避免让这些操作"污染"这些对象的类，使用访问者模式将这些封装到类中。</p>
<p><strong>如何解决：</strong>在被访问的类里面加一个对外提供接待访问者的接口。</p>
<p><strong>关键代码：</strong>在数据基础类里面有一个方法接受访问者，将自身引用传入访问者。</p>
<p><strong>应用实例：</strong>您在朋友家做客，您是访问者，朋友接受您的访问，您通过朋友的描述，然后对朋友的描述做出一个判断，这就是访问者模式。</p>
<p><strong>优点：</strong> 1、符合单一职责原则。 2、优秀的扩展性。 3、灵活性。</p>
<p><strong>缺点：</strong> 1、具体元素对访问者公布细节，违反了迪米特原则。 2、具体元素变更比较困难。 3、违反了依赖倒置原则，依赖了具体类，没有依赖抽象。</p>
<p><strong>使用场景：</strong> 1、对象结构中对象对应的类很少改变，但经常需要在此对象结构上定义新的操作。 2、需要对一个对象结构中的对象进行很多不同的并且不相关的操作，而需要避免让这些操作"污染"这些对象的类，也不希望在增加新操作时修改这些类。</p>
<p><strong>注意事项：</strong>访问者可以对功能进行统一，可以做报表、UI、拦截器与过滤器。</p>
<h2 id="实现-24">实现</h2>
<p>我们将创建一个定义接受操作的 <em>ComputerPart</em> 接口。<em>Keyboard</em>、<em>Mouse</em>、<em>Monitor</em> 和 <em>Computer</em> 是实现了 <em>ComputerPart</em> 接口的实体类。我们将定义另一个接口 <em>ComputerPartVisitor</em>，它定义了访问者类的操作。<em>Computer</em> 使用实体访问者来执行相应的动作。</p>
<p><em>VisitorPatternDemo</em>，我们的演示类使用 <em>Computer</em>、<em>ComputerPartVisitor</em> 类来演示访问者模式的用法。</p>
<p><img src="/20200426l2/visitor_pattern_uml_diagram.jpg"></p>
<hr>
<p>参考文献：</p>
<p><a href="https://www.runoob.com/design-pattern/design-pattern-tutorial.html" target="_blank" rel="external nofollow noopener noreferrer">设计模式|菜鸟教程</a></p>
]]></content>
      <categories>
        <category>知识点</category>
      </categories>
      <tags>
        <tag>软件工程</tag>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title>【2020 MIA】Marginal loss and exclusion loss for partially supervised multi-organ segmentation</title>
    <url>/20200914l1/</url>
    <content><![CDATA[<h1 id="motivation">Motivation</h1>
<ul>
<li>首先，由于大量的部分标记图像，我们将所有未标记的器官像素与背景标记合并，形成了边际损失。</li>
<li>其次，关于器官相互排斥的众所周知的先验知识边缘，我们设计了一个排除损失，该损失会在每个图像像素上添加排除信息，以进一步减少分割误差。</li>
</ul>
<h1 id="methods">Methods</h1>
<h2 id="常规cross-entropy-loss和常规dice-loss">常规cross-entropy loss和常规Dice loss</h2>
<p>输入结果通过softmax获得后验概率： <span class="math display">\[
p_{n}=\frac{\exp \left(a_{n}\right)}{\sum_{k \in \Omega_{N}} \exp \left(a_{k}\right)}, n \in \Omega_{N}
\]</span> cross-entropy loss: <span class="math display">\[
L_{r C E}=-\sum_{n \in \Omega_{V}} y_{n} \log \left(p_{n}\right)
\]</span> Dice loss: <span class="math display">\[
L_{r \text { Dice }}=\sum_{n \in \Omega_{N}}\left(1-2 \cdot \frac{y_{n} p_{n}}{y_{n}+p_{n}}\right)
\]</span></p>
<h2 id="marginal-loss">Marginal loss</h2>
<p>通过下图方式将不同类别预测标签融合在一起。</p>
<p><img src="/20200914l1/image-20200914143606638.png"></p>
<p>合并后的类别m的marginal概率为： <span class="math display">\[
q_{m}=\sum_{n \in \Phi_{m}} p_{n}
\]</span> 进一步生成对应的one-hot标签</p>
<p>使用定义好的标签进行训练： <span class="math display">\[
\begin{array}{l}
L_{m C E}=-\sum_{m \in \Omega_{M}^{\prime}} z_{m} \log \left(q_{m}\right) \\
L_{m D i c e}=\sum_{m \in \Omega_{M}}\left(1-2 \cdot \frac{z_{m} q_{m}}{z_{m}+q_{m}}\right)
\end{array}
\]</span></p>
<h3 id="问题">问题</h3>
<ol type="1">
<li>这里作者使用one-hot标签进行训练，那么是否可以考虑使用软标签进行训练？</li>
<li>为什么称之为边际损失？</li>
</ol>
<h2 id="exclusion-loss">Exclusion loss</h2>
<p>在多器官分割任务中，某些类是相互排斥的。 排他性损失旨在将排他性作为每个图像像素的附加先验知识添加。</p>
<p>排除损失标签：</p>
<p><img src="/20200914l1/image-20200914150719683.png"></p>
<p>Dice loss: <span class="math display">\[
L_{e D i c e}=\sum_{n \in \Omega_{N}} 2 \cdot \frac{e_{n} \cdot p_{n}}{e_{n}+p_{n}}
\]</span> Cross-entropy loss: <span class="math display">\[
L_{e C E}=\sum_{n \in \Omega_{N}} e_{n} \log \left(p_{n}+\epsilon\right)
\]</span></p>
<h1 id="experiments">Experiments</h1>
<h2 id="数据集">数据集</h2>
<p>数据集<span class="math inline">\(F\)</span>表示全器官标注数据集，<span class="math inline">\(P_i\)</span>表示单个器官标注的数据集。</p>
<h3 id="数据集f">数据集<span class="math inline">\(F\)</span></h3>
<p>采用Multi-Atlas Labeling Beyond the Cranial Bault - Workshop and Challenge全标注数据集。包含13个器官的分割标签，肝脏、脾脏、胰腺、右肾、左肾和其他器官（胆囊、食道、胃、主动脉、下腔静脉、门静脉、脾静脉、右肾上腺、左肾上腺）。</p>
<h3 id="数据集p_1">数据集<span class="math inline">\(P_1\)</span></h3>
<p>采用Decathlon-10数据集中的task03肝脏数据集。将肿瘤合并为器官作为两类（肝脏和背景）的数据集。</p>
<h3 id="数据集p_2">数据集<span class="math inline">\(P_2\)</span></h3>
<p>采用task09脾脏数据集。包含41个带有脾脏分割标签的CT。</p>
<h3 id="数据集p_3">数据集<span class="math inline">\(P_3\)</span></h3>
<p>采用task07胰腺数据集。见癌标签合并到胰腺获得两类（胰腺和背景）的数据集。</p>
<h3 id="数据集p_4">数据集<span class="math inline">\(P_4\)</span></h3>
<p>将KiTS肾脏数据集。手动将数据分为左肾和右肾，并将癌症标签合并到相应的肾脏标签中。</p>
<p>所有数据集重采样为<span class="math inline">\((1.5\times 1.5 \times 3)mm^2\)</span>。</p>
<p>数据集切分：<span class="math inline">\(F\)</span>中随机抽取6个样本，<span class="math inline">\(P_1\)</span>中随机抽取26个样本，<span class="math inline">\(P_3\)</span>中随机抽取56个样本，<span class="math inline">\(P_4\)</span>随机抽取42个样本作为测试，其余用于训练。</p>
<p><img src="/20200914l1/image-20200914160113147.png"></p>
<p><img src="/20200914l1/image-20200914160126165.png"></p>
<h2 id="实验结果">实验结果</h2>
<p><img src="/20200914l1/image-20200914162208393.png"></p>
<p><img src="/20200914l1/image-20200914162223016.png"></p>
<p><img src="/20200914l1/image-20200914162553251.png"></p>
<p><img src="/20200914l1/image-20200914162621846.png"></p>
]]></content>
      <categories>
        <category>阅读笔记</category>
      </categories>
      <tags>
        <tag>2020</tag>
        <tag>图像分割</tag>
        <tag>MIA</tag>
        <tag>多器官</tag>
        <tag>中科院</tag>
      </tags>
  </entry>
  <entry>
    <title>【2019 ICCV】Prior-aware Neural Network for Partially-Supervised Multi-Organ Segmentation</title>
    <url>/20200917l1/</url>
    <content><![CDATA[<h1 id="motivation">Motivation</h1>
<ul>
<li>准确的多器官腹部CT分割对于许多临床应用是必不可少的</li>
<li>通常数据集倍部分标注，如胰腺数据集仅标注胰腺儿其他部分标记为背景</li>
<li>背景标签通常包含其他感兴趣器官，在多器官分割中容易引起误导</li>
</ul>
<p>3D可视化几个腹部器官，可以看出患者的腹部器官大小分布很相似。</p>
<p><img src="/20200917l1/image-20200917213925812.png"></p>
<h1 id="methods">Methods</h1>
<p>将腹部器官大小的解剖学先验结合起来，假设腹部的平均器官大小分布应该接近它们的经验分布，从完全标记的数据集获得的先前统计数据。</p>
<p>通过先验感知损失来实现，先验损失作为辅助和软约束，将不同器官的平均输出大小分布应近似于先前的比例。</p>
<p><img src="/20200917l1/image-20200917214449754.png"></p>
<h2 id="partial-supervision">Partial Supervision</h2>
<p>假设医学图像分析中常见的数据集有以下特征：</p>
<ul>
<li>数据标准化，扫描数据的内部结构是一致的</li>
<li>内部器官具有解剖和空间关系，如胃、十二指肠、小肠和结肠以固定顺序连接。</li>
</ul>
<p>一个简单的解决方案是直接在完全标注和部分标注数据上交替训练分割模型。然而这种类似EM的方法需要高质量的伪标签，且无法引入准确的解剖学先验。</p>
<p>本文提出PaNN模型，嵌入解剖学先验，将其作为附加惩罚项，充当软约束使得器官大小的平均分布应接近经验比例。通过计算完全标记数据集的器官大小统计信息获得此先验。</p>
<h2 id="prior-aware-loss">Prior-aware Loss</h2>
<p>定义全标注数据的标签分布为<span class="math inline">\(q \in \R^{(|\mathcal{L}|+1)\times 1}\)</span>，部分标注的网络输出为<span class="math inline">\(p\)</span>，则部分标注数据的分布为<span class="math inline">\(\overline{p}=\frac{1}{N}\sum_{t=1}^{T}{\sum_{i\in P_t}{\sum_{j}{p_{ij}}}}\)</span>。</p>
<p>通过KL散度匹配两个数据的分布，先验损失定义为： <span class="math display">\[
\mathcal{J}_c = KL(q|\overline{p}) = H(q, \overline{p}) - H(q) = qlog{\overline{p}} + (1-q)log{(1-\overline{p})} + const
\]</span> 等式的基本原理是不同器官的大小分布p赢近似于它们的经验比例q，通常反映了特定领域知识。</p>
<p>最终的训练目标为： <span class="math display">\[
\min _{\boldsymbol{\Theta}, \mathbf{Y}_{\mathrm{P}}} \mathcal{J}_{\mathrm{L}}(\boldsymbol{\Theta})+\lambda_{1} \mathcal{J}_{\mathrm{P}}\left(\boldsymbol{\Theta}, \mathbf{Y}_{\mathrm{P}}\right)-\lambda_{2} \mathcal{J}_{\mathrm{C}}(\boldsymbol{\Theta})
\]</span> 前两项分别为在全标注数据和部分标注数据上的交叉熵损失，最后一项作为软约束来稳定训练过程。 <span class="math display">\[
\mathcal{J}_{\mathrm{L}}=-\frac{1}{N} \sum_{i \in \mathrm{L}} \sum_{j} \sum_{l=0}^{|\mathcal{L}|} \mathbb{1}\left(y_{i j}=l\right) \log p_{i j}^{l}
\]</span></p>
<p><span class="math display">\[
\begin{array}{r}
\mathcal{J}_{\mathrm{P}}=-\frac{1}{N} \sum_{t=1}^{T} \sum_{i \in \mathrm{P}_{t}} \sum_{j} \sum_{l=0}^{|\mathcal{L}|}\left\{\mathbb{1}\left(y_{i j}=l\right) \log p_{i j}^{l}\right. \\
\left.+\mathbb{1}\left(y_{i j}^{\prime}=l\right) \log p_{i j}^{l}\right\}
\end{array}
\]</span></p>
<h2 id="derivation">Derivation</h2>
<p>将<span class="math inline">\(\mathcal{J}_C\)</span>展开得到： <span class="math display">\[
\begin{aligned}
\mathcal{J}_{\mathrm{C}}=&amp; \sum_{l=0}^{|\mathcal{L}|}\left\{q^{l} \log \frac{1}{N} \sum_{t=1}^{T} \sum_{i \in \mathrm{P}_{t}} \sum_{j} p_{i j}^{l}+\right.\\
&amp;\left.\left(1-q^{l}\right) \log \left(1-\frac{1}{N} \sum_{t=1}^{T} \sum_{i \in \mathrm{P}_{t}} \sum_{j} p_{i j}^{l}\right)\right\}+\text {const}
\end{aligned}
\]</span> 等式引入了器官尺寸分布<span class="math inline">\(\overline{p}\)</span>的对数损失，和标准机器学习损失有很大不同，其他的是平均项在对数损失外部。通过随机梯度下降直接最小化损失项非常困难，随机梯度存在固有的偏差。</p>
<p>针对此问题，提出了随机的原始对偶梯度优化KL散度项。通过从对数损失中减去样本平均值，将先验损失转化为等效的min-max问题。引入两个辅助变量： <span class="math display">\[
-\log \alpha=\max _{\beta}(\alpha \beta+1+\log (-\beta))
\]</span> 定义<span class="math inline">\(\mathcal{v}\in \R^{|\mathcal{L}|\times 1}\)</span>和<span class="math inline">\(\mu\in \R^{|\mathcal{L}|\times 1}\)</span>为<span class="math inline">\(\overline{p}\)</span>和<span class="math inline">\((1-\overline{p})\)</span>的对偶变量： <span class="math display">\[
\begin{aligned}
-\log \bar{p}^{l} &amp;=\max _{\nu^{l}}\left(\bar{p}^{l} \nu^{l}+1+\log \left(-\nu^{l}\right)\right) \\
-\log \left(1-\bar{p}^{l}\right) &amp;=\max _{\mu^{l}}\left(\left(1-\bar{p}^{l}\right) \mu^{l}+1+\log \left(-\mu^{l}\right)\right)
\end{aligned}
\]</span> 替换原始公式后得到对偶损失 <span class="math display">\[
\begin{array}{c}
\min _{\boldsymbol{\Theta}} \max _{\nu, \boldsymbol{\mu}} \sum_{l} q^{l}\left(\bar{p}^{l} \nu^{l}+1+\log \left(-\nu^{l}\right)\right) \\
\quad+\sum_{l}\left(1-q^{l}\right)\left(\left(1-\bar{p}^{l}\right) \mu^{l}+1+\log \left(-\mu^{l}\right)\right) \\
\Leftrightarrow \min _{\boldsymbol{\Theta}} \max _{\nu, \boldsymbol{\mu}} \sum_{l}\left(q^{l} \nu^{l}-\left(1-q^{l}\right) \mu^{l}\right) \bar{p}^{l}+q^{l} \log \left(-\nu^{l}\right) \\
\quad+\sum_{l}\left(1-q^{l}\right)\left(\mu^{l}+\log \left(-\mu^{l}\right)\right)
\end{array}
\]</span> 这样样本的平均值从对数损失中取了出来，上述公式中省略了常数项。</p>
<h2 id="model-training">Model Training</h2>
<p>训练分为两个阶段：</p>
<ul>
<li>在全标注数据集上训练器官分割模型，目标是找到合适的初始化值<span class="math inline">\(\theta_O\)</span>，可以稳定第二阶段训练过程</li>
<li>在完全标注数据集和部分标注数据集的并集上训练模型，存在两组变量，网络权重和三个辅助变量<span class="math inline">\(\{ \mathcal{v}, \mu, Y_P \}\)</span>，交替训练优化
<ul>
<li>固定网络参数，更新辅助变量：首先估计伪标签<span class="math inline">\(Y_P\)</span>，同时优化另外两个辅助参数，通过随机梯度上升优化参数最大化问题。</li>
<li>固定辅助变量，更新网络：使用标准的随机梯度下降。</li>
</ul></li>
</ul>
<p><img src="/20200917l1/image-20200918105931642.png"></p>
<h1 id="experiments">Experiments</h1>
<p>采用5折交叉验证，评价指标为Dice系数。</p>
<p><img src="/20200917l1/image-20200918110416475.png"></p>
<p><img src="/20200917l1/image-20200918110706846.png"></p>
<p><img src="/20200917l1/image-20200918110719838.png"></p>
<p><img src="/20200917l1/image-20200918110910038.png"></p>
]]></content>
      <categories>
        <category>阅读笔记</category>
      </categories>
      <tags>
        <tag>图像分割</tag>
        <tag>约翰霍普金斯大学</tag>
        <tag>ICCV</tag>
        <tag>2019</tag>
        <tag>多器官</tag>
        <tag>先验知识</tag>
        <tag>牛津大学</tag>
      </tags>
  </entry>
  <entry>
    <title>【2020】PseudoSeg: Designing pseudo labels for semantic segmentation</title>
    <url>/20201022l1/</url>
    <content><![CDATA[<p>Code: <a href="https://github.com/googleinterns/wss" target="_blank" rel="external nofollow noopener noreferrer">https://github.com/googleinterns/wss</a></p>
<p>Pdf: <a href="https://arxiv.org/abs/2010.09713" target="_blank" rel="external nofollow noopener noreferrer">https://arxiv.org/abs/2010.09713</a></p>
<h1 id="motivation">Motivation</h1>
<ul>
<li>语义分割任务需要大量密集的标签，标注成本高</li>
<li>分割中结构化的输出应用在现有的半监督任务中存在特殊的困难（设计伪标签和数据增强）</li>
</ul>
<p>本文提出PsudoSeg框架，利用图像级标签或无标签数据改善图像分割，贡献有三点：</p>
<ul>
<li>提出一个简单的单阶段框架，通过数量有限的像素标记数据和足够的未标记数据或图像级标记数据来改善语义分割。框架与网络结构无关。</li>
<li>直接应用在图像分类中验证的一致性训练方法会给分割带来特殊挑战。通过融合多源预测结果来获得较好的软伪标签以极大改善分割的一致性训练。</li>
<li>在多个数据集上进行实验，验证了方法的有效性。</li>
</ul>
<h1 id="methods">Methods</h1>
<p>框架结构图如下：</p>
<p><img src="/20201022l1/image-20201022192002650.png"></p>
<h2 id="问题定义">问题定义</h2>
<p>有监督损失为<span class="math inline">\(\mathcal{L}_s\)</span>应用于有标注数据<span class="math inline">\(\mathcal{D}_l\)</span>，一致性损失<span class="math inline">\(\mathcal{L}_u\)</span>应用于无标签数据<span class="math inline">\(\mathcal{D}_u\)</span>。</p>
<p><span class="math inline">\(\mathcal{L}_s\)</span>为标准的cross-entropy loss： <span class="math display">\[
\mathcal{L}_{\mathrm{s}}=\frac{1}{N \times\left|\mathcal{D}_{l}\right|} \sum_{x \in \mathcal{D}_{l}} \sum_{i=0}^{N-1} \text { Cross Entropy }\left(y_{i}, f_{\theta}\left(\omega\left(x_{i}\right)\right)\right)
\]</span> 一致性损失使用的提出的方法估计的伪标签<span class="math inline">\(\widetilde{y}\)</span>进行训练 <span class="math display">\[
\mathcal{L}_{\mathrm{u}}=\frac{1}{N \times\left|\mathcal{D}_{u}\right|} \sum_{x \in \mathcal{D}_{u}} \sum_{i=0}^{N-1} \text { CrossEntropy }\left(\widetilde{y}_{i}, f_{\theta}\left(\beta \circ \omega\left(x_{i}\right)\right)\right)
\]</span></p>
<h2 id="框架设计">框架设计</h2>
<p>如何生成想要的伪标签<span class="math inline">\(\widetilde{y}\)</span>？</p>
<p>最直接的想法是使用训练好的分割模型的decoder的输出作为伪标签。然而在少量数据下这样生成的硬/软标签以及其他输出的后处理的结果都很差。</p>
<p>针对上述问题，本文方法有两个关键点：</p>
<ul>
<li>寻求一种独特而有效的决策机制来补偿decoder输出的潜在错误。</li>
<li>融合多源预测结果生成一个较好的伪标签。</li>
</ul>
<h3 id="开始定位">开始定位</h3>
<p>与分割任务相比，定位任务相对简单，首先从定位角度提升decoder的预测结果。</p>
<p>CAM广泛应用于弱监督定位任务，本文采用Grad-CAM的一个变体进行定位。</p>
<h3 id="从定位到分割">从定位到分割</h3>
<p>尽管CAM仅仅定位感兴趣的部分区域，但如果知道区域之间成对的相似性，可以将CAM分数从判别区域传播到其他未关注区域，可以利用这个特点实现从定位到分割的转换。本文中发现相似性度量方法Hypercolumn在这里很有效。</p>
<p>给定所有C个类别的Grad-CAM输出，将其看做spatially-flatten 2-D向量<span class="math inline">\(m \in \mathbb{R}^{L \times C}\)</span>，每一行<span class="math inline">\(m_i\)</span>是区域<span class="math inline">\(i\)</span>的每个类别的对应权重。使用核函数<span class="math inline">\(\mathcal{K}(·,·): \mathbb{R}^H \times \mathbb{R}^H \rightarrow \mathbb{R}\)</span>测量给定特征的逐像素相似性。可以如下计算传播分数<span class="math inline">\(\hat{m_i} \in \mathbb{R}^C\)</span>： <span class="math display">\[
\hat{m}_{i}=\left(m_{i}+\sum_{j=0}^{L-1} \frac{e^{\mathcal{K}\left(W_{k} h_{i}, W_{v} h_{j}\right)}}{\sum_{k=0}^{L-1} e^{\mathcal{K}\left(W_{k} h_{i}, W_{v} h_{k}\right)}} m_{j}\right) \cdot W_{c}
\]</span> 这个函数用于训练模型，使得m中的高值传播到特征空间的相邻区域，其中<span class="math inline">\(m_i\)</span>为skip-connection。公式可以用自监督的乘法实现，最终得到self-attention Grad-CAM(SGC) maps。</p>
<p><img src="/20201022l1/image-20201022203725820.png"></p>
<h3 id="预测结果的融合">预测结果的融合</h3>
<p>SGC图从低分辨率缩放到需要的大小，不能够很好输出清晰的边界，但与分割的decoder相比，能够更好生成的局部一致的分割图。</p>
<p>通过下面公式将SGC图和decoder的结果融合： <span class="math display">\[
\mathcal{F}(\hat{p}, \hat{m})=\operatorname{Sharpen}\left(\gamma \operatorname{Softmax}\left(\frac{\hat{p}}{\operatorname{Norm}(\hat{p}, \hat{m})}\right)+(1-\gamma) \operatorname{Softmax}\left(\frac{\hat{m}}{\operatorname{Norm}(\hat{p}, \hat{m})}\right), T\right)
\]</span> 融合结果的成功来自两点：</p>
<ul>
<li><span class="math inline">\(\hat{p}\)</span>和<span class="math inline">\(\hat{m}\)</span>来自不同决策机制，拥有不同程度的置信度。</li>
<li>分布的sharpening操作<span class="math inline">\(Sharpen(a, T)_i = \frac{a_i^{\frac{1}{T}}}{\sum_{j}^{C}{a_i^{\frac{1}{T}}}}\)</span></li>
</ul>
<p>下图为不同阶段结果：</p>
<p><img src="/20201022l1/image-20201022202827800.png"></p>
<h3 id="训练">训练</h3>
<p>最终的训练目标有两个额外的损失：分类损失<span class="math inline">\(\mathcal{L}_x\)</span>和分割损失<span class="math inline">\(\mathcal{L}_{sa}\)</span>。</p>
<p>首先，计算Grad-CAM时，在分割backbone后加一层分类头以及一个多标签的分类损失<span class="math inline">\(\mathcal{L}_x\)</span>。</p>
<p>其次，SGC图使用一层卷积进行缩放，预测SGC图需要完全标注数据，使用到额外的分割损失<span class="math inline">\(\mathcal{L}_{sa}\)</span>。</p>
<p>强数据增强使用到了color jittering，去除了所有形态学操作。此外还用到了随机CutOut操作进行结果提升。</p>
<h1 id="experiments">Experiments</h1>
<p><img src="/20201022l1/image-20201022203602649.png"></p>
<p><img src="/20201022l1/image-20201022203612558.png"></p>
<p><img src="/20201022l1/image-20201022203625432.png"></p>
<p><img src="/20201022l1/image-20201022203639236.png"></p>
<p><img src="/20201022l1/image-20201022203651123.png"></p>
]]></content>
      <categories>
        <category>阅读笔记</category>
      </categories>
      <tags>
        <tag>2020</tag>
        <tag>图像分割</tag>
        <tag>code</tag>
        <tag>伪标签</tag>
      </tags>
  </entry>
  <entry>
    <title>【2020】AbdomenCT-1K: Is Abdominal Organ Segmentation A Solved Problem?</title>
    <url>/20201030l1/</url>
    <content><![CDATA[<p>Code: <a href="https://github.com/JunMa11/AbdomenCT-1K" target="_blank" rel="external nofollow noopener noreferrer">https://github.com/JunMa11/AbdomenCT-1K</a></p>
<p>Pdf: <a href="https://arxiv.org/abs/2010.14808" target="_blank" rel="external nofollow noopener noreferrer">https://arxiv.org/abs/2010.14808</a></p>
<h1 id="值得重新考虑的是腹腔器官分割是一个解决的问题吗">值得重新考虑的是，腹腔器官分割是一个解决的问题吗？</h1>
<p>存在以下限制：</p>
<ul>
<li>缺乏大规模和多样化的数据集</li>
<li>缺乏对SOTA方法的综合评估</li>
<li>缺乏针对新出现的注释有效分割任务的基准</li>
<li>对基于器官边界的评估指标缺乏关注</li>
</ul>
<h1 id="abdomenct-1k-dataset">ABDOMENCT-1K DATASET</h1>
<p>基于现有的单中心数据集提出了一个大型腹部CT数据集，包括来自11个具有多中心、多阶段、多供应商的医疗中心的1064个CT病例和多种疾病病例。</p>
<p>数据集包含四个器官：肝脏、肾脏、脾和胰腺</p>
<h1 id="评价指标">评价指标</h1>
<h2 id="基于区域">基于区域</h2>
<p>Dice similarity coeffificient (DSC)： <span class="math display">\[
DSC(G,S)=\frac{2|G\cap S|}{|G|+|S|}
\]</span></p>
<h2 id="基于边缘">基于边缘</h2>
<p>Normalized surface Dice (NSD)： <span class="math display">\[
N S D(G, S)=\frac{\left|\partial G \cap B_{\partial S}^{(\tau)}\right|+\left|\partial S \cap B_{\partial G}^{(\tau)}\right|}{|\partial G|+|\partial S|}
\]</span></p>
<h1 id="分类讨论is-abdominal-organ-segmentation-a-solved-problem">分类讨论“<em>Is abdominal organ segmentation a solved problem?</em>”</h1>
<p>the answer would be <strong>Yes</strong> for liver, kidney and spleen segmentation, if</p>
<ul>
<li>评估指标为DSC，主要用于评估基于区域的分割误差</li>
<li>测试集的数据分布与训练集相同。</li>
<li>测试集中的病例是微不足道的，这意味着这些病例没有严重的疾病且图像质量较低。</li>
</ul>
<p>still remains to be an unsolved problem in following situations</p>
<ul>
<li>评估指标为NSD，其重点是评估器官边界的准确性。</li>
<li>测试集来自新的医疗中心，其数据分布与培训集不同。</li>
<li>测试集中的病例有看不见或严重的不适，图像质量低下，例如异质性病变和噪音，而训练集中的病例则没有或只有很少的类似病例。</li>
</ul>
]]></content>
      <categories>
        <category>阅读笔记</category>
      </categories>
      <tags>
        <tag>2020</tag>
        <tag>code</tag>
        <tag>数据集</tag>
      </tags>
  </entry>
  <entry>
    <title>【踩坑】tmux</title>
    <url>/20201031l1/</url>
    <content><![CDATA[<p>Tmux用于将会话与窗口进行“解绑”，将它们彻底分离。</p>
<p>快捷键使用<code>ctrl+b Pageup</code>可进行翻页查看一定范围的输出记录。</p>
<p>然而最近发现，当进行翻页的时候自己忘记了按<code>esc</code>取消翻页，导致程序一直停留在远处，浪费很多时间。【谨记】</p>
<p>如下图，原本20分钟的程序，停滞了7个多小时。</p>
<p><img src="/20201031l1/Snipaste_2020-10-31_07-09-15.png"></p>
]]></content>
      <categories>
        <category>踩坑</category>
      </categories>
      <tags>
        <tag>踩坑</tag>
        <tag>tmux</tag>
      </tags>
  </entry>
  <entry>
    <title>【2020】EfficientSeg: An Efficient Semantic Segmentation Network</title>
    <url>/20201031l3/</url>
    <content><![CDATA[<p>Pdf: <a href="https://arxiv.org/abs/2009.06469" target="_blank" rel="external nofollow noopener noreferrer">https://arxiv.org/abs/2009.06469</a></p>
<h1 id="abstract">Abstract</h1>
<blockquote>
<p>没有预先训练的权重和很少的数据的深度神经网络训练表明需要更多的训练迭代。 还已知的是，对于语义分割任务，较深层的模型比浅层模型更成功。 因此，我们介绍了EfficientSeg体系结构，它是U-Net的修改和可扩展版本，尽管其深度也可以有效地进行培训。 我们评估了Minicity数据集上的EfficientSeg体系结构，并使用相同的参数计数（51.5％mIoU）胜过了U-Net基线得分（40％mIoU）。 我们最成功的模型获得了58.1％的mIoU分数，并在ECCV 2020 VIPriors挑战的语义细分中排名第四。</p>
</blockquote>
<h1 id="methods">Methods</h1>
<p><img src="/20201031l3/image-20201031182939034.png"></p>
<h1 id="experiments">Experiments</h1>
<p><img src="/20201031l3/image-20201031183001733.png"></p>
]]></content>
      <categories>
        <category>阅读笔记</category>
      </categories>
      <tags>
        <tag>2020</tag>
        <tag>图像分割</tag>
      </tags>
  </entry>
  <entry>
    <title>【2020】Big Self-Supervised Models are Strong Semi-Supervised Learners</title>
    <url>/20201031l4/</url>
    <content><![CDATA[<p>Code: <a href="https://github.com/google-research/simclr" target="_blank" rel="external nofollow noopener noreferrer">https://github.com/google-research/simclr</a></p>
<p>Pdf: <a href="https://arxiv.org/abs/2006.10029" target="_blank" rel="external nofollow noopener noreferrer">https://arxiv.org/abs/2006.10029</a></p>
<h1 id="motivation">Motivation</h1>
<p>通过与任务无关的方式使用未标记数据，作者发现，网络规模非常重要。</p>
<p>也就是说，使用大型（在深度和广度上）神经网络进行自监督的预训练和微调，可以大大提高准确率。</p>
<p>除了网络规模之外，作者表示，这项研究还为对比表示学习提供了一些重要的设计选择，这些选择有益于监督微调和半监督学习。</p>
<p>一旦卷积网络完成了预训练和微调，其在特定任务上的预测就可以得到进一步改善，并可以提炼成更小的网络。</p>
<p>作者表示，对于这种范式的半监督学习，标记越少，就越有可能受益于更大的模型。</p>
<p><img src="/20201031l4/image-20201031184255525.png"></p>
<h1 id="methods">Methods</h1>
<p>方法概括为三个主要步骤：预训练，微调和蒸馏：</p>
<ul>
<li>使用未标记数据，以与任务无关的方式通过无监督的预训练来学习常规（可视）表示形式。</li>
<li>通过监督的微调使一般表示适合于特定任务。</li>
<li>使用未标记的数据，它以特定于任务的方式用于进一步改善预测性能并获得紧凑的模型。使用经过微调的教师网络中的推定标签在未标签数据上训练学生网络。</li>
</ul>
<p><img src="/20201031l4/image-20201031184149183.png"></p>
<h1 id="experiments">Experiments</h1>
<p><img src="/20201031l4/image-20201031184316423.png"></p>
<p><img src="/20201031l4/image-20201031184352418.png"></p>
]]></content>
      <categories>
        <category>阅读笔记</category>
      </categories>
      <tags>
        <tag>2020</tag>
        <tag>半监督学习</tag>
        <tag>code</tag>
      </tags>
  </entry>
  <entry>
    <title>【2020】Selective Information Passing for MR/CT Image Segmentation</title>
    <url>/20201031l2/</url>
    <content><![CDATA[<p>Code: <a href="https://github.com/ahukui/SIPNet" target="_blank" rel="external nofollow noopener noreferrer">https://github.com/ahukui/SIPNet</a></p>
<p>Pdf: <a href="https://arxiv.org/abs/2010.04920" target="_blank" rel="external nofollow noopener noreferrer">https://arxiv.org/abs/2010.04920</a></p>
<h1 id="abstract">Abstract</h1>
<blockquote>
<p>自动化的医学图像分割在许多临床应用中都起着重要作用，但是由于背景纹理复杂，缺乏清晰的边界以及图像之间的形状和纹理变化显着，这是一项非常具有挑战性的任务。 许多研究人员提出了一种具有跳过连接的编码器-解码器体系结构，以将编码器路径中的低级特征图与解码器路径中的高级特征图进行组合，以自动分割医学图像。 跳跃连接已显示在恢复目标对象的细粒度细节方面有效，并且可以促进渐变反向传播。 但是，并非所有由这些连接传输的功能图都对网络性能产生积极的影响。 在本文中，为了自适应地选择有用的信息以通过那些跳过连接，我们提出了一种具有自我监督功能的新型3D网络，称为选择性信息传递网络（SIP-Net）。 我们在MICCAI前列腺MR图像分割2012 Grant挑战数据集，TCIA胰腺CT-82和MICCAI 2017肝肿瘤分割（LiTS）挑战数据集上评估了我们提出的模型。 这些数据集上的实验结果表明，我们的模型实现了改进的分割结果，并优于其他最新方法。 该工作的源代码可从以下https URL获得。</p>
</blockquote>
<blockquote>
<p>Compared with 2D networks, these 3D networks were able to achieve better segmentation performance. However, 3D CNNs have a much larger number of parameters and computational complexity than 2D networks. Due to the limited size of typical medical image dataset, it makes the network dififificult to train. Furthermore, the trained network easily suffers from overfifitting. Therefore, there is still much need in pushing the potential of CNNs by effectively extracting the information from limited training data to improve the segmentation performance and also reduce the complexity of the networks to avoid overfifitting.</p>
</blockquote>
<p><img src="/20201031l2/image-20201031182209161.png"></p>
<h1 id="methods">Methods</h1>
<p><img src="/20201031l2/image-20201031182240321.png"></p>
<h1 id="results">Results</h1>
<p><img src="/20201031l2/image-20201031182312841.png"></p>
<p><img src="/20201031l2/image-20201031182328899.png"></p>
<p><img src="/20201031l2/image-20201031182344786.png"></p>
<p><img src="/20201031l2/image-20201031182400657.png"></p>
]]></content>
      <categories>
        <category>阅读笔记</category>
      </categories>
      <tags>
        <tag>2020</tag>
        <tag>图像分割</tag>
        <tag>code</tag>
        <tag>武汉大学</tag>
      </tags>
  </entry>
</search>
